{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok1qkt3pvrpi"
      },
      "source": [
        "## Initial settings (authorize, connect GD, install packages, etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2YjXZTBuyY8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title get GEE authorization\n",
        "import ee\n",
        "\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "# Initialize the library.\n",
        "ee.Initialize(project='ee-scai62')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFzcQBXFzb3P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Connect to my Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr9sC06bziKw"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/' .\n",
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/EnKF/LGAR' .\n",
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/EnKF/LGAR_EnKF' .\n",
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/EnKF/settingInfo.txt' .\n",
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/EnKF/vG_default_params_HYDRUS.csv' ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWGkiKt518nz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Install necessary packages\n",
        "!pip install folium\n",
        "!pip install geopandas\n",
        "!pip install netCDF4\n",
        "!pip install --upgrade xee\n",
        "!pip install rasterio\n",
        "!pip install ipywidgets\n",
        "!pip install permetrics==2.0.0\n",
        "!pip install pyDEM\n",
        "# !pip install richdem\n",
        "!pip install pyflwdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku6RglgO2FUR"
      },
      "outputs": [],
      "source": [
        "# @title Import packages\n",
        "import folium\n",
        "from folium import Figure\n",
        "import geopandas as gpd\n",
        "import json\n",
        "import geemap\n",
        "import numpy as np\n",
        "from shapely.geometry import mapping\n",
        "import pandas as pd\n",
        "import netCDF4 as nc\n",
        "import xarray\n",
        "import rasterio\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from rasterio.crs import CRS\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import joblib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from permetrics import RegressionMetric\n",
        "import osgeo.gdal as gdal\n",
        "# import richdem as rd\n",
        "import pyflwdir\n",
        "from osgeo import osr, gdalconst\n",
        "from scipy.stats import rankdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR7GKJyKwmqJ"
      },
      "source": [
        "## Define study site: set longitude and latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEk-95TB2LUK"
      },
      "outputs": [],
      "source": [
        "# @title Input longitude and latitude\n",
        "\n",
        "global lon, lat\n",
        "lon, lat = None, None\n",
        "\n",
        "def validate_coordinates(lon, lat):\n",
        "    return -125 <= lon <= -66 and 24 <= lat <= 50\n",
        "\n",
        "def on_button_click(b):\n",
        "    global lon, lat\n",
        "    try:\n",
        "        lon = float(longitude_input.value)\n",
        "        lat = float(latitude_input.value)\n",
        "        if validate_coordinates(lon, lat):\n",
        "            output_area.clear_output()\n",
        "            with output_area:\n",
        "                print(f\"Valid coordinates: Longitude {lon}, Latitude {lat}\")\n",
        "        else:\n",
        "            output_area.clear_output()\n",
        "            with output_area:\n",
        "                print(\"Invalid coordinates. Please enter values within the contiguous USA.\")\n",
        "    except ValueError:\n",
        "        output_area.clear_output()\n",
        "        with output_area:\n",
        "            print(\"Invalid input. Please enter numeric values.\")\n",
        "\n",
        "longitude_input = widgets.Text(placeholder=\"Enter Longitude\")\n",
        "latitude_input = widgets.Text(placeholder=\"Enter Latitude\")\n",
        "submit_button = widgets.Button(description=\"OK\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "submit_button.on_click(on_button_click)\n",
        "\n",
        "display(widgets.HBox([widgets.Label(\"Longitude:\"), longitude_input]))\n",
        "display(widgets.HBox([widgets.Label(\"Latitude:\"), latitude_input]))\n",
        "display(submit_button)\n",
        "display(output_area)\n",
        "\n",
        "# test: Lon: Text(value=' -89.35613412', placeholder='Enter Longitude')\n",
        "# Text(value=' 40.66050086', placeholder='Enter Latitude')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Store in form of list\n",
        "id_list = [0]\n",
        "lat_list = [lat]\n",
        "lon_list = [lon]\n",
        "print(id_list)\n",
        "print(lat_list)\n",
        "print(lon_list)"
      ],
      "metadata": {
        "id": "P81Z0sDWnrXK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwxhZuVW3agB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define feature collection: points ID, longitude, and latitude\n",
        "lons_sub = ee.List(lon_list)\n",
        "lats_sub = ee.List(lat_list)\n",
        "idnum_sub = ee.List(id_list)\n",
        "points = lons_sub.zip(lats_sub).zip(idnum_sub).map(lambda coords: ee.Feature(ee.Geometry.Point(ee.List(coords).get(0)),{'id_num': ee.List(coords).get(1)}))\n",
        "feature_collection = ee.FeatureCollection(points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_collection.getInfo()"
      ],
      "metadata": {
        "id": "nImn7d2OpIz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZEEwnoR3tvw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Visualization - spatial distribution of sites\n",
        "import folium\n",
        "from folium import Figure\n",
        "\n",
        "fig = Figure(width=800, height=600)\n",
        "m = folium.Map(location=[42, -95.25], zoom_start=4)\n",
        "\n",
        "\n",
        "roi_geojson = feature_collection.getInfo()\n",
        "folium.TileLayer(\n",
        "    tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "    attr='Google Satellite',\n",
        "    name='Satellite',\n",
        "    overlay=True\n",
        ").add_to(m)\n",
        "folium.GeoJson(roi_geojson).add_to(m)\n",
        "\n",
        "fig.add_child(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHUbWlcdw0UD"
      },
      "source": [
        "## Read ee.images (constant images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get TWI - predefine functions\n",
        "\n",
        "def set_nodata_value(raster_file, nodata_value):\n",
        "    # Open the raster file in read mode\n",
        "    with rasterio.open(raster_file) as src:\n",
        "        # Copy the metadata of the source raster\n",
        "        meta = src.meta.copy()\n",
        "        # Read the data\n",
        "        data = src.read()\n",
        "\n",
        "    # Set the nodata value\n",
        "    meta.update(nodata=nodata_value)\n",
        "\n",
        "    # Open the raster in write mode and overwrite the existing file\n",
        "    with rasterio.open(raster_file, 'w', **meta) as dst:\n",
        "        # Write the data to the same raster with the updated metadata\n",
        "        dst.write(data)\n",
        "\n",
        "def readRaster(filename):\n",
        "    dataset=gdal.Open(filename, gdal.GA_ReadOnly)\n",
        "    if not dataset:\n",
        "        print(\"false\")\n",
        "        return 0,0\n",
        "    else:\n",
        "        band=dataset.GetRasterBand(1)\n",
        "        arr = band.ReadAsArray()\n",
        "        return np.array(arr), band.GetNoDataValue(),dataset.GetProjectionRef(),dataset.GetGeoTransform()\n",
        "\n",
        "def arr2raster(filename,arr,proj,transform):\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    etype = gdal.GDT_Float32\n",
        "    # no_data_value = ''\n",
        "    ds = driver.Create(filename, len(arr[0]), len(arr), 1, etype)\n",
        "    ds.SetGeoTransform(transform)\n",
        "    ds.SetProjection(proj)\n",
        "    # ds.SetMetadataItem(\"NODATA_VALUES\", \"-3.40282346639e+038\")\n",
        "    # ds.GetRasterBand(1).SetNoDataValue(-1)\n",
        "    ds.GetRasterBand(1).WriteArray(arr)\n",
        "    return\n",
        "\n",
        "\n",
        "def surface_area(phi_a, phi_b, lambda_c, lambda_d):\n",
        "    # this function from : https://github.com/chaneyn/geospatialtools.git\n",
        "    Re = 6371000.0   # m\n",
        "    #Convert to radians\n",
        "    phi_a = phi_a*np.pi/180.0\n",
        "    phi_b = phi_b*np.pi/180.0\n",
        "    lambda_c = lambda_c*np.pi/180.0\n",
        "    lambda_d = lambda_d*np.pi/180.0\n",
        "    #Calculate surface area\n",
        "    return Re**2*np.abs(np.sin(phi_a)-np.sin(phi_b))*np.abs(lambda_c - lambda_d)\n",
        "\n",
        "def get_resx(file):\n",
        "    fp = rasterio.open(file)\n",
        "    if fp.crs == 'EPSG:4326':  # This is a temporary fix\n",
        "        phi_a = fp.bounds.bottom\n",
        "        phi_b = fp.bounds.top\n",
        "        lambda_c = fp.bounds.left\n",
        "        lambda_d = fp.bounds.right\n",
        "        area = surface_area(phi_a, phi_b, lambda_c, lambda_d)\n",
        "        area = area / fp.width / fp.height\n",
        "        dx = area ** 0.5\n",
        "    else:\n",
        "        dx = rasterio.open(file).res[0]  # meters\n",
        "\n",
        "    return dx\n",
        "\n",
        "def fillnodata(infile):\n",
        "    input_ds = gdal.Open(infile, gdal.GA_Update)\n",
        "    if input_ds is None:\n",
        "        print(\"Failed to open the input raster.\")\n",
        "        return\n",
        "    # Fill the nodata areas\n",
        "    gdal.FillNodata(input_ds.GetRasterBand(1), None, 10, 0)\n",
        "    return\n",
        "\n",
        "# def calculate_d8_acc(indem, outAccd8):\n",
        "#     dem = rd.LoadGDAL(indem)\n",
        "#     dem_filled = rd.FillDepressions(dem, epsilon=True, in_place=False)\n",
        "#     accum_d8 = rd.FlowAccumulation(dem_filled, method='D8')\n",
        "\n",
        "#     # Save the DEM\n",
        "#     rd.SaveGDAL(outAccd8, accum_d8)\n",
        "#     return\n",
        "\n",
        "def calculate_d8_acc(indem, outAccd8):\n",
        "    # Load DEM\n",
        "    with rasterio.open(indem, \"r\") as src:\n",
        "      elevtn = src.read(1)\n",
        "      nodata = src.nodata\n",
        "      transform = src.transform\n",
        "      crs = src.crs\n",
        "      profile = src.profile\n",
        "\n",
        "\n",
        "    flw = pyflwdir.from_dem(\n",
        "        data=elevtn,\n",
        "        nodata=src.nodata,\n",
        "        transform=transform,\n",
        "        latlon=crs.is_geographic,\n",
        "        outlets=\"min\",\n",
        "        ) # FlwdirRaster        Actionable flow direction object  (FlwdirRaster class)\n",
        "    # this function will perform pit pression.\n",
        "\n",
        "    # Compute flow accumulation\n",
        "    # data = np.ones(elevtn.shape)\n",
        "    # accum_d8 = flw.accuflux(data)  # Flwdir class\n",
        "    # print(accum_d8)\n",
        "\n",
        "\n",
        "    accum_d8 = flw.upstream_area()\n",
        "    print(accum_d8)\n",
        "    # they are doing same job: flw.accuflux(data) and flw.upstream_area()\n",
        "\n",
        "\n",
        "    # Save output\n",
        "    profile.update(dtype=rasterio.float32, count=1, compress='lzw')\n",
        "    with rasterio.open(outAccd8, \"w\", **profile) as dst:\n",
        "        dst.write(accum_d8.astype(np.float32), 1)\n",
        "\n",
        "    return\n",
        "\n",
        "def calculate_slope(indem, outSlope):\n",
        "    dem = gdal.Open(indem, gdalconst.GA_ReadOnly)\n",
        "    dx0 = dem.GetGeoTransform()[1]\n",
        "    dx = get_resx(indem)\n",
        "    ratio = dx/dx0   # ratio of vertical units to horizontal\n",
        "    if dem is None:\n",
        "        print(\"Failed to open the input DEM dataset.\")\n",
        "        return\n",
        "\n",
        "    gdal.DEMProcessing(destName=outSlope,\n",
        "                       srcDS=dem,\n",
        "                       processing='slope',\n",
        "                       scale=ratio)\n",
        "\n",
        "    return\n",
        "\n",
        "def TWI(inAccd8, inSlope, outTWI, dx):\n",
        "    # twi = ln(a/tanb), where a is uplope contributing area (m2), b is slope in radians\n",
        "    accd8_arr, accd8_nodata, accd8_proj, accd8_geotransform = readRaster(inAccd8)\n",
        "\n",
        "    slope_arr, slope_nodata, slope_proj, slope_geotransform = readRaster(inSlope)\n",
        "\n",
        "    slope_arr[slope_arr<=0] = 1   # avoid 0 values\n",
        "    slope_arr = slope_arr * 0.01745  # convert to radians\n",
        "\n",
        "    uparea_arr = accd8_arr * dx **2\n",
        "\n",
        "    twi_arr = np.log(uparea_arr/np.tan(slope_arr))\n",
        "\n",
        "    arr2raster(outTWI,twi_arr,accd8_proj, accd8_geotransform)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "def focal_stat(infile,  lat,lon, dxl, dxs, undef=-9999):\n",
        "    # Open file and get geotransformation\n",
        "    ds = gdal.Open(infile)\n",
        "    gt = ds.GetGeoTransform()\n",
        "    rb = ds.GetRasterBand(1)\n",
        "    nx = ds.RasterXSize\n",
        "    ny = ds.RasterYSize\n",
        "    # Compute ilats and ilons\n",
        "\n",
        "    ilon = int((lon - (gt[0] + gt[1] / 2)) / gt[1])\n",
        "    ilat = int((lat - (gt[3] + gt[5] / 2)) / gt[5])\n",
        "\n",
        "    focal_width = int(dxl/dxs)\n",
        "\n",
        "    # Extract data\n",
        "    if ((ilon - int(focal_width/2) < 0) | (ilon + int(focal_width/2) >= nx)) | \\\n",
        "            ((ilat - int(focal_width/2) < 0) | (ilat + int(focal_width/2) >= ny)):\n",
        "        focal_arr = undef\n",
        "    else:\n",
        "        focal_arr = rb.ReadAsArray(ilon - int(focal_width/2), ilat - int(focal_width/2), focal_width, focal_width) #[0] #[0]\n",
        "\n",
        "    Lambda = np.mean(focal_arr)\n",
        "    mask = focal_arr >= Lambda\n",
        "    Fmax = np.sum(mask)/focal_arr.size\n",
        "\n",
        "    return Fmax, Lambda\n",
        "\n",
        "\n",
        "def extract_point_data(file, lat, lon, undef=-9999.0):\n",
        "    # Open file and get geotransformation\n",
        "    ds = gdal.Open(file)\n",
        "    gt = ds.GetGeoTransform()\n",
        "    rb = ds.GetRasterBand(1)\n",
        "    nx = ds.RasterXSize\n",
        "    ny = ds.RasterYSize\n",
        "\n",
        "    # Compute ilats and ilons\n",
        "    ilon = np.round((np.array(lon) - (gt[0] + gt[1] / 2)) / gt[1]).astype(int)\n",
        "    ilat = np.round((np.array(lat) - (gt[3] + gt[5] / 2)) / gt[5]).astype(int)\n",
        "\n",
        "    ilon = int(ilon)\n",
        "    ilat = int(ilat)\n",
        "    if ((ilon < 0) | (ilon >= nx)) | ((ilat < 0) | (ilat >= ny)):\n",
        "        return undef\n",
        "    else:\n",
        "        return rb.ReadAsArray(ilon, ilat, 1, 1)[0][0]"
      ],
      "metadata": {
        "id": "1H0abe9LuIT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get TWI\n",
        "# step1: get DEM data\n",
        "dem = ee.Image(\"USGS/3DEP/10m\")\n",
        "\n",
        "for i in range(len(id_list)):\n",
        "  name = id_list[i]\n",
        "  latt = lat_list[i]\n",
        "  lonn = lon_list[i]\n",
        "\n",
        "  coordinates = [\n",
        "      [\n",
        "          [lonn - 0.1, latt + 0.1],\n",
        "          [lonn + 0.1, latt + 0.1],\n",
        "          [lonn + 0.1, latt - 0.1],\n",
        "          [lonn - 0.1, latt - 0.1],\n",
        "          [lonn - 0.1, latt + 0.1]\n",
        "      ]\n",
        "  ]\n",
        "\n",
        "  # Create a Python dictionary representing the GeoJSON Polygon\n",
        "  polygon_geojson = {\n",
        "      \"type\": \"Polygon\",\n",
        "      \"coordinates\": coordinates\n",
        "  }\n",
        "\n",
        "  # Convert the Python dictionary to a JSON string\n",
        "  roi = json.dumps(polygon_geojson)\n",
        "\n",
        "  #NIFA_Download\n",
        "  geemap.ee_export_image(dem, filename=f'dem_{name}.tif', scale=10, region=roi, file_per_band=False, crs='EPSG:4326')\n",
        "\n",
        "# step2: calculate TWI\n",
        "def calculate_twi(idd, lonn, latt):\n",
        "  if os.path.exists(f'TWI_{idd}.tif'):\n",
        "    return i\n",
        "\n",
        "  # step 2.-1: set nodata\n",
        "  set_nodata_value(f'dem_{idd}.tif', -9999)\n",
        "\n",
        "  # step 2.0: get resolution\n",
        "  dx = get_resx(f'dem_{idd}.tif')\n",
        "\n",
        "  # step 2.1: fill voids\n",
        "  fillnodata(f'dem_{idd}.tif')\n",
        "\n",
        "  # step 2.2: calculate slope\n",
        "  calculate_slope(f'dem_{idd}.tif', f'slope_{idd}.tif')\n",
        "\n",
        "  # step 2.3: calculate accumulate 8d\n",
        "  calculate_d8_acc(f'dem_{idd}.tif', f'acc8d_{idd}.tif')\n",
        "\n",
        "  # step 2.4: calculate TWI\n",
        "  TWI(inAccd8=f'acc8d_{idd}.tif',\n",
        "      inSlope=f'slope_{idd}.tif',\n",
        "      outTWI=f'TWI_{idd}.tif',\n",
        "      dx=dx)\n",
        "  return\n",
        "\n",
        "\n",
        "for i in range(len(id_list)):\n",
        "  idd = id_list[i]\n",
        "  latt = lat_list[i]\n",
        "  lonn = lon_list[i]\n",
        "  calculate_twi(idd, lonn, latt)"
      ],
      "metadata": {
        "id": "hq4DskW_slv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title extract values\n",
        "twi_list = []\n",
        "Fmax_list = []\n",
        "Lambda_list = []\n",
        "# GW_list = []\n",
        "\n",
        "for i in range(len(id_list)):\n",
        "  idd = id_list[i]\n",
        "  latt = lat_list[i]\n",
        "  lonn = lon_list[i]\n",
        "\n",
        "  twi_list.append(extract_point_data(f'TWI_{idd}.tif', latt, lonn))\n",
        "\n",
        "  # GW_list.append(extract_point_data('./StartFolder/BaseData/GroundWater.tif'))\n",
        "\n",
        "  dxxx = get_resx(f'TWI_{idd}.tif')\n",
        "  print(dxxx)\n",
        "  Fmax, Lambda = focal_stat(f'TWI_{idd}.tif', latt, lonn, 100, dxxx)\n",
        "  Fmax_list.append(Fmax)\n",
        "  Lambda_list.append(Lambda)"
      ],
      "metadata": {
        "id": "JNrwdDJA2aah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzE4MRXr7DQn"
      },
      "outputs": [],
      "source": [
        "# @title Constant images\n",
        "# DEM\n",
        "dem = ee.Terrain.products(ee.Image(\"USGS/3DEP/10m\"))\n",
        "\n",
        "# Landcover\n",
        "LC = ee.ImageCollection('USGS/NLCD_RELEASES/2019_REL/NLCD').filterDate('2016-01-01', '2022-12-31').select('landcover').first()\n",
        "# print(LC);\n",
        "\n",
        "#polaris\n",
        "bd_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_0_5').first().rename('bd_0_5')\n",
        "# print(bd_0_5);\n",
        "bd_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_5_15').first().rename('bd_5_15')\n",
        "# print(bd_5_15);\n",
        "bd_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_15_30').first().rename('bd_15_30')\n",
        "bd_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_30_60').first().rename('bd_30_60')\n",
        "bd_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_60_100').first().rename('bd_60_100')\n",
        "bd_0_100 = bd_0_5.multiply(0.05).add(bd_5_15.multiply(0.1)).add(bd_15_30.multiply(0.15)).add(bd_30_60.multiply(0.3)).add(bd_60_100.multiply(0.4)).rename('bd_0_100')\n",
        "\n",
        "clay_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_0_5').first().rename('clay_0_5')\n",
        "clay_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_5_15').first().rename('clay_5_15')\n",
        "clay_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_15_30').first().rename('clay_15_30')\n",
        "clay_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_30_60').first().rename('clay_30_60')\n",
        "clay_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_60_100').first().rename('clay_60_100')\n",
        "clay_0_100 = clay_0_5.multiply(0.05).add(clay_5_15.multiply(0.1)).add(clay_15_30.multiply(0.15)).add(clay_30_60.multiply(0.3)).add(clay_60_100.multiply(0.4)).rename('clay_0_100')\n",
        "\n",
        "ksat_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_0_5').first().rename('ksat_0_5')\n",
        "ksat_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_5_15').first().rename('ksat_5_15')\n",
        "ksat_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_15_30').first().rename('ksat_15_30')\n",
        "ksat_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_30_60').first().rename('ksat_30_60')\n",
        "ksat_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_60_100').first().rename('ksat_60_100')\n",
        "ksat_0_100 = ksat_0_5.multiply(0.05).add(ksat_5_15.multiply(0.1)).add(ksat_15_30.multiply(0.15)).add(ksat_30_60.multiply(0.3)).add(ksat_60_100.multiply(0.4)).rename('ksat_0_100')\n",
        "\n",
        "n_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/n_mean').filterMetadata('system:index', 'equals', 'n_0_5').first().rename('n_0_5')\n",
        "n_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/n_mean').filterMetadata('system:index', 'equals', 'n_5_15').first().rename('n_5_15')\n",
        "n_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/n_mean').filterMetadata('system:index', 'equals', 'n_15_30').first().rename('n_15_30')\n",
        "n_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/n_mean').filterMetadata('system:index', 'equals', 'n_30_60').first().rename('n_30_60')\n",
        "n_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/n_mean').filterMetadata('system:index', 'equals', 'n_60_100').first().rename('n_60_100')\n",
        "n_0_100 = n_0_5.multiply(0.05).add(n_5_15.multiply(0.1)).add(n_15_30.multiply(0.15)).add(n_30_60.multiply(0.3)).add(n_60_100.multiply(0.4)).rename('n_0_100')\n",
        "\n",
        "sand_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_0_5').first().rename('sand_0_5')\n",
        "sand_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_5_15').first().rename('sand_5_15')\n",
        "sand_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_15_30').first().rename('sand_15_30')\n",
        "sand_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_30_60').first().rename('sand_30_60')\n",
        "sand_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_60_100').first().rename('sand_60_100')\n",
        "sand_0_100 = sand_0_5.multiply(0.05).add(sand_5_15.multiply(0.1)).add(sand_15_30.multiply(0.15)).add(sand_30_60.multiply(0.3)).add(sand_60_100.multiply(0.4)).rename('sand_0_100')\n",
        "\n",
        "silt_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_0_5').first().rename('silt_0_5')\n",
        "silt_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_5_15').first().rename('silt_5_15')\n",
        "silt_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_15_30').first().rename('silt_15_30')\n",
        "silt_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_30_60').first().rename('silt_30_60')\n",
        "silt_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_60_100').first().rename('silt_60_100')\n",
        "silt_0_100 = silt_0_5.multiply(0.05).add(silt_5_15.multiply(0.1)).add(silt_15_30.multiply(0.15)).add(silt_30_60.multiply(0.3)).add(silt_60_100.multiply(0.4)).rename('silt_0_100')\n",
        "\n",
        "theta_r_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_r_mean').filterMetadata('system:index', 'equals', 'theta_r_0_5').first().rename('theta_r_0_5')\n",
        "theta_r_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_r_mean').filterMetadata('system:index', 'equals', 'theta_r_5_15').first().rename('theta_r_5_15')\n",
        "theta_r_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_r_mean').filterMetadata('system:index', 'equals', 'theta_r_15_30').first().rename('theta_r_15_30')\n",
        "theta_r_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_r_mean').filterMetadata('system:index', 'equals', 'theta_r_30_60').first().rename('theta_r_30_60')\n",
        "theta_r_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_r_mean').filterMetadata('system:index', 'equals', 'theta_r_60_100').first().rename('theta_r_60_100')\n",
        "theta_r_0_100 = theta_r_0_5.multiply(0.05).add(theta_r_5_15.multiply(0.1)).add(theta_r_15_30.multiply(0.15)).add(theta_r_30_60.multiply(0.3)).add(theta_r_60_100.multiply(0.4)).rename('theta_r_0_100')\n",
        "\n",
        "theta_s_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_s_mean').filterMetadata('system:index', 'equals', 'theta_s_0_5').first().rename('theta_s_0_5')\n",
        "theta_s_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_s_mean').filterMetadata('system:index', 'equals', 'theta_s_5_15').first().rename('theta_s_5_15')\n",
        "theta_s_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_s_mean').filterMetadata('system:index', 'equals', 'theta_s_15_30').first().rename('theta_s_15_30')\n",
        "theta_s_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_s_mean').filterMetadata('system:index', 'equals', 'theta_s_30_60').first().rename('theta_s_30_60')\n",
        "theta_s_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/theta_s_mean').filterMetadata('system:index', 'equals', 'theta_s_60_100').first().rename('theta_s_60_100')\n",
        "theta_s_0_100 = theta_s_0_5.multiply(0.05).add(theta_s_5_15.multiply(0.1)).add(theta_s_15_30.multiply(0.15)).add(theta_s_30_60.multiply(0.3)).add(theta_s_60_100.multiply(0.4)).rename('theta_s_0_100')\n",
        "\n",
        "alpha_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/alpha_mean').filterMetadata('system:index', 'equals', 'alpha_0_5').first().rename('alpha_0_5')\n",
        "alpha_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/alpha_mean').filterMetadata('system:index', 'equals', 'alpha_5_15').first().rename('alpha_5_15')\n",
        "alpha_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/alpha_mean').filterMetadata('system:index', 'equals', 'alpha_15_30').first().rename('alpha_15_30')\n",
        "alpha_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/alpha_mean').filterMetadata('system:index', 'equals', 'alpha_30_60').first().rename('alpha_30_60')\n",
        "alpha_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/alpha_mean').filterMetadata('system:index', 'equals', 'alpha_60_100').first().rename('alpha_60_100')\n",
        "alpha_0_100 = alpha_0_5.multiply(0.05).add(alpha_5_15.multiply(0.1)).add(alpha_15_30.multiply(0.15)).add(alpha_30_60.multiply(0.3)).add(alpha_60_100.multiply(0.4)).rename('alpha_0_100')\n",
        "\n",
        "polaris = bd_0_5.addBands(bd_5_15).addBands(bd_15_30).addBands(bd_30_60).addBands(bd_60_100).addBands(bd_0_100)\\\n",
        "          .addBands(clay_0_5).addBands(clay_5_15).addBands(clay_15_30).addBands(clay_30_60).addBands(clay_60_100).addBands(clay_0_100)\\\n",
        "          .addBands(ksat_0_5).addBands(ksat_5_15).addBands(ksat_15_30).addBands(ksat_30_60).addBands(ksat_60_100).addBands(ksat_0_100)\\\n",
        "          .addBands(n_0_5).addBands(n_5_15).addBands(n_15_30).addBands(n_30_60).addBands(n_60_100).addBands(n_0_100)\\\n",
        "          .addBands(sand_0_5).addBands(sand_5_15).addBands(sand_15_30).addBands(sand_30_60).addBands(sand_60_100).addBands(sand_0_100)\\\n",
        "          .addBands(silt_0_5).addBands(silt_5_15).addBands(silt_15_30).addBands(silt_30_60).addBands(silt_60_100).addBands(silt_0_100)\\\n",
        "          .addBands(theta_r_0_5).addBands(theta_r_5_15).addBands(theta_r_15_30).addBands(theta_r_30_60).addBands(theta_r_60_100).addBands(theta_r_0_100)\\\n",
        "          .addBands(theta_s_0_5).addBands(theta_s_5_15).addBands(theta_s_15_30).addBands(theta_s_30_60).addBands(theta_s_60_100).addBands(theta_s_0_100)\\\n",
        "          .addBands(alpha_0_5).addBands(alpha_5_15).addBands(alpha_15_30).addBands(alpha_30_60).addBands(alpha_60_100).addBands(alpha_0_100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A10sYM3k8bj_"
      },
      "outputs": [],
      "source": [
        "# @title Extract values from constant images using feature collection\n",
        "\n",
        "# extract pixel values of DEM using feature collection\n",
        "dem_values = dem.sampleRegions(collection=feature_collection,scale=100)\n",
        "# print(dem_values.getInfo())\n",
        "elevation_arr = np.array(dem_values.aggregate_array('elevation').getInfo())[:]\n",
        "aspect_arr = np.array(dem_values.aggregate_array('aspect').getInfo())[:]\n",
        "hillshade_arr = np.array(dem_values.aggregate_array('hillshade').getInfo())[:]\n",
        "slope_arr = np.array(dem_values.aggregate_array('slope').getInfo())[:]\n",
        "dem_idnum_arr = np.array(dem_values.aggregate_array('id_num').getInfo())[:]\n",
        "\n",
        "# extract pixel values of LC using feature collection\n",
        "LC_values = LC.sampleRegions(collection=feature_collection,scale=100)\n",
        "landcover_arr = np.array(LC_values.aggregate_array('landcover').getInfo())[:]\n",
        "LC_idnum_arr = np.array(LC_values.aggregate_array('id_num').getInfo())[:]\n",
        "\n",
        "# extract pixel values of Polaris soil using feature collection\n",
        "polaris_values = polaris.sampleRegions(collection=feature_collection,scale=100)\n",
        "ml_sand_0_5 = np.array(polaris_values.aggregate_array('sand_0_5').getInfo())[:]\n",
        "ml_sand_0_100 = np.array(polaris_values.aggregate_array('sand_0_100').getInfo())[:]\n",
        "ml_clay_0_5 = np.array(polaris_values.aggregate_array('clay_0_5').getInfo())[:]\n",
        "ml_clay_0_100 = np.array(polaris_values.aggregate_array('clay_0_100').getInfo())[:]\n",
        "ml_bd_0_5 = np.array(polaris_values.aggregate_array('bd_0_5').getInfo())[:]\n",
        "ml_bd_0_100 = np.array(polaris_values.aggregate_array('bd_0_100').getInfo())[:]\n",
        "ml_ksat_0_5 = np.array(polaris_values.aggregate_array('ksat_0_5').getInfo())[:]\n",
        "ml_ksat_0_100 = np.array(polaris_values.aggregate_array('ksat_0_100').getInfo())[:]\n",
        "\n",
        "#clay\n",
        "clay_0_5_arr = np.array(polaris_values.aggregate_array('clay_0_5').getInfo())[:]\n",
        "clay_5_15_arr = np.array(polaris_values.aggregate_array('clay_5_15').getInfo())[:]\n",
        "clay_15_30_arr = np.array(polaris_values.aggregate_array('clay_15_30').getInfo())[:]\n",
        "clay_30_60_arr = np.array(polaris_values.aggregate_array('clay_30_60').getInfo())[:]\n",
        "clay_60_100_arr = np.array(polaris_values.aggregate_array('clay_60_100').getInfo())[:]\n",
        "#sand\n",
        "sand_0_5_arr = np.array(polaris_values.aggregate_array('sand_0_5').getInfo())[:]\n",
        "sand_5_15_arr = np.array(polaris_values.aggregate_array('sand_5_15').getInfo())[:]\n",
        "sand_15_30_arr = np.array(polaris_values.aggregate_array('sand_15_30').getInfo())[:]\n",
        "sand_30_60_arr = np.array(polaris_values.aggregate_array('sand_30_60').getInfo())[:]\n",
        "sand_60_100_arr = np.array(polaris_values.aggregate_array('sand_60_100').getInfo())[:]\n",
        "#silt\n",
        "silt_0_5_arr = np.array(polaris_values.aggregate_array('silt_0_5').getInfo())[:]\n",
        "silt_5_15_arr = np.array(polaris_values.aggregate_array('silt_5_15').getInfo())[:]\n",
        "silt_15_30_arr = np.array(polaris_values.aggregate_array('silt_15_30').getInfo())[:]\n",
        "silt_30_60_arr = np.array(polaris_values.aggregate_array('silt_30_60').getInfo())[:]\n",
        "silt_60_100_arr = np.array(polaris_values.aggregate_array('silt_60_100').getInfo())[:]\n",
        "#theta_s\n",
        "theta_s_0_5_arr = np.array(polaris_values.aggregate_array('theta_s_0_5').getInfo())[:]\n",
        "theta_s_5_15_arr = np.array(polaris_values.aggregate_array('theta_s_5_15').getInfo())[:]\n",
        "theta_s_15_30_arr = np.array(polaris_values.aggregate_array('theta_s_15_30').getInfo())[:]\n",
        "theta_s_30_60_arr = np.array(polaris_values.aggregate_array('theta_s_30_60').getInfo())[:]\n",
        "theta_s_60_100_arr = np.array(polaris_values.aggregate_array('theta_s_60_100').getInfo())[:]\n",
        "#theta_r\n",
        "theta_r_0_5_arr = np.array(polaris_values.aggregate_array('theta_r_0_5').getInfo())[:]\n",
        "theta_r_5_15_arr = np.array(polaris_values.aggregate_array('theta_r_5_15').getInfo())[:]\n",
        "theta_r_15_30_arr = np.array(polaris_values.aggregate_array('theta_r_15_30').getInfo())[:]\n",
        "theta_r_30_60_arr = np.array(polaris_values.aggregate_array('theta_r_30_60').getInfo())[:]\n",
        "theta_r_60_100_arr = np.array(polaris_values.aggregate_array('theta_r_60_100').getInfo())[:]\n",
        "#bd\n",
        "bd_0_5_arr = np.array(polaris_values.aggregate_array('bd_0_5').getInfo())[:]\n",
        "bd_5_15_arr = np.array(polaris_values.aggregate_array('bd_5_15').getInfo())[:]\n",
        "bd_15_30_arr = np.array(polaris_values.aggregate_array('bd_15_30').getInfo())[:]\n",
        "bd_30_60_arr = np.array(polaris_values.aggregate_array('bd_30_60').getInfo())[:]\n",
        "bd_60_100_arr = np.array(polaris_values.aggregate_array('bd_60_100').getInfo())[:]\n",
        "#n\n",
        "n_0_5_arr = np.array(polaris_values.aggregate_array('n_0_5').getInfo())[:]\n",
        "n_5_15_arr = np.array(polaris_values.aggregate_array('n_5_15').getInfo())[:]\n",
        "n_15_30_arr = np.array(polaris_values.aggregate_array('n_15_30').getInfo())[:]\n",
        "n_30_60_arr = np.array(polaris_values.aggregate_array('n_30_60').getInfo())[:]\n",
        "n_60_100_arr = np.array(polaris_values.aggregate_array('n_60_100').getInfo())[:]\n",
        "#alpha\n",
        "alpha_0_5_arr = np.array(polaris_values.aggregate_array('alpha_0_5').getInfo())[:]\n",
        "alpha_5_15_arr = np.array(polaris_values.aggregate_array('alpha_5_15').getInfo())[:]\n",
        "alpha_15_30_arr = np.array(polaris_values.aggregate_array('alpha_15_30').getInfo())[:]\n",
        "alpha_30_60_arr = np.array(polaris_values.aggregate_array('alpha_30_60').getInfo())[:]\n",
        "alpha_60_100_arr = np.array(polaris_values.aggregate_array('alpha_60_100').getInfo())[:]\n",
        "#ksat\n",
        "ksat_0_5_arr = np.array(polaris_values.aggregate_array('ksat_0_5').getInfo())[:]\n",
        "ksat_5_15_arr = np.array(polaris_values.aggregate_array('ksat_5_15').getInfo())[:]\n",
        "ksat_15_30_arr = np.array(polaris_values.aggregate_array('ksat_15_30').getInfo())[:]\n",
        "ksat_30_60_arr = np.array(polaris_values.aggregate_array('ksat_30_60').getInfo())[:]\n",
        "ksat_60_100_arr = np.array(polaris_values.aggregate_array('ksat_60_100').getInfo())[:]\n",
        "\n",
        "polaris_idnum_arr = np.array(polaris_values.aggregate_array('id_num').getInfo())[:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpcrAcL2_KBJ"
      },
      "outputs": [],
      "source": [
        "# @title Convert units of soil properties for ML inputs\n",
        "ml_sand_0_5 = ml_sand_0_5 * 0.01   # convert to numerical\n",
        "ml_sand_0_100 = ml_sand_0_100 * 0.01\n",
        "ml_clay_0_5 = ml_clay_0_5 * 0.01\n",
        "ml_clay_0_100 = ml_clay_0_100 * 0.01\n",
        "\n",
        "ml_ksat_0_5 = np.power(10, ml_ksat_0_5)  # convert to cm/hr\n",
        "ml_ksat_0_100 = np.power(10, ml_ksat_0_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anP6wSbpihC0"
      },
      "source": [
        "## Add constant image values to dataframe and refine sites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coM40qvaiyOR"
      },
      "outputs": [],
      "source": [
        "# @title Define extract_points_data function: extract raster values using cooradates\n",
        "import osgeo.gdal as gdal\n",
        "# extract values from image\n",
        "def extract_points_data(file, lats, lons, undef=-9999.0):\n",
        "    # Open file and get geotransformation\n",
        "    ds = gdal.Open(file)\n",
        "    gt = ds.GetGeoTransform()\n",
        "    rb = ds.GetRasterBand(1)\n",
        "    nx = ds.RasterXSize\n",
        "    ny = ds.RasterYSize\n",
        "\n",
        "    # Compute ilats and ilons\n",
        "    ilons = np.round((np.array(lons) - (gt[0] + gt[1] / 2)) / gt[1]).astype(int)\n",
        "    ilats = np.round((np.array(lats) - (gt[3] + gt[5] / 2)) / gt[5]).astype(int)\n",
        "\n",
        "    # Extract data\n",
        "    values = []\n",
        "    for i in range(ilons.size):\n",
        "        ilon = int(ilons[i])\n",
        "        ilat = int(ilats[i])\n",
        "        if ((ilon < 0) | (ilon >= nx)) | ((ilat < 0) | (ilat >= ny)):\n",
        "            values.append(undef)\n",
        "        else:\n",
        "            values.append(rb.ReadAsArray(ilon, ilat, 1, 1)[0][0])\n",
        "\n",
        "    # ds.close()\n",
        "\n",
        "    return np.array(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYVbpa9OKRkF"
      },
      "outputs": [],
      "source": [
        "# @title Refine sites: only keep cropland based on landcover properties and networks\n",
        "df_constant = pd.DataFrame({\n",
        "    'id_num': dem_idnum_arr,\n",
        "    'Longitude': lon_list,\n",
        "    'Latitude': lat_list,\n",
        "    'elevation': elevation_arr,\n",
        "    'aspect': aspect_arr,\n",
        "    'hillshade': hillshade_arr,\n",
        "    'slope': slope_arr,\n",
        "    'landcover': landcover_arr,\n",
        "    'sand_0_5[0-1]': ml_sand_0_5,\n",
        "    'sand_0_100[0-1]': ml_sand_0_100,\n",
        "    'clay_0_5[0-1]': ml_clay_0_5,\n",
        "    'clay_0_100[0-1]': ml_clay_0_100,\n",
        "    'ksat_0_5[cm/hr]': ml_ksat_0_5,\n",
        "    'ksat_0_100[cm/hr]': ml_ksat_0_100,\n",
        "    'bd_0_5[g/cm3]': ml_bd_0_5,\n",
        "    'bd_0_100[g/cm3]': ml_bd_0_100,\n",
        "    'TWI':twi_list})\n",
        "\n",
        "\n",
        "refer_id_arr = df_constant['id_num'].to_numpy()\n",
        "refer_lon_arr = df_constant['Longitude'].to_numpy()\n",
        "refer_lat_arr = df_constant['Latitude'].to_numpy()\n",
        "\n",
        "refer_id_idx_dict = {}\n",
        "for i in range(len(refer_id_arr)):\n",
        "    refer_id_idx_dict[refer_id_arr[i]] = i\n",
        "\n",
        "\n",
        "len(refer_id_arr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_constant"
      ],
      "metadata": {
        "id": "13RlbetLxjyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mDIS0H0eNF6"
      },
      "outputs": [],
      "source": [
        "# @title New feature collection  - feature_collection_crop\n",
        "lons_sub = ee.List(df_constant['Longitude'].tolist())\n",
        "lats_sub = ee.List(df_constant['Latitude'].tolist())\n",
        "idnum_sub = ee.List(df_constant['id_num'].tolist())\n",
        "points = lons_sub.zip(lats_sub).zip(idnum_sub).map(lambda coords: ee.Feature(ee.Geometry.Point(ee.List(coords).get(0)),{'id_num': ee.List(coords).get(1)}))\n",
        "feature_collection_crop = ee.FeatureCollection(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irpDaBGt7m-W"
      },
      "source": [
        "## Define study area and period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRJHKFkR7mgg"
      },
      "outputs": [],
      "source": [
        "# prompt: get the boundary of feature_collection\n",
        "\n",
        "roi = feature_collection.geometry().bounds()\n",
        "\n",
        "\n",
        "start_date = '2016-01-01'\n",
        "end_date = '2022-12-31'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QL9ACxF7h2t"
      },
      "source": [
        "## Get ee imagecollections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5HQkf4L7ijj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Era5-land\n",
        "start_date_era5 = (datetime.strptime(start_date, '%Y-%m-%d')- timedelta(hours=120+24)).strftime('%Y-%m-%d')\n",
        "\n",
        "era5hour = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").select([\n",
        "    'temperature_2m',    # 2m air temperature, K\n",
        "    'total_evaporation_hourly',  # total evaporation, m\n",
        "    'total_precipitation_hourly',   # precipitation, m\n",
        "    'surface_solar_radiation_downwards_hourly',  # solar radiation, J\n",
        "    'surface_thermal_radiation_downwards_hourly',  # thermal radiation, J\n",
        "    'surface_pressure',  # atmospheric surface pressure, Pa\n",
        "    'u_component_of_wind_10m',  # u-direction wind speed, m/s\n",
        "    'v_component_of_wind_10m',  # v-direction wind speed, m/s\n",
        "    'dewpoint_temperature_2m'  # 2m dew point temperature, K\n",
        "    ]).filterBounds(roi).filterDate(start_date_era5, end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59lw-CMw2E8f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SMAP L3 (not used in this study)\n",
        "# retrieval_qual_flag_am  - quality flag 0 = Pass\n",
        "# def maskSMAPL3_am(image):\n",
        "#   qc = image.select('retrieval_qual_flag_am')\n",
        "#   good = qc.eq(0)\n",
        "#   mask = good.updateMask(good)\n",
        "#   return image.updateMask(mask).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "\n",
        "smapl3_am = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\").filterBounds(roi)\\\n",
        "                .filterDate(start_date,end_date).select(['soil_moisture_am', 'retrieval_qual_flag_am']) # .map(maskSMAPL3_am)\n",
        "\n",
        "\n",
        "# def maskSMAPL3_pm(image):\n",
        "#   qc = image.select('retrieval_qual_flag_pm')\n",
        "#   good = qc.eq(0)\n",
        "#   mask = good.updateMask(good)\n",
        "#   return image.updateMask(mask).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "smapl3_pm = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\").filterBounds(roi)\\\n",
        "                .filterDate(start_date,end_date).select(['soil_moisture_pm', 'retrieval_qual_flag_pm']) # .map(maskSMAPL3_pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN5LAdsW73HW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SMAP L4\n",
        "smap = ee.ImageCollection(\"NASA/SMAP/SPL4SMGP/007\")\\\n",
        "                .select(['sm_surface','sm_rootzone']).filterBounds(roi)\\\n",
        "                .filterDate(start_date,end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L--AZmxi8Kx-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title HLSL30\n",
        "\n",
        "###note: no scale factor for HLSL30 product.\n",
        "\n",
        "# a function to add other bands (NDVI, NDWI, Tasseled Cap Transformation components)\n",
        "# TCT coefficients are from paper: Baig et al. (2014)\n",
        "def add_other_bands_hls(img):\n",
        "  # Calculate NDVI and NDWI\n",
        "  ndvi = img.normalizedDifference(['B5', 'B4']).rename('NDVI')\n",
        "  ndwi = img.normalizedDifference(['B3', 'B5']).rename('NDWI')\n",
        "\n",
        "  greenness_exp = '-0.2941 * b(\"B2\") - 0.243 * b(\"B3\") - 0.5424 * b(\"B4\") +0.7276 * b(\"B5\") +0.0713 * b(\"B6\") - 0.1608 * b(\"B7\")';\n",
        "  greenness = img.expression(greenness_exp).rename('greenness')\n",
        "  brightness_exp = '0.3029 * b(\"B2\") + 0.2786 * b(\"B3\") + 0.4733 * b(\"B4\") + 0.5599 * b(\"B5\") + 0.508 * b(\"B6\") + 0.1872 * b(\"B7\")';\n",
        "  brightness = img.expression(brightness_exp).rename('brightness')\n",
        "  wetness_exp = '0.1511 * b(\"B2\") + 0.1973 * b(\"B3\") + 0.3283 * b(\"B4\") + 0.3407 * b(\"B5\") - 0.7117 * b(\"B6\") - 0.4559 * b(\"B7\")';\n",
        "  wetness = img.expression(wetness_exp).rename('wetness')\n",
        "  return img.addBands([ndvi, ndwi, greenness, brightness, wetness]).copyProperties(img, ['system:time_start'])\n",
        "\n",
        "\n",
        "def bitwiseExtract(img, fromBit, toBit):\n",
        "  maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
        "  mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
        "  return img.rightShift(fromBit).bitwiseAnd(mask)\n",
        "\n",
        "# remove low quality data\n",
        "def maskHLSL30(image):\n",
        "  qcDay = image.select('Fmask')\n",
        "  cloud = bitwiseExtract(qcDay, 1, 1).eq(0)\n",
        "  cloudshadow = bitwiseExtract(qcDay, 3, 3).eq(0)\n",
        "  snowice = bitwiseExtract(qcDay, 4, 4).eq(0)\n",
        "  water = bitwiseExtract(qcDay, 5, 5).eq(0)\n",
        "  aerosol = bitwiseExtract(qcDay, 6, 7).lte(2)\n",
        "  mask = cloud.And(cloudshadow).And(snowice)\n",
        "\n",
        "  return    image.updateMask(mask).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "\n",
        "HLSL30 = ee.ImageCollection(\"NASA/HLS/HLSL30/v002\").filterBounds(roi)\\\n",
        "    .filterDate(start_date, end_date).map(maskHLSL30) \\\n",
        "\t\t.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'SZA', 'SAA', 'VZA', 'VAA' ]) \\\n",
        "\t\t.map(add_other_bands_hls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8OPoVwv8NSC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sentinel-2\n",
        "# Sentinel-2 images\n",
        "\n",
        "def maskSentinel2(img):\n",
        "  cloudOpaqueBitMask = (1 << 10);\n",
        "  cloudCirrusMask = (1 << 11);\n",
        "  # Get the pixel QA band.\n",
        "  qa = img.select('QA60')\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudOpaqueBitMask).eq(0) \\\n",
        "                .And(qa.bitwiseAnd(cloudCirrusMask).eq(0))\n",
        "  return img.updateMask(mask).copyProperties(img, ['system:time_start'])                  #.multiply(0.0001).toFloat().copyProperties(img, ['mydate'])  # after applying updateMask(). all properties will be lost\n",
        "\n",
        "# a function to add other bands (NDVI, NDWI, Tasseled Cap Transformation components)\n",
        "# TCT coefficients are from paper: Shi and Xu (2019)\n",
        "def add_other_bands(img):\n",
        "  NDWI = img.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
        "  NDVI = img.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "\n",
        "  greenness_exp = '-0.3599 * b(\"B2\") - 0.3533 * b(\"B3\") - 0.4734 * b(\"B4\") + 0.6633 * b(\"B8\") - 0.0087 * b(\"B11\") - 0.2856 * b(\"B12\")'\n",
        "  greenness = img.expression(greenness_exp).rename('greenness')\n",
        "\n",
        "  brightness_exp = '0.3510 * b(\"B2\") + 0.3813 * b(\"B3\") + 0.3437 * b(\"B4\") + 0.7196 * b(\"B8\") + 0.2396 * b(\"B11\") + 0.1949 * b(\"B12\")'\n",
        "  brightness = img.expression(brightness_exp).rename('brightness')\n",
        "\n",
        "  wetness_exp = '0.2578 * b(\"B2\") + 0.2305 * b(\"B3\") + 0.0883 * b(\"B4\") + 0.1071 * b(\"B8\") - 0.7611 * b(\"B11\") - 0.5308 * b(\"B12\")'\n",
        "  wetness = img.expression(wetness_exp).rename('wetness')\n",
        "\n",
        "  return img.addBands([NDVI, NDWI, greenness, brightness, wetness]).copyProperties(img, ['system:time_start'])\n",
        "\n",
        "\n",
        "def add_date(img):\n",
        "  date_start = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd-HH')\n",
        "  return img.set('mydate', date_start)\n",
        "\n",
        "\n",
        "Sentinel2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
        "Sentinel2 = Sentinel2.filterDate(start_date, end_date).filterBounds(roi)\n",
        "Sentinel2 = Sentinel2.map(maskSentinel2)\n",
        "Sentinel2 = Sentinel2.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'])\n",
        "Sentinel2 = Sentinel2.map(add_other_bands)\n",
        "Sentinel2 = Sentinel2.map(add_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eol3scCY8Prg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sentinel-1\n",
        "\n",
        "def preprocess_vv(image):\n",
        "  vv_masked = image.updateMask(image.gt(-20).And(image.lt(-5)))\n",
        "  vv_filtered = vv_masked.convolve(ee.Kernel.gaussian(3))\n",
        "  return vv_filtered.copyProperties(image, ['system:time_start'])\n",
        "\n",
        "def preprocess_vh(image):\n",
        "  vh_masked = image.updateMask(image.gt(-30).And(image.lt(-10)))\n",
        "  vh_filtered = vh_masked.convolve(ee.Kernel.gaussian(3))\n",
        "  return vh_filtered.copyProperties(image, ['system:time_start'])\n",
        "\n",
        "sentinel1 = ee.ImageCollection('COPERNICUS/S1_GRD').filterDate(start_date, end_date).filterBounds(roi).sort('SLC_Processing_start')\n",
        "\n",
        "sentinel1_vvIw = sentinel1.filter(ee.Filter.eq('instrumentMode', 'IW')).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n",
        "sentinel1_vvIw = sentinel1_vvIw.map(preprocess_vv)\n",
        "\n",
        "sentinel1_vhIw = sentinel1.filter(ee.Filter.eq('instrumentMode', 'IW')).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')).select('VH')\n",
        "sentinel1_vhIw = sentinel1_vhIw.map(preprocess_vh)\n",
        "\n",
        "sentinel1_angleIw = sentinel1.filter(ee.Filter.eq('instrumentMode', 'IW')).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('angle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nnw_zfC8SA7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title MODIS VI (MOD13Q1)\n",
        "def maskMODISVI(image):\n",
        "  qcDay = image.select('DetailedQA')\n",
        "  qaMask = bitwiseExtract(qcDay, 0, 1).lte(1)\n",
        "  # dataQualityMask = bitwiseExtract(qcDay, 2, 5).lte(1)\n",
        "  # mask = qaMask.And(dataQualityMask)\n",
        "  return image.updateMask(qaMask)\n",
        "\n",
        "MOD13Q1VI = ee.ImageCollection(\"MODIS/061/MOD13Q1\").filterBounds(roi)\\\n",
        "    .filterDate(start_date, end_date).map(maskMODISVI).select(['NDVI', 'EVI']) #\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8zoldPaKbIt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title MODIS LAI (MCD15A3H)\n",
        "def maskMODISLAI(image):\n",
        "  qcDay = image.select('FparLai_QC')\n",
        "  qaMask = bitwiseExtract(qcDay, 0, 0).eq(0)\n",
        "  return image.updateMask(qaMask)\n",
        "\n",
        "MCD15A3HLAI = ee.ImageCollection(\"MODIS/061/MCD15A3H\").filterBounds(roi)\\\n",
        "    .filterDate(start_date, end_date).map(maskMODISLAI).select(['Lai']) #\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwJUQCxJsU3"
      },
      "source": [
        "## Extract data from image collection\n",
        "\n",
        "The following code is to extract pixel values from imagecollections. Due to the request limiation of google earth engine (e.g., each time no more than 5000 observations), the feature collection is divdided into samll groups, and the study period is divided into small periods, and the parallel processing strategy is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjy78933OUPB"
      },
      "outputs": [],
      "source": [
        "# initiate variables\n",
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "\n",
        "num_hours = int((end_datetime - start_datetime).total_seconds()/3600)\n",
        "\n",
        "smap_arr = np.zeros((2, num_hours, len(refer_id_arr)))\n",
        "era5hour_arr = np.zeros((8, num_hours, len(refer_id_arr)))\n",
        "sentinel1_arr = np.zeros((3, num_hours, len(refer_id_arr)))\n",
        "MOD13Q1VI_arr = np.zeros((2, num_hours, len(refer_id_arr)))\n",
        "MCD15A3HLAI_arr = np.zeros((1, num_hours, len(refer_id_arr)))\n",
        "HLSL30_arr = np.zeros((14, num_hours, len(refer_id_arr)))\n",
        "Sentinel2_arr = np.zeros((16, num_hours, len(refer_id_arr)))\n",
        "\n",
        "smap_arr[:] = np.nan\n",
        "era5hour_arr[:] = np.nan\n",
        "sentinel1_arr[:] = np.nan\n",
        "MOD13Q1VI_arr[:] = np.nan\n",
        "MCD15A3HLAI_arr[:] = np.nan\n",
        "HLSL30_arr[:] = np.nan\n",
        "Sentinel2_arr[:] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIhwYG2pau-k"
      },
      "outputs": [],
      "source": [
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "current_datetime = start_datetime\n",
        "tfirsts = []\n",
        "tlasts = []\n",
        "deltaa = 4000 # hours\n",
        "while current_datetime < end_datetime:\n",
        "  tfirsts.append(current_datetime)\n",
        "  tlasts.append( current_datetime + timedelta(hours=deltaa))\n",
        "  current_datetime += timedelta(hours=deltaa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOT5h7aPfW8I"
      },
      "outputs": [],
      "source": [
        "len(tfirsts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lSw4b_mpeKT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parallel processing\n",
        "import concurrent.futures\n",
        "def process_day(idx, feature_collection_crop):\n",
        "  print(f'is running the {idx}th group')\n",
        "  tfirst = tfirsts[idx]\n",
        "  tlast = tlasts[idx]\n",
        "\n",
        "  MOD13Q1VI_sub = MOD13Q1VI.filterDate(tfirst, tlast)\n",
        "  era5hour_sub = era5hour.filterDate(tfirst, tlast)\n",
        "  smap_sub = smap.filterDate(tfirst, tlast)\n",
        "  # smapl3_am_sub = smapl3_am.filterDate(tfirst, tlast)\n",
        "  # smapl3_pm_sub = smapl3_pm.filterDate(tfirst, tlast)\n",
        "  HLSL30_sub = HLSL30.filterDate(tfirst, tlast)\n",
        "  Sentinel2_sub = Sentinel2.filterDate(tfirst, tlast)\n",
        "  sentinel1_vvIw_sub = sentinel1_vvIw.filterDate(tfirst, tlast)\n",
        "  sentinel1_vhIw_sub = sentinel1_vhIw.filterDate(tfirst, tlast)\n",
        "  sentinel1_angleIw_sub = sentinel1_angleIw.filterDate(tfirst, tlast)\n",
        "\n",
        "  # MODIS VI\n",
        "  MOD13Q1VI_sampled_points = MOD13Q1VI_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = MOD13Q1VI_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  if 'id_num' in df.columns:\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = [int((datetime.strptime(systm.split('_')[0]+systm.split('_')[1]+systm.split('_')[2]+'15', '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      MOD13Q1VI_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'EVI']\n",
        "      MOD13Q1VI_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'NDVI']\n",
        "\n",
        "  # ERA5\n",
        "  era5hour_sampled_points = era5hour_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = era5hour_sampled_points.getInfo()['features']\n",
        "  # print(points_list)\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  df['time_idx'] = [int((datetime.strptime(systm[0:8]+systm[9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "  grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "  indices = grouped_df.index.tolist()\n",
        "  for idx in indices:\n",
        "    era5hour_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'total_precipitation_hourly']\n",
        "    era5hour_arr[1, idx[0], idx[1]] = max(0, grouped_df.loc[idx, 'total_evaporation_hourly'])\n",
        "    era5hour_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'surface_solar_radiation_downwards_hourly']\n",
        "    era5hour_arr[3, idx[0], idx[1]] = grouped_df.loc[idx, 'surface_thermal_radiation_downwards_hourly']\n",
        "    era5hour_arr[4, idx[0], idx[1]] = np.sqrt(grouped_df.loc[idx, 'u_component_of_wind_10m']**2 + grouped_df.loc[idx, 'v_component_of_wind_10m']**2)\n",
        "    era5hour_arr[5, idx[0], idx[1]] = grouped_df.loc[idx, 'temperature_2m']\n",
        "    era5hour_arr[6, idx[0], idx[1]] = grouped_df.loc[idx, 'dewpoint_temperature_2m']\n",
        "    era5hour_arr[7, idx[0], idx[1]] = grouped_df.loc[idx, 'surface_pressure']\n",
        "\n",
        "  # # SMAP L3 products\n",
        "  # smapl3_am_sampled_points = smapl3_am_sub.map(lambda image: image.sampleRegions(\n",
        "  #   collection=feature_collection_crop,\n",
        "  #   scale=100  # Adjust scale as needed\n",
        "  # ).map(lambda feature: feature.set('dt', image.date().format('YYYYMMddHH')))).flatten()\n",
        "  # # print(smapl3_am_sampled_points.getInfo())\n",
        "  # points_list = smapl3_am_sampled_points.getInfo()['features']\n",
        "  # # print(points_list)\n",
        "  # data = [point['properties'] for point in points_list]\n",
        "  # df = pd.DataFrame(data)\n",
        "  # print(df)\n",
        "  # df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  # df['time_idx'] = [int((datetime.strptime(timestr, '%Y%m%d%H') + timedelta(hours=6) - start_datetime).total_seconds()/3600) for timestr in df['dt'].tolist()]\n",
        "  # grouped_df = df.groupby(['time_idx', 'id_idx']).agg({'soil_moisture_am': 'mean'})\n",
        "  # indices = grouped_df.index.tolist()\n",
        "  # for idx in indices:\n",
        "  #   smapl3_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'soil_moisture_am']\n",
        "\n",
        "  # smapl3_pm_sampled_points = smapl3_pm_sub.map(lambda image: image.sampleRegions(\n",
        "  #   collection=feature_collection_crop,\n",
        "  #   scale=100  # Adjust scale as needed\n",
        "  # ).map(lambda feature: feature.set('dt', image.date().format('YYYYMMddHH')))).flatten()\n",
        "  # points_list = smapl3_pm_sampled_points.getInfo()['features']\n",
        "  # data = [point['properties'] for point in points_list]\n",
        "  # df = pd.DataFrame(data)\n",
        "  # df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  # df['time_idx'] = [int((datetime.strptime(timestr, '%Y%m%d%H') + timedelta(hours=18) - start_datetime).total_seconds()/3600) for timestr in df['dt'].tolist()]\n",
        "  # grouped_df = df.groupby(['time_idx', 'id_idx']).agg({'soil_moisture_pm': 'mean'})\n",
        "  # indices = grouped_df.index.tolist()\n",
        "  # for idx in indices:\n",
        "  #   smapl3_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'soil_moisture_pm']\n",
        "\n",
        "\n",
        "\n",
        "  # smap L4\n",
        "  smap_sampled_points = smap_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = smap_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  df['time_idx'] = [int((datetime.strptime(systm[0:8]+systm[9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "  grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "  indices = grouped_df.index.tolist()\n",
        "  for idx in indices:\n",
        "    smap_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'sm_surface']\n",
        "    smap_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'sm_rootzone']\n",
        "\n",
        "  # sentinel2\n",
        "  Sentinel2_sampled_points = Sentinel2_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = Sentinel2_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  if 'id_num' in df.columns:\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = [int((datetime.strptime(systm[0:8]+systm[9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      Sentinel2_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'B2']\n",
        "      Sentinel2_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'B3']\n",
        "      Sentinel2_arr[3, idx[0], idx[1]] = grouped_df.loc[idx, 'B4']\n",
        "      Sentinel2_arr[4, idx[0], idx[1]] = grouped_df.loc[idx, 'B5']\n",
        "      Sentinel2_arr[5, idx[0], idx[1]] = grouped_df.loc[idx, 'B6']\n",
        "      Sentinel2_arr[6, idx[0], idx[1]] = grouped_df.loc[idx, 'B7']\n",
        "      Sentinel2_arr[7, idx[0], idx[1]] = grouped_df.loc[idx, 'B8']\n",
        "      Sentinel2_arr[8, idx[0], idx[1]] = grouped_df.loc[idx, 'B8A']\n",
        "      Sentinel2_arr[9, idx[0], idx[1]] = grouped_df.loc[idx, 'B11']\n",
        "      Sentinel2_arr[10, idx[0], idx[1]] = grouped_df.loc[idx, 'B12']\n",
        "      Sentinel2_arr[11, idx[0], idx[1]] = grouped_df.loc[idx, 'NDVI']\n",
        "      Sentinel2_arr[12, idx[0], idx[1]] = grouped_df.loc[idx, 'NDWI']\n",
        "      Sentinel2_arr[13, idx[0], idx[1]] = grouped_df.loc[idx, 'greenness']\n",
        "      Sentinel2_arr[14, idx[0], idx[1]] = grouped_df.loc[idx, 'brightness']\n",
        "      Sentinel2_arr[15, idx[0], idx[1]] = grouped_df.loc[idx, 'wetness']\n",
        "      # General formula: 2.5 * (NIR - RED) / ((NIR + 6*RED - 7.5*BLUE) + 1)\n",
        "      numerator = Sentinel2_arr[7, idx[0], idx[1]]*0.0001-Sentinel2_arr[3, idx[0], idx[1]]*0.0001\n",
        "      denominator = Sentinel2_arr[7, idx[0], idx[1]]*0.0001+6*Sentinel2_arr[3, idx[0], idx[1]]*0.0001 - 7.5*Sentinel2_arr[1, idx[0], idx[1]]*0.0001 + 1\n",
        "      if denominator == 0:\n",
        "        Sentinel2_arr[0, idx[0], idx[1]] = np.nan\n",
        "      else:\n",
        "        Sentinel2_arr[0, idx[0], idx[1]] = 2.5 * (numerator / denominator)\n",
        "\n",
        "  # hlsl30\n",
        "  HLSL30_sampled_points = HLSL30_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = HLSL30_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  if 'id_num' in df.columns:\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = [int((datetime.strptime(systm[7:15]+systm[16:18], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      HLSL30_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'B2']\n",
        "      HLSL30_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'B3']\n",
        "      HLSL30_arr[3, idx[0], idx[1]] = grouped_df.loc[idx, 'B4']\n",
        "      HLSL30_arr[4, idx[0], idx[1]] = grouped_df.loc[idx, 'B5']\n",
        "      HLSL30_arr[5, idx[0], idx[1]] = grouped_df.loc[idx, 'B6']\n",
        "      HLSL30_arr[6, idx[0], idx[1]] = grouped_df.loc[idx, 'B7']\n",
        "      HLSL30_arr[7, idx[0], idx[1]] = grouped_df.loc[idx, 'B10']\n",
        "      HLSL30_arr[8, idx[0], idx[1]] = grouped_df.loc[idx, 'B11']\n",
        "      HLSL30_arr[9, idx[0], idx[1]] = grouped_df.loc[idx, 'NDVI']\n",
        "      HLSL30_arr[10, idx[0], idx[1]] = grouped_df.loc[idx, 'NDWI']\n",
        "      HLSL30_arr[11, idx[0], idx[1]] = grouped_df.loc[idx, 'greenness']\n",
        "      HLSL30_arr[12, idx[0], idx[1]] = grouped_df.loc[idx, 'brightness']\n",
        "      HLSL30_arr[13, idx[0], idx[1]] = grouped_df.loc[idx, 'wetness']\n",
        "      # General formula: 2.5 * (NIR - RED) / ((NIR + 6*RED - 7.5*BLUE) + 1)\n",
        "      numerator = HLSL30_arr[4, idx[0], idx[1]]-HLSL30_arr[3, idx[0], idx[1]]\n",
        "      denominator = HLSL30_arr[4, idx[0], idx[1]]+6*HLSL30_arr[3, idx[0], idx[1]]-7.5*HLSL30_arr[1, idx[0], idx[1]]+1\n",
        "      if denominator == 0:\n",
        "        HLSL30_arr[0, idx[0], idx[1]] = np.nan\n",
        "      else:\n",
        "        HLSL30_arr[0, idx[0], idx[1]] = 2.5 * (numerator / denominator)\n",
        "\n",
        "  # sentinel1\n",
        "  sentinel1_vvIw_sampled_points = sentinel1_vvIw_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = sentinel1_vvIw_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  if 'id_num' in df.columns:\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = [int((datetime.strptime(systm.split('_')[4][0:8]+systm.split('_')[4][9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      sentinel1_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'VV']\n",
        "\n",
        "  sentinel1_vhIw_sampled_points = sentinel1_vhIw_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = sentinel1_vhIw_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  if 'id_num' in df.columns:\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = [int((datetime.strptime(systm.split('_')[4][0:8]+systm.split('_')[4][9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      sentinel1_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'VH']\n",
        "\n",
        "  sentinel1_angleIw_sampled_points = sentinel1_angleIw_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  points_list = sentinel1_angleIw_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  if 'id_num' in df.columns:\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = [int((datetime.strptime(systm.split('_')[4][0:8]+systm.split('_')[4][9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      sentinel1_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'angle']\n",
        "\n",
        "  return idx\n",
        "\n",
        "if os.path.exists('era5hour_arr.npy'):\n",
        "  era5hour_arr = np.load('era5hour_arr.npy')\n",
        "  sentinel1_arr = np.load('sentinel1_arr.npy')\n",
        "  Sentinel2_arr = np.load('Sentinel2_arr.npy')\n",
        "  MOD13Q1VI_arr = np.load('MOD13Q1VI_arr.npy')\n",
        "  HLSL30_arr = np.load('HLSL30_arr.npy')\n",
        "  smap_arr = np.load('smap_arr.npy')\n",
        "else:\n",
        "  with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        results = list(executor.map(lambda idx: process_day(idx, feature_collection_crop), range(len(tfirsts)))) # len(tfirsts)\n",
        "  print('done')\n",
        "  np.save('era5hour_arr.npy', era5hour_arr)\n",
        "  np.save('sentinel1_arr.npy', sentinel1_arr)\n",
        "  np.save('Sentinel2_arr.npy', Sentinel2_arr)\n",
        "  np.save('MOD13Q1VI_arr.npy', MOD13Q1VI_arr)\n",
        "  np.save('HLSL30_arr.npy', HLSL30_arr)\n",
        "  np.save('smap_arr.npy', smap_arr)\n",
        "  # np.save('smapl3_arr.npy', smapl3_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxxeoZCKBEeX"
      },
      "outputs": [],
      "source": [
        "axis1 = np.where(np.isnan(era5hour_arr))[0]\n",
        "axis2 = np.where(np.isnan(era5hour_arr))[1]\n",
        "axis3 = np.where(np.isnan(era5hour_arr))[2]\n",
        "\n",
        "axis1_unique = np.unique(axis1)\n",
        "axis2_unique = np.unique(axis2)\n",
        "axis3_unique = np.unique(axis3)\n",
        "\n",
        "axis3_unique\n",
        "# array([255, 353, 354, 355, 356, 374, 375, 384])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IZ5oSDcn1Hi"
      },
      "source": [
        "## ML model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RUHFfXfLo7e"
      },
      "source": [
        "### Define XGBOOSTQUANTILE class\n",
        "This class is used to train XGBoot quantile regression models for uncertainty calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS6FIDUXoZbU"
      },
      "outputs": [],
      "source": [
        "# DEfine XGBOOSTQUANTILE class\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from joblib import dump, load\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def quantile_loss(y_true, y_pred, _alpha, _delta, _threshold, _var):\n",
        "    x = y_true - y_pred\n",
        "    grad = (x<(_alpha-1.0)*_delta)*(1.0-_alpha)- ((x>=(_alpha-1.0)*_delta)&\n",
        "                            (x<_alpha*_delta) )*x/_delta-_alpha*(x>_alpha*_delta)\n",
        "    hess = ((x>=(_alpha-1.0)*_delta)& (x<_alpha*_delta) )/_delta\n",
        "    _len = np.array([y_true]).size\n",
        "    var = (2*np.random.randint(2, size=_len)-1.0)*_var\n",
        "    grad = (np.abs(x)<_threshold )*grad - (np.abs(x)>=_threshold )*var\n",
        "    hess = (np.abs(x)<_threshold )*hess + (np.abs(x)>=_threshold )\n",
        "    return grad, hess\n",
        "\n",
        "class XGBOOSTQUANTILE(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, quant_alpha, quant_delta, quant_thres, quant_var,\n",
        "                 n_estimators=100, max_depth=3, reg_alpha=5., reg_lambda=1.0, gamma=0.5, learning_rate=0.05,\n",
        "                 min_child_weight=3, subsample=0.8, colsample_bytree = 0.8):\n",
        "        self.quant_alpha = quant_alpha\n",
        "        self.quant_delta = quant_delta\n",
        "        self.quant_thres = quant_thres\n",
        "        self.quant_var = quant_var\n",
        "        #xgboost parameters\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.reg_alpha= reg_alpha\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_child_weight = min_child_weight\n",
        "        self.subsample = subsample\n",
        "        self.colsample_bytree = colsample_bytree\n",
        "        #keep xgboost estimator in memory\n",
        "        self.clf = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.clf = XGBRegressor(\n",
        "                objective=partial(quantile_loss,\n",
        "                                    _alpha = self.quant_alpha,\n",
        "                                    _delta = self.quant_delta,\n",
        "                                    _threshold = self.quant_thres,\n",
        "                                    _var = self.quant_var),\n",
        "            n_estimators = self.n_estimators,\n",
        "            max_depth = self.max_depth,\n",
        "            reg_alpha =self.reg_alpha,\n",
        "            reg_lambda = self.reg_lambda,\n",
        "            gamma = self.gamma,\n",
        "            learning_rate=self.learning_rate,\n",
        "            min_child_weight = self.min_child_weight,\n",
        "            subsample = self.subsample,\n",
        "            colsample_bytree = self.colsample_bytree\n",
        "        )\n",
        "\n",
        "        self.clf.fit(X,y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.clf.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.clf.predict(X)\n",
        "        score = (self.quant_alpha-1.0)*(y-y_pred)*(y<y_pred)+self.quant_alpha*(y-y_pred)* (y>=y_pred)\n",
        "        score = 1./np.sum(score)\n",
        "        return score\n",
        "\n",
        "def Mergee(dict1, dict2):\n",
        "    res = {**dict1, **dict2}\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JXbDpQOLE7O"
      },
      "source": [
        "### Prepare ML inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_datetime = datetime(2016, 1, 1, 0, 0, 0)\n",
        "datetime_list = [start_datetime + timedelta(hours=i) for i in range(61344)]\n",
        "doy_list = np.array([dt.timetuple().tm_yday for dt in datetime_list])\n",
        "year_list = np.array([dt.year for dt in datetime_list])\n",
        "hour_list = np.array([dt.hour for dt in datetime_list])"
      ],
      "metadata": {
        "id": "iGDg4rSV-6ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interplate SMAP L4 and MODIS VIs into hourly\n",
        "MOD13Q1VI_arr[0] = pd.DataFrame(MOD13Q1VI_arr[0]).interpolate(method='nearest', axis=0)\\\n",
        "                    .bfill(axis=0).ffill(axis=0).to_numpy()\n",
        "MOD13Q1VI_arr[1] = pd.DataFrame(MOD13Q1VI_arr[1]).interpolate(method='nearest', axis=0)\\\n",
        "                    .bfill(axis=0).ffill(axis=0).to_numpy()\n",
        "smap_arr[0] = pd.DataFrame(smap_arr[0]).interpolate(method='nearest', axis=0)\\\n",
        "                    .bfill(axis=0).ffill(axis=0).to_numpy()\n",
        "smap_arr[1] = pd.DataFrame(smap_arr[1]).interpolate(method='nearest', axis=0)\\\n",
        "                    .bfill(axis=0).ffill(axis=0).to_numpy()\n",
        "MCD15A3HLAI_arr[0] = pd.DataFrame(MCD15A3HLAI_arr[0]).interpolate(method='nearest', axis=0)\\\n",
        "                    .bfill(axis=0).ffill(axis=0).to_numpy()"
      ],
      "metadata": {
        "id": "FYz7-ok--7T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate accumulated values of prior 120 hours of climate data\n",
        "weights = np.array(range(1, 121))\n",
        "weights = np.flip(weights)\n",
        "weights = 1.0/weights\n",
        "weights = weights/sum(weights)\n",
        "print(sum(weights))\n",
        "weights = weights.reshape(1, 120, 1)\n",
        "\n",
        "era5hour_lag_arr = np.zeros(era5hour_arr.shape)\n",
        "for row_idx in range(120, era5hour_arr.shape[1]):\n",
        "  era5hour_lag_arr[0:4, row_idx, :] = np.nansum(era5hour_arr[0:4, row_idx-119 : row_idx+1, :]*weights, axis=1)\n",
        "  era5hour_lag_arr[4:8, row_idx, :] = np.nanmean(era5hour_arr[4:8, row_idx-119 : row_idx+1, :], axis=1)\n",
        "\n",
        "era5hour_lag_arr[0:4, 0:120, :] = era5hour_arr[0:4, 0:120, :]\n",
        "era5hour_lag_arr[4:8, 0:120, :] = era5hour_arr[4:8, 0:120, :]\n",
        "\n",
        "era5hour_lag_arr[0,:, :] =  era5hour_lag_arr[0,:, :] * 100  # precipitation m, convert to cm\n",
        "era5hour_lag_arr[1,:, :] =  era5hour_lag_arr[1,:, :] * 100  # evaporation m, convert to cm"
      ],
      "metadata": {
        "id": "FW540-JP-90w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "era5hour_lag_arr[1,:, :]"
      ],
      "metadata": {
        "id": "Iofww51w_7tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypKcvoajUiP0"
      },
      "outputs": [],
      "source": [
        "# @title ML1S - ML1 for surface layer\n",
        "num_sites = len(refer_id_arr)\n",
        "ML1_sly_input = np.zeros((19, num_hours, num_sites))\n",
        "\n",
        "ML1_sly_input[0, :, :] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML1_sly_input[1, :, :] = np.tile(df_constant['Longitude'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[2, :, :] = np.tile(df_constant['Latitude'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[3, :, :] = np.tile(df_constant['landcover'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[4, :, :] = np.tile(df_constant['hillshade'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[5, :, :] = np.tile(df_constant['slope'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[6, :, :] = np.tile(df_constant['aspect'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[7, :, :] = np.tile(df_constant['elevation'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[8, :, :] = np.tile(df_constant['TWI'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[9, :, :] = np.tile(df_constant['sand_0_5[0-1]'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[10, :, :] = np.tile(df_constant['clay_0_5[0-1]'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[11, :, :] = np.tile(df_constant['bd_0_5[g/cm3]'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[12, :, :] = np.tile(df_constant['ksat_0_5[cm/hr]'].values, (num_hours, 1))[:]\n",
        "ML1_sly_input[13:15,:,:] = MOD13Q1VI_arr[:]\n",
        "ML1_sly_input[15:18,:,:] = sentinel1_arr[:]\n",
        "ML1_sly_input[18,:,:] = smap_arr[0,:,:]\n",
        "\n",
        "ML1_sly_input_all = np.concatenate((ML1_sly_input, era5hour_lag_arr), axis=0)\n",
        "mask = np.isnan(ML1_sly_input_all).any(axis=0)\n",
        "ii,jj,kk = ML1_sly_input_all.shape\n",
        "ML1_sly_input_all = ML1_sly_input_all.reshape(ii, jj*kk)\n",
        "mask = mask.reshape(jj*kk)\n",
        "\n",
        "ML1_sly_input_all = ML1_sly_input_all[:, ~mask].T\n",
        "\n",
        "ML1_sly_input_info = np.zeros((4, num_hours, num_sites))\n",
        "ML1_sly_input_info[0] = np.tile(refer_id_arr, (num_hours, 1))[:]\n",
        "ML1_sly_input_info[1] = np.tile(year_list, ( num_sites,1)).T\n",
        "ML1_sly_input_info[2] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML1_sly_input_info[3] = np.tile(hour_list, ( num_sites,1)).T\n",
        "ML1_sly_input_info_all = ML1_sly_input_info.reshape(4, jj*kk)\n",
        "ML1_sly_input_info_all = ML1_sly_input_info_all[:, ~mask].T\n",
        "\n",
        "del ML1_sly_input\n",
        "del ML1_sly_input_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6XMOIqvUiP8"
      },
      "outputs": [],
      "source": [
        "# @title ML1R - ML1 for Rootzone\n",
        "num_sites = len(refer_id_arr)\n",
        "ML1_rly_input = np.zeros((19, num_hours, num_sites))\n",
        "\n",
        "ML1_rly_input[0, :, :] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML1_rly_input[1, :, :] = np.tile(df_constant['Longitude'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[2, :, :] = np.tile(df_constant['Latitude'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[3, :, :] = np.tile(df_constant['landcover'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[4, :, :] = np.tile(df_constant['hillshade'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[5, :, :] = np.tile(df_constant['slope'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[6, :, :] = np.tile(df_constant['aspect'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[7, :, :] = np.tile(df_constant['elevation'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[8, :, :] = np.tile(df_constant['TWI'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[9, :, :] = np.tile(df_constant['sand_0_100[0-1]'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[10, :, :] = np.tile(df_constant['clay_0_100[0-1]'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[11, :, :] = np.tile(df_constant['bd_0_100[g/cm3]'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[12, :, :] = np.tile(df_constant['ksat_0_100[cm/hr]'].values, (num_hours, 1))[:]\n",
        "ML1_rly_input[13:15,:,:] = MOD13Q1VI_arr[:]\n",
        "ML1_rly_input[15:18,:,:] = sentinel1_arr[:]\n",
        "ML1_rly_input[18,:,:] = smap_arr[1,:,:]\n",
        "\n",
        "ML1_rly_input_all = np.concatenate((ML1_rly_input, era5hour_lag_arr), axis=0)\n",
        "mask = np.isnan(ML1_rly_input_all).any(axis=0)\n",
        "ii,jj,kk = ML1_rly_input_all.shape\n",
        "ML1_rly_input_all = ML1_rly_input_all.reshape(ii, jj*kk)\n",
        "mask = mask.reshape(jj*kk)\n",
        "\n",
        "ML1_rly_input_all = ML1_rly_input_all[:, ~mask].T\n",
        "\n",
        "ML1_rly_input_info = np.zeros((4, num_hours, num_sites))\n",
        "ML1_rly_input_info[0] = np.tile(refer_id_arr, (num_hours, 1))[:]\n",
        "ML1_rly_input_info[1] = np.tile(year_list, ( num_sites,1)).T\n",
        "ML1_rly_input_info[2] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML1_rly_input_info[3] = np.tile(hour_list, ( num_sites,1)).T\n",
        "ML1_rly_input_info_all = ML1_rly_input_info.reshape(4, jj*kk)\n",
        "ML1_rly_input_info_all = ML1_rly_input_info_all[:, ~mask].T\n",
        "\n",
        "del ML1_rly_input\n",
        "del ML1_rly_input_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo7lHBjLUiP9"
      },
      "outputs": [],
      "source": [
        "# @title ML2S - ML2 for surface layer\n",
        "num_sites = len(refer_id_arr)\n",
        "ML2_sly_input = np.zeros((30, num_hours, num_sites))\n",
        "\n",
        "\n",
        "ML2_sly_input[0, :, :] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML2_sly_input[1, :, :] = np.tile(df_constant['Longitude'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[2, :, :] = np.tile(df_constant['Latitude'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[3, :, :] = np.tile(df_constant['landcover'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[4, :, :] = np.tile(df_constant['hillshade'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[5, :, :] = np.tile(df_constant['slope'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[6, :, :] = np.tile(df_constant['aspect'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[7, :, :] = np.tile(df_constant['elevation'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[8, :, :] = np.tile(df_constant['TWI'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[9, :, :] = np.tile(df_constant['sand_0_5[0-1]'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[10, :, :] = np.tile(df_constant['clay_0_5[0-1]'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[11, :, :] = np.tile(df_constant['bd_0_5[g/cm3]'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[12, :, :] = np.tile(df_constant['ksat_0_5[cm/hr]'].values, (num_hours, 1))[:]\n",
        "ML2_sly_input[13:29,:,:] = Sentinel2_arr[:]\n",
        "ML2_sly_input[29,:,:] = smap_arr[0,:,:]\n",
        "\n",
        "ML2_sly_input_all = np.concatenate((ML2_sly_input, era5hour_lag_arr), axis=0)\n",
        "mask = np.isnan(ML2_sly_input_all).any(axis=0)\n",
        "ii,jj,kk = ML2_sly_input_all.shape\n",
        "ML2_sly_input_all = ML2_sly_input_all.reshape(ii, jj*kk)\n",
        "mask = mask.reshape(jj*kk)\n",
        "\n",
        "ML2_sly_input_all = ML2_sly_input_all[:, ~mask].T\n",
        "\n",
        "ML2_sly_input_info = np.zeros((4, num_hours, num_sites))\n",
        "ML2_sly_input_info[0] = np.tile(refer_id_arr, (num_hours, 1))[:]\n",
        "ML2_sly_input_info[1] = np.tile(year_list, ( num_sites,1)).T\n",
        "ML2_sly_input_info[2] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML2_sly_input_info[3] = np.tile(hour_list, ( num_sites,1)).T\n",
        "ML2_sly_input_info_all = ML2_sly_input_info.reshape(4, jj*kk)\n",
        "ML2_sly_input_info_all = ML2_sly_input_info_all[:, ~mask].T\n",
        "\n",
        "del ML2_sly_input\n",
        "del ML2_sly_input_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUIoWmjVUiP9"
      },
      "outputs": [],
      "source": [
        "# @title ML2R - ML2 for rootzone\n",
        "num_sites = len(refer_id_arr)\n",
        "ML2_rly_input = np.zeros((30, num_hours, num_sites))\n",
        "\n",
        "ML2_rly_input[0, :, :] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML2_rly_input[1, :, :] = np.tile(df_constant['Longitude'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[2, :, :] = np.tile(df_constant['Latitude'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[3, :, :] = np.tile(df_constant['landcover'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[4, :, :] = np.tile(df_constant['hillshade'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[5, :, :] = np.tile(df_constant['slope'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[6, :, :] = np.tile(df_constant['aspect'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[7, :, :] = np.tile(df_constant['elevation'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[8, :, :] = np.tile(df_constant['TWI'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[9, :, :] = np.tile(df_constant['sand_0_100[0-1]'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[10, :, :] = np.tile(df_constant['clay_0_100[0-1]'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[11, :, :] = np.tile(df_constant['bd_0_100[g/cm3]'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[12, :, :] = np.tile(df_constant['ksat_0_100[cm/hr]'].values, (num_hours, 1))[:]\n",
        "ML2_rly_input[13:29,:,:] = Sentinel2_arr[:]\n",
        "ML2_rly_input[29,:,:] = smap_arr[1,:,:]\n",
        "\n",
        "ML2_rly_input_all = np.concatenate((ML2_rly_input, era5hour_lag_arr), axis=0)\n",
        "mask = np.isnan(ML2_rly_input_all).any(axis=0)\n",
        "ii,jj,kk = ML2_rly_input_all.shape\n",
        "ML2_rly_input_all = ML2_rly_input_all.reshape(ii, jj*kk)\n",
        "mask = mask.reshape(jj*kk)\n",
        "\n",
        "ML2_rly_input_all = ML2_rly_input_all[:, ~mask].T\n",
        "\n",
        "ML2_rly_input_info = np.zeros((4, num_hours, num_sites))\n",
        "ML2_rly_input_info[0] = np.tile(refer_id_arr, (num_hours, 1))[:]\n",
        "ML2_rly_input_info[1] = np.tile(year_list, ( num_sites,1)).T\n",
        "ML2_rly_input_info[2] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML2_rly_input_info[3] = np.tile(hour_list, ( num_sites,1)).T\n",
        "ML2_rly_input_info_all = ML2_rly_input_info.reshape(4, jj*kk)\n",
        "ML2_rly_input_info_all = ML2_rly_input_info_all[:, ~mask].T\n",
        "\n",
        "del ML2_rly_input\n",
        "del ML2_rly_input_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHwCO204UiP9"
      },
      "outputs": [],
      "source": [
        "# @title ML3S - ML3 for surface layer\n",
        "num_sites = len(refer_id_arr)\n",
        "ML3_sly_input = np.zeros((28, num_hours, num_sites))\n",
        "\n",
        "\n",
        "ML3_sly_input[0, :, :] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML3_sly_input[1, :, :] = np.tile(df_constant['Longitude'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[2, :, :] = np.tile(df_constant['Latitude'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[3, :, :] = np.tile(df_constant['landcover'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[4, :, :] = np.tile(df_constant['hillshade'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[5, :, :] = np.tile(df_constant['slope'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[6, :, :] = np.tile(df_constant['aspect'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[7, :, :] = np.tile(df_constant['elevation'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[8, :, :] = np.tile(df_constant['TWI'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[9, :, :] = np.tile(df_constant['sand_0_5[0-1]'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[10, :, :] = np.tile(df_constant['clay_0_5[0-1]'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[11, :, :] = np.tile(df_constant['bd_0_5[g/cm3]'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[12, :, :] = np.tile(df_constant['ksat_0_5[cm/hr]'].values, (num_hours, 1))[:]\n",
        "ML3_sly_input[13:27,:,:] = HLSL30_arr[:]\n",
        "ML3_sly_input[27,:,:] = smap_arr[0,:,:]\n",
        "\n",
        "ML3_sly_input_all = np.concatenate((ML3_sly_input, era5hour_lag_arr), axis=0)\n",
        "mask = np.isnan(ML3_sly_input_all).any(axis=0)\n",
        "ii,jj,kk = ML3_sly_input_all.shape\n",
        "ML3_sly_input_all = ML3_sly_input_all.reshape(ii, jj*kk)\n",
        "mask = mask.reshape(jj*kk)\n",
        "\n",
        "ML3_sly_input_all = ML3_sly_input_all[:, ~mask].T\n",
        "\n",
        "ML3_sly_input_info = np.zeros((4, num_hours, num_sites))\n",
        "ML3_sly_input_info[0] = np.tile(refer_id_arr, (num_hours, 1))[:]\n",
        "ML3_sly_input_info[1] = np.tile(year_list, ( num_sites,1)).T\n",
        "ML3_sly_input_info[2] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML3_sly_input_info[3] = np.tile(hour_list, ( num_sites,1)).T\n",
        "ML3_sly_input_info_all = ML3_sly_input_info.reshape(4, jj*kk)\n",
        "ML3_sly_input_info_all = ML3_sly_input_info_all[:, ~mask].T\n",
        "\n",
        "del ML3_sly_input\n",
        "del ML3_sly_input_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hFXi1kIUiP9"
      },
      "outputs": [],
      "source": [
        "# @title ML3R - ML3 for rootzone\n",
        "num_sites = len(refer_id_arr)\n",
        "ML3_rly_input = np.zeros((28, num_hours, num_sites))\n",
        "\n",
        "ML3_rly_input[0, :, :] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML3_rly_input[1, :, :] = np.tile(df_constant['Longitude'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[2, :, :] = np.tile(df_constant['Latitude'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[3, :, :] = np.tile(df_constant['landcover'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[4, :, :] = np.tile(df_constant['hillshade'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[5, :, :] = np.tile(df_constant['slope'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[6, :, :] = np.tile(df_constant['aspect'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[7, :, :] = np.tile(df_constant['elevation'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[8, :, :] = np.tile(df_constant['TWI'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[9, :, :] = np.tile(df_constant['sand_0_100[0-1]'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[10, :, :] = np.tile(df_constant['clay_0_100[0-1]'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[11, :, :] = np.tile(df_constant['bd_0_100[g/cm3]'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[12, :, :] = np.tile(df_constant['ksat_0_100[cm/hr]'].values, (num_hours, 1))[:]\n",
        "ML3_rly_input[13:27,:,:] = HLSL30_arr[:]\n",
        "ML3_rly_input[27,:,:] = smap_arr[1,:,:]\n",
        "\n",
        "ML3_rly_input_all = np.concatenate((ML3_rly_input, era5hour_lag_arr), axis=0)\n",
        "mask = np.isnan(ML3_rly_input_all).any(axis=0)\n",
        "ii,jj,kk = ML3_rly_input_all.shape\n",
        "ML3_rly_input_all = ML3_rly_input_all.reshape(ii, jj*kk)\n",
        "mask = mask.reshape(jj*kk)\n",
        "\n",
        "ML3_rly_input_all = ML3_rly_input_all[:, ~mask].T\n",
        "\n",
        "ML3_rly_input_info = np.zeros((4, num_hours, num_sites))\n",
        "ML3_rly_input_info[0] = np.tile(refer_id_arr, (num_hours, 1))[:]\n",
        "ML3_rly_input_info[1] = np.tile(year_list, ( num_sites,1)).T\n",
        "ML3_rly_input_info[2] = np.tile(doy_list, ( num_sites,1)).T\n",
        "ML3_rly_input_info[3] = np.tile(hour_list, ( num_sites,1)).T\n",
        "ML3_rly_input_info_all = ML3_rly_input_info.reshape(4, jj*kk)\n",
        "ML3_rly_input_info_all = ML3_rly_input_info_all[:, ~mask].T\n",
        "\n",
        "del ML3_rly_input\n",
        "del ML3_rly_input_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6wgGbJ5VxNe"
      },
      "outputs": [],
      "source": [
        "print('ml1, sly, X:      ', ML1_sly_input_all.shape)\n",
        "print('ml1, sly, X_info: ', ML1_sly_input_info_all.shape)\n",
        "print('ml1, rly, X:      ', ML1_rly_input_all.shape)\n",
        "print('ml1, rly, X_info: ', ML1_rly_input_info_all.shape)\n",
        "print()\n",
        "print('ml2, sly, X:      ', ML2_sly_input_all.shape)\n",
        "print('ml2, sly, X_info: ', ML2_sly_input_info_all.shape)\n",
        "print('ml2, rly, X:      ', ML2_rly_input_all.shape)\n",
        "print('ml2, rly, X_info: ', ML2_rly_input_info_all.shape)\n",
        "print()\n",
        "print('ml3, sly, X:      ', ML3_sly_input_all.shape)\n",
        "print('ml3, sly, X_info: ', ML3_sly_input_info_all.shape)\n",
        "print('ml3, rly, X:      ', ML3_rly_input_all.shape)\n",
        "print('ml3, rly, X_info: ', ML3_rly_input_info_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAfYYW-fVxNf"
      },
      "outputs": [],
      "source": [
        "# remove landcover column:\n",
        "ML1_sly_input_all = np.delete(ML1_sly_input_all, 3, axis=1)\n",
        "ML1_rly_input_all = np.delete(ML1_rly_input_all, 3, axis=1)\n",
        "ML2_sly_input_all = np.delete(ML2_sly_input_all, 3, axis=1)\n",
        "ML2_rly_input_all = np.delete(ML2_rly_input_all, 3, axis=1)\n",
        "ML3_sly_input_all = np.delete(ML3_sly_input_all, 3, axis=1)\n",
        "ML3_rly_input_all = np.delete(ML3_rly_input_all, 3, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR2dUZMpVxNf"
      },
      "outputs": [],
      "source": [
        "print('ml1, sly, X:      ', ML1_sly_input_all.shape)\n",
        "print('ml1, sly, X_info: ', ML1_sly_input_info_all.shape)\n",
        "print('ml1, rly, X:      ', ML1_rly_input_all.shape)\n",
        "print('ml1, rly, X_info: ', ML1_rly_input_info_all.shape)\n",
        "print()\n",
        "print('ml2, sly, X:      ', ML2_sly_input_all.shape)\n",
        "print('ml2, sly, X_info: ', ML2_sly_input_info_all.shape)\n",
        "print('ml2, rly, X:      ', ML2_rly_input_all.shape)\n",
        "print('ml2, rly, X_info: ', ML2_rly_input_info_all.shape)\n",
        "print()\n",
        "print('ml3, sly, X:      ', ML3_sly_input_all.shape)\n",
        "print('ml3, sly, X_info: ', ML3_sly_input_info_all.shape)\n",
        "print('ml3, rly, X:      ', ML3_rly_input_all.shape)\n",
        "print('ml3, rly, X_info: ', ML3_rly_input_info_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ML1_sly_input_all[0]"
      ],
      "metadata": {
        "id": "JiZY_b5wGhlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load trained ML models\n",
        "\n",
        "ML1_sly_xgbq_mean = joblib.load('./StartFolder/TrainedML/ML1_sly_xgbq_mean.joblib')\n",
        "ML1_sly_xgbq_up = joblib.load('./StartFolder/TrainedML/ML1_sly_xgbq_up.joblib')\n",
        "ML1_sly_xgbq_down = joblib.load('./StartFolder/TrainedML/ML1_sly_xgbq_low.joblib')\n",
        "ML1_sly_scaler = joblib.load('./StartFolder/TrainedML/ML1_sly_scaler.joblib')\n",
        "\n",
        "ML1_rly_xgbq_mean = joblib.load('./StartFolder/TrainedML/ML1_rly_xgbq_mean.joblib')\n",
        "ML1_rly_xgbq_up = joblib.load('./StartFolder/TrainedML/ML1_rly_xgbq_up.joblib')\n",
        "ML1_rly_xgbq_down = joblib.load('./StartFolder/TrainedML/ML1_rly_xgbq_low.joblib')\n",
        "ML1_rly_scaler = joblib.load('./StartFolder/TrainedML/ML1_rly_scaler.joblib')\n",
        "\n",
        "ML2_sly_xgbq_mean = joblib.load('./StartFolder/TrainedML/ML2_sly_xgbq_mean.joblib')\n",
        "ML2_sly_xgbq_up = joblib.load('./StartFolder/TrainedML/ML2_sly_xgbq_up.joblib')\n",
        "ML2_sly_xgbq_down = joblib.load('./StartFolder/TrainedML/ML2_sly_xgbq_low.joblib')\n",
        "ML2_sly_scaler = joblib.load('./StartFolder/TrainedML/ML2_sly_scaler.joblib')\n",
        "\n",
        "ML2_rly_xgbq_mean = joblib.load('./StartFolder/TrainedML/ML2_rly_xgbq_mean.joblib')\n",
        "ML2_rly_xgbq_up = joblib.load('./StartFolder/TrainedML/ML2_rly_xgbq_up.joblib')\n",
        "ML2_rly_xgbq_down = joblib.load('./StartFolder/TrainedML/ML2_rly_xgbq_low.joblib')\n",
        "ML2_rly_scaler = joblib.load('./StartFolder/TrainedML/ML2_rly_scaler.joblib')\n",
        "\n",
        "ML3_sly_xgbq_mean = joblib.load('./StartFolder/TrainedML/ML3_sly_xgbq_mean.joblib')\n",
        "ML3_sly_xgbq_up = joblib.load('./StartFolder/TrainedML/ML3_sly_xgbq_up.joblib')\n",
        "ML3_sly_xgbq_down = joblib.load('./StartFolder/TrainedML/ML3_sly_xgbq_low.joblib')\n",
        "ML3_sly_scaler = joblib.load('./StartFolder/TrainedML/ML3_sly_scaler.joblib')\n",
        "\n",
        "ML3_rly_xgbq_mean = joblib.load('./StartFolder/TrainedML/ML3_rly_xgbq_mean.joblib')\n",
        "ML3_rly_xgbq_up = joblib.load('./StartFolder/TrainedML/ML3_rly_xgbq_up.joblib')\n",
        "ML3_rly_xgbq_down = joblib.load('./StartFolder/TrainedML/ML3_rly_xgbq_low.joblib')\n",
        "ML3_rly_scaler = joblib.load('./StartFolder/TrainedML/ML3_rly_scaler.joblib')"
      ],
      "metadata": {
        "id": "Px9kArhh6N2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ML1_sly_input_all_scaled = ML1_sly_scaler.transform(ML1_sly_input_all)\n",
        "ML2_sly_input_all_scaled = ML2_sly_scaler.transform(ML2_sly_input_all)\n",
        "ML3_sly_input_all_scaled = ML3_sly_scaler.transform(ML3_sly_input_all)\n",
        "\n",
        "ML1_rly_input_all_scaled = ML1_rly_scaler.transform(ML1_rly_input_all)\n",
        "ML2_rly_input_all_scaled = ML2_rly_scaler.transform(ML2_rly_input_all)\n",
        "ML3_rly_input_all_scaled = ML3_rly_scaler.transform(ML3_rly_input_all)"
      ],
      "metadata": {
        "id": "RmdaV6Sx8yuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ML1_sly_y_hat = ML1_sly_xgbq_mean.predict(ML1_sly_input_all_scaled)\n",
        "ML1_sly_y_up_hat = ML1_sly_xgbq_up.predict(ML1_sly_input_all_scaled)\n",
        "ML1_sly_y_low_hat = ML1_sly_xgbq_down.predict(ML1_sly_input_all_scaled)\n",
        "ML1_sly_y_pred_all_arr = np.column_stack((ML1_sly_input_info_all, ML1_sly_y_hat, ML1_sly_y_up_hat, ML1_sly_y_low_hat))\n",
        "\n",
        "ML2_sly_y_hat = ML2_sly_xgbq_mean.predict(ML2_sly_input_all_scaled)\n",
        "ML2_sly_y_up_hat = ML2_sly_xgbq_up.predict(ML2_sly_input_all_scaled)\n",
        "ML2_sly_y_low_hat = ML2_sly_xgbq_down.predict(ML2_sly_input_all_scaled)\n",
        "ML2_sly_y_pred_all_arr = np.column_stack((ML2_sly_input_info_all, ML2_sly_y_hat, ML2_sly_y_up_hat, ML2_sly_y_low_hat))\n",
        "\n",
        "ML3_sly_y_hat = ML3_sly_xgbq_mean.predict(ML3_sly_input_all_scaled)\n",
        "ML3_sly_y_up_hat = ML3_sly_xgbq_up.predict(ML3_sly_input_all_scaled)\n",
        "ML3_sly_y_low_hat = ML3_sly_xgbq_down.predict(ML3_sly_input_all_scaled)\n",
        "ML3_sly_y_pred_all_arr = np.column_stack((ML3_sly_input_info_all, ML3_sly_y_hat, ML3_sly_y_up_hat, ML3_sly_y_low_hat))\n",
        "\n",
        "ML1_rly_y_hat = ML1_rly_xgbq_mean.predict(ML1_rly_input_all_scaled)\n",
        "ML1_rly_y_up_hat = ML1_rly_xgbq_up.predict(ML1_rly_input_all_scaled)\n",
        "ML1_rly_y_low_hat = ML1_rly_xgbq_down.predict(ML1_rly_input_all_scaled)\n",
        "ML1_rly_y_pred_all_arr = np.column_stack((ML1_rly_input_info_all, ML1_rly_y_hat, ML1_rly_y_up_hat, ML1_rly_y_low_hat))\n",
        "\n",
        "ML2_rly_y_hat = ML2_rly_xgbq_mean.predict(ML2_rly_input_all_scaled)\n",
        "ML2_rly_y_up_hat = ML2_rly_xgbq_up.predict(ML2_rly_input_all_scaled)\n",
        "ML2_rly_y_low_hat = ML2_rly_xgbq_down.predict(ML2_rly_input_all_scaled)\n",
        "ML2_rly_y_pred_all_arr = np.column_stack((ML2_rly_input_info_all, ML2_rly_y_hat, ML2_rly_y_up_hat, ML2_rly_y_low_hat))\n",
        "\n",
        "ML3_rly_y_hat = ML3_rly_xgbq_mean.predict(ML3_rly_input_all_scaled)\n",
        "ML3_rly_y_up_hat = ML3_rly_xgbq_up.predict(ML3_rly_input_all_scaled)\n",
        "ML3_rly_y_low_hat = ML3_rly_xgbq_down.predict(ML3_rly_input_all_scaled)\n",
        "ML3_rly_y_pred_all_arr = np.column_stack((ML3_rly_input_info_all, ML3_rly_y_hat, ML3_rly_y_up_hat, ML3_rly_y_low_hat))\n"
      ],
      "metadata": {
        "id": "dVGbhq4pKyNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reorgnize ML results - perform full CDF matching\n",
        "def empirical_cdf(data):\n",
        "    ranks = rankdata(data, method='average')  # Rank the data\n",
        "    return ranks / len(data)  # Normalize by the number of points\n",
        "\n",
        "def cdf_matching(source, target):\n",
        "    sorted_target = np.sort(target)  # Target distribution sorted\n",
        "    sorted_target_ranks = rankdata(sorted_target, method='average')\n",
        "    sorted_target_quantiles = sorted_target_ranks / len(sorted_target)\n",
        "    ranks = rankdata(source, method='average')  # Source data ranks\n",
        "    percentiles = ranks / len(source)  # Percentile positions of source data\n",
        "    matched_values = np.interp(percentiles, sorted_target_quantiles, sorted_target)  # Interpolation\n",
        "    return matched_values\n",
        "\n",
        "\n",
        "def correct_2_ML3(Sentinel_sl_arr, sentinel2_sl_arr, hls_sl_arr, Sentinel_rly_arr, sentinel2_rly_arr, hls_rly_arr):\n",
        "    Sentinel_sl_arr_ = Sentinel_sl_arr.copy()\n",
        "    sentinel2_sl_arr_ = sentinel2_sl_arr.copy()\n",
        "    hls_sl_arr_ = hls_sl_arr.copy()\n",
        "    Sentinel_rly_arr_ = Sentinel_rly_arr.copy()\n",
        "    sentinel2_rly_arr_ = sentinel2_rly_arr.copy()\n",
        "    hls_rly_arr_ = hls_rly_arr.copy()\n",
        "    # Xid_all, Xyear_all, Xdoy_all, Xutc_all, y_hat, y_up_hat, y_median_hat, y_low_hat\n",
        "    # surface layer\n",
        "    ids = list(set(sentinel2_sl_arr_[:, 0]))\n",
        "    for idd in ids:\n",
        "        hls_mask = hls_sl_arr_[:, 0] == idd\n",
        "        Sentinel_mask = Sentinel_sl_arr_[:, 0] == idd\n",
        "        sentinel2_mask = sentinel2_sl_arr_[:, 0] == idd\n",
        "\n",
        "        Sentinel_sl_arr_[Sentinel_mask, 4] = cdf_matching(Sentinel_sl_arr_[Sentinel_mask, 4], hls_sl_arr_[hls_mask, 4])\n",
        "        sentinel2_sl_arr_[sentinel2_mask, 4] = cdf_matching(sentinel2_sl_arr_[sentinel2_mask, 4], hls_sl_arr_[hls_mask, 4])\n",
        "\n",
        "\n",
        "    # rootzone\n",
        "    ids = list(set(sentinel2_rly_arr_[:, 0]))\n",
        "    for idd in ids:\n",
        "        hls_mask = hls_rly_arr_[:, 0] == idd\n",
        "        Sentinel_mask = Sentinel_rly_arr_[:, 0] == idd\n",
        "        sentinel2_mask = sentinel2_rly_arr_[:, 0] == idd\n",
        "\n",
        "        Sentinel_rly_arr_[Sentinel_mask, 4] = cdf_matching(Sentinel_rly_arr_[Sentinel_mask, 4], hls_rly_arr_[hls_mask, 4])\n",
        "        sentinel2_rly_arr_[sentinel2_mask, 4] = cdf_matching(sentinel2_rly_arr_[sentinel2_mask, 4], hls_rly_arr_[hls_mask, 4])\n",
        "\n",
        "    return Sentinel_sl_arr_, sentinel2_sl_arr_, hls_sl_arr_, Sentinel_rly_arr_, sentinel2_rly_arr_, hls_rly_arr_\n",
        "\n",
        "def calculate_sd_and_miu(ML1_sl_arr, ML2_sl_arr, ML3_sl_arr, ML1_rly_arr, ML2_rly_arr, ML3_rly_arr):\n",
        "    # Xid_all, Xyear_all, Xdoy_all, Xutc_all, y_hat\n",
        "\n",
        "    # soil moisture\n",
        "    hls_sl_arr = ML1_sl_arr.copy()\n",
        "    temp_arr = np.zeros((hls_sl_arr.shape[0], 6)) + 3\n",
        "    temp_arr[:, 0:5] = hls_sl_arr[:, 0:5]\n",
        "    hls_sl_arr = temp_arr.copy()\n",
        "\n",
        "    Sentinel_sl_arr = ML2_sl_arr.copy()\n",
        "    temp_arr = np.zeros((Sentinel_sl_arr.shape[0], 6)) + 1\n",
        "    temp_arr[:, 0:5] = Sentinel_sl_arr[:, 0:5]\n",
        "    Sentinel_sl_arr = temp_arr.copy()\n",
        "\n",
        "    sentinel2_sl_arr = ML3_sl_arr.copy()\n",
        "    temp_arr = np.zeros((sentinel2_sl_arr.shape[0], 6)) + 2\n",
        "    temp_arr[:, 0:5] = sentinel2_sl_arr[:, 0:5]\n",
        "    sentinel2_sl_arr = temp_arr.copy()\n",
        "\n",
        "    ssm_sl_arr = np.vstack((hls_sl_arr, Sentinel_sl_arr, sentinel2_sl_arr))\n",
        "    ssm_sl_arr_0 = ssm_sl_arr.copy()\n",
        "    ssm_sl_arr_0[:, 3] += -1\n",
        "    ssm_sl_arr_1 = ssm_sl_arr.copy()\n",
        "    ssm_sl_arr_1[:, 3] += 1\n",
        "    ssm_sl_arr = np.vstack((ssm_sl_arr_0, ssm_sl_arr, ssm_sl_arr_1)).copy()\n",
        "\n",
        "    # soil moisture standard deviation\n",
        "    # The ubRMSE is also referred to as the standard deviation of the error\n",
        "    hls_sl_arr = ML1_sl_arr.copy()\n",
        "    temp_arr = np.zeros((hls_sl_arr.shape[0], 6)) + 3\n",
        "    temp_arr[:, 0:4] = hls_sl_arr[:, 0:4]\n",
        "    temp_arr[:, 4] = (hls_sl_arr[:, 5] - hls_sl_arr[:, 6])/3.29\n",
        "    temp_arr[temp_arr[:, 4] <= 0, 4] = 0.02  # a small value\n",
        "    hls_sl_arr = temp_arr.copy()\n",
        "\n",
        "    Sentinel_sl_arr = ML2_sl_arr.copy()\n",
        "    temp_arr = np.zeros((Sentinel_sl_arr.shape[0], 6)) + 1\n",
        "    temp_arr[:, 0:4] = Sentinel_sl_arr[:, 0:4]\n",
        "    temp_arr[:, 4] = (Sentinel_sl_arr[:, 5] - Sentinel_sl_arr[:, 6]) / 3.29\n",
        "    temp_arr[temp_arr[:, 4] <= 0, 4] = 0.02  # a small value\n",
        "    Sentinel_sl_arr = temp_arr.copy()\n",
        "\n",
        "    sentinel2_sl_arr = ML3_sl_arr.copy()\n",
        "    temp_arr = np.zeros((sentinel2_sl_arr.shape[0], 6)) + 2\n",
        "    temp_arr[:, 0:4] = sentinel2_sl_arr[:, 0:4]\n",
        "    temp_arr[:, 4] = (sentinel2_sl_arr[:, 5] - sentinel2_sl_arr[:, 6]) / 3.29\n",
        "    temp_arr[temp_arr[:, 4] <= 0, 4] = 0.02  # a small value\n",
        "    sentinel2_sl_arr = temp_arr.copy()\n",
        "\n",
        "    sd_sl_arr = np.vstack((hls_sl_arr, Sentinel_sl_arr, sentinel2_sl_arr))\n",
        "    sd_sl_arr_0 = sd_sl_arr.copy()\n",
        "    sd_sl_arr_0[:, 3] += -1\n",
        "    sd_sl_arr_1 = sd_sl_arr.copy()\n",
        "    sd_sl_arr_1[:, 3] += 1\n",
        "    sd_sl_arr = np.vstack((sd_sl_arr_0, sd_sl_arr, sd_sl_arr_1)).copy()\n",
        "\n",
        "\n",
        "    ids = list(set(ssm_sl_arr[:, 0]))\n",
        "    temp_arr = np.zeros((len(ids), 2))\n",
        "    for i in range(len(ids)):\n",
        "        temp_arr[i, 0] = ids[i]\n",
        "        mask = ssm_sl_arr[:, 0] == ids[i]\n",
        "        temp_arr[i, 1] = np.mean(ssm_sl_arr[mask, 4])\n",
        "    miu_sl_arr = temp_arr.copy()\n",
        "\n",
        "    # rootzone\n",
        "    hls_rly_arr = ML1_rly_arr.copy()\n",
        "    temp_arr = np.zeros((hls_rly_arr.shape[0], 6)) + 3\n",
        "    temp_arr[:, 0:5] = hls_rly_arr[:, 0:5]\n",
        "    hls_rly_arr = temp_arr.copy()\n",
        "\n",
        "    Sentinel_rly_arr = ML2_rly_arr.copy()\n",
        "    temp_arr = np.zeros((Sentinel_rly_arr.shape[0], 6)) + 1\n",
        "    temp_arr[:, 0:5] = Sentinel_rly_arr[:, 0:5]\n",
        "    Sentinel_rly_arr = temp_arr.copy()\n",
        "\n",
        "    sentinel2_rly_arr = ML3_rly_arr.copy()\n",
        "    temp_arr = np.zeros((sentinel2_rly_arr.shape[0], 6)) + 2\n",
        "    temp_arr[:, 0:5] = sentinel2_rly_arr[:, 0:5]\n",
        "    sentinel2_rly_arr = temp_arr.copy()\n",
        "\n",
        "\n",
        "    ssm_rly_arr = np.vstack((hls_rly_arr, Sentinel_rly_arr, sentinel2_rly_arr))\n",
        "    ssm_rly_arr_0 = ssm_rly_arr.copy()\n",
        "    ssm_rly_arr_0[:, 3] += -1\n",
        "    ssm_rly_arr_1 = ssm_rly_arr.copy()\n",
        "    ssm_rly_arr_1[:, 3] += 1\n",
        "    ssm_rly_arr = np.vstack((ssm_rly_arr_0, ssm_rly_arr, ssm_rly_arr_1)).copy()\n",
        "\n",
        "    # The ubRMSE is also referred to as the standard deviation of the error\n",
        "    hls_rly_arr = ML1_rly_arr.copy()\n",
        "    temp_arr = np.zeros((hls_rly_arr.shape[0], 6)) + 3\n",
        "    temp_arr[:, 0:4] = hls_rly_arr[:, 0:4]\n",
        "    temp_arr[:, 4] = (hls_rly_arr[:, 5] - hls_rly_arr[:, 6]) / 3.29\n",
        "    temp_arr[temp_arr[:, 4] <= 0, 4] = 0.02  # a small value\n",
        "    hls_rly_arr = temp_arr.copy()\n",
        "\n",
        "    Sentinel_rly_arr = ML2_rly_arr.copy()\n",
        "    temp_arr = np.zeros((Sentinel_rly_arr.shape[0], 6)) + 1\n",
        "    temp_arr[:, 0:4] = Sentinel_rly_arr[:, 0:4]\n",
        "    temp_arr[:, 4] = (Sentinel_rly_arr[:, 5] - Sentinel_rly_arr[:, 6]) / 3.29\n",
        "    temp_arr[temp_arr[:, 4] <= 0, 4] = 0.02  # a small value\n",
        "    Sentinel_rly_arr = temp_arr.copy()\n",
        "\n",
        "    sentinel2_rly_arr = ML3_rly_arr.copy()\n",
        "    temp_arr = np.zeros((sentinel2_rly_arr.shape[0], 6)) + 2\n",
        "    temp_arr[:, 0:4] = sentinel2_rly_arr[:, 0:4]\n",
        "    temp_arr[:, 4] = (sentinel2_rly_arr[:, 5] - sentinel2_rly_arr[:, 6]) / 3.29\n",
        "    temp_arr[temp_arr[:, 4] <= 0, 4] = 0.02  # a small value\n",
        "    sentinel2_rly_arr = temp_arr.copy()\n",
        "\n",
        "    sd_rly_arr = np.vstack((hls_rly_arr, Sentinel_rly_arr, sentinel2_rly_arr))\n",
        "    sd_rly_arr_0 = sd_rly_arr.copy()\n",
        "    sd_rly_arr_0[:, 3] += -1\n",
        "    sd_rly_arr_1 = sd_rly_arr.copy()\n",
        "    sd_rly_arr_1[:, 3] += 1\n",
        "    sd_rly_arr = np.vstack((sd_rly_arr_0, sd_rly_arr, sd_rly_arr_1)).copy()\n",
        "\n",
        "\n",
        "    ids = list(set(ssm_rly_arr[:, 0]))\n",
        "    temp_arr = np.zeros((len(ids), 2))\n",
        "    for i in range(len(ids)):\n",
        "        temp_arr[i, 0] = ids[i]\n",
        "        mask = ssm_rly_arr[:, 0] == ids[i]\n",
        "        temp_arr[i, 1] = np.mean(ssm_rly_arr[mask, 4])\n",
        "    miu_rly_arr = temp_arr.copy()\n",
        "\n",
        "    return ssm_sl_arr, ssm_rly_arr, sd_sl_arr, sd_rly_arr, miu_sl_arr, miu_rly_arr\n",
        "\n",
        "\n",
        "\n",
        "def create_date_list(start_date_str, num, deltat, unit='d'):\n",
        "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "\n",
        "    date_list = []\n",
        "    for i in range(num):\n",
        "        date_list.append(start_date)\n",
        "        if unit == 'd':\n",
        "            start_date += timedelta(days=deltat)\n",
        "        elif unit == 'h':\n",
        "            start_date += timedelta(hours=deltat)\n",
        "\n",
        "    return date_list\n",
        "\n",
        "\n",
        "def DoY2Date(year, doy, hh=10):\n",
        "    base_date = datetime(year, 1, 1, hour=hh)\n",
        "    target_date = base_date + timedelta(days=(doy - 1))\n",
        "    return target_date\n",
        "\n",
        "\n",
        "def reorgnize_2_2d(ssm_sl_arr, ssm_rly_arr, sd_sl_arr, sd_rly_arr, miu_sl_arr, miu_rly_arr, refer_id_arr):\n",
        "    ids = refer_id_arr\n",
        "    len_seq = 61344\n",
        "    len_id = len(ids)\n",
        "\n",
        "    # ----------------------------\n",
        "    datelist = create_date_list('2016-01-01', len_seq, 1, unit='h')\n",
        "    df_date = pd.DataFrame({'time': datelist})\n",
        "    df_date.set_index('time', inplace=True)\n",
        "\n",
        "    # surface layer - sm\n",
        "    df_ssm_sly = pd.DataFrame(ssm_sl_arr, columns=['id', 'year', 'doy', 'hour', 'sm', 'sensor'])\n",
        "    df_ssm_sly = df_ssm_sly[(df_ssm_sly['hour'] >= 0) & (df_ssm_sly['hour'] <= 23)]\n",
        "    df_ssm_sly['time'] = df_ssm_sly.apply(lambda row: DoY2Date(int(row['year']), int(row['doy']), int(row['hour'])), axis=1)\n",
        "\n",
        "    ssm_sl_arr3D = np.zeros((3, len_seq, len_id))\n",
        "    ssm_sl_arr3D[:] = np.nan\n",
        "    sors = [1, 2, 3]\n",
        "    for i in range(len(sors)):\n",
        "        for j in range(len(ids)):\n",
        "            sor = sors[i]\n",
        "            iid = ids[j]\n",
        "            df_ssm_sly_sor_id = df_ssm_sly[(df_ssm_sly['sensor']==sor)&(df_ssm_sly['id']==iid)]\n",
        "            df_ssm_sly_sor_id.set_index('time', inplace=True)\n",
        "            df_new = df_date.join(df_ssm_sly_sor_id, how='left')\n",
        "            df_new = df_new.loc[~df_new.index.duplicated(keep='first')]\n",
        "            ssm_sl_arr3D[i, :, j] = df_new['sm'].to_numpy()[:]\n",
        "\n",
        "\n",
        "\n",
        "    # rootzone - sm\n",
        "    df_ssm_rly = pd.DataFrame(ssm_rly_arr, columns=['id', 'year', 'doy', 'hour', 'sm', 'sensor'])\n",
        "    df_ssm_rly = df_ssm_rly[(df_ssm_rly['hour'] >= 0) & (df_ssm_rly['hour'] <= 23)]\n",
        "    df_ssm_rly['time'] = df_ssm_rly.apply(lambda row: DoY2Date(int(row['year']), int(row['doy']), int(row['hour'])),\n",
        "                                          axis=1)\n",
        "\n",
        "    ssm_rly_arr3D = np.zeros((3, len_seq, len_id))\n",
        "    ssm_rly_arr3D[:] = np.nan\n",
        "    sors = [1, 2, 3]\n",
        "    for i in range(len(sors)):\n",
        "        for j in range(len(ids)):\n",
        "            sor = sors[i]\n",
        "            iid = ids[j]\n",
        "\n",
        "            df_ssm_rly_sor_id = df_ssm_rly[(df_ssm_rly['sensor']==sor)&(df_ssm_rly['id']==iid)]\n",
        "            df_ssm_rly_sor_id.set_index('time', inplace=True)\n",
        "            df_new = df_date.join(df_ssm_rly_sor_id, how='left')\n",
        "            df_new = df_new.loc[~df_new.index.duplicated(keep='first')]\n",
        "            ssm_rly_arr3D[i, :, j] = df_new['sm'].to_numpy()[:]\n",
        "\n",
        "\n",
        "\n",
        "    # surface layer - SD\n",
        "    df_sd_sly = pd.DataFrame(sd_sl_arr, columns=['id', 'year', 'doy', 'hour', 'sm', 'sensor'])\n",
        "    df_sd_sly = df_sd_sly[(df_sd_sly['hour'] >= 0) & (df_sd_sly['hour'] <= 23)]\n",
        "    df_sd_sly['time'] = df_sd_sly.apply(lambda row: DoY2Date(int(row['year']), int(row['doy']), int(row['hour'])),\n",
        "                                          axis=1)\n",
        "\n",
        "    sd_sl_arr3D = np.zeros((3, len_seq, len_id))\n",
        "    sd_sl_arr3D[:] = np.nan\n",
        "\n",
        "    fsors = [1, 2, 3]\n",
        "    for i in range(len(sors)):\n",
        "        for j in range(len(ids)):\n",
        "            sor = sors[i]\n",
        "            iid = ids[j]\n",
        "            df_sd_sly_sor_id = df_sd_sly[(df_sd_sly['sensor'] == sor) & (df_sd_sly['id'] == iid)]\n",
        "            df_sd_sly_sor_id.set_index('time', inplace=True)\n",
        "            df_new = df_date.join(df_sd_sly_sor_id, how='left')\n",
        "            df_new = df_new.loc[~df_new.index.duplicated(keep='first')]\n",
        "            sd_sl_arr3D[i, :, j] = df_new['sm'].to_numpy()[:]\n",
        "\n",
        "\n",
        "\n",
        "    # rootzone - sd\n",
        "    df_sd_rly = pd.DataFrame(sd_rly_arr, columns=['id', 'year', 'doy', 'hour', 'sm', 'sensor'])\n",
        "    df_sd_rly = df_sd_rly[(df_sd_rly['hour'] >= 0) & (df_sd_rly['hour'] <= 23)]\n",
        "    df_sd_rly['time'] = df_sd_rly.apply(lambda row: DoY2Date(int(row['year']), int(row['doy']), int(row['hour'])),\n",
        "                                          axis=1)\n",
        "\n",
        "    sd_rly_arr3D = np.zeros((3, len_seq, len_id))\n",
        "    sd_rly_arr3D[:] = np.nan\n",
        "    for i in range(len(sors)):\n",
        "        for j in range(len(ids)):\n",
        "            sor = sors[i]\n",
        "            iid = ids[j]\n",
        "            df_sd_rly_sor_id = df_sd_rly[(df_sd_rly['sensor'] == sor) & (df_sd_rly['id'] == iid)]\n",
        "            df_sd_rly_sor_id.set_index('time', inplace=True)\n",
        "            df_new = df_date.join(df_sd_rly_sor_id, how='left')\n",
        "            df_new = df_new.loc[~df_new.index.duplicated(keep='first')]\n",
        "            sd_rly_arr3D[i, :, j] = df_new['sm'].to_numpy()[:]\n",
        "\n",
        "    # ======================================\n",
        "\n",
        "    miu_sl_arr1D = np.zeros((len(ids)))\n",
        "    miu_sl_arr1D[:] = np.nan\n",
        "    for j in range(len(ids)):\n",
        "        iid = ids[j]\n",
        "        if len(miu_sl_arr[miu_sl_arr[:, 0]==iid, 1]) == 0:\n",
        "            miu_sl_arr1D[j] = np.nan\n",
        "            continue\n",
        "        meanvalue = miu_sl_arr[miu_sl_arr[:, 0]==iid, 1][0]\n",
        "        miu_sl_arr1D[j] = meanvalue\n",
        "\n",
        "    miu_rly_arr1D = np.zeros((len(ids)))\n",
        "    miu_rly_arr1D[:] = np.nan\n",
        "    for j in range(len(ids)):\n",
        "        iid = ids[j]\n",
        "        if len(miu_rly_arr[miu_rly_arr[:, 0] == iid, 1]) == 0:\n",
        "            miu_rly_arr1D[j] = np.nan\n",
        "            continue\n",
        "        meanvalue = miu_rly_arr[miu_rly_arr[:, 0] == iid, 1][0]\n",
        "        miu_rly_arr1D[j] = meanvalue\n",
        "\n",
        "\n",
        "    ssm_sl_arr2D = np.nanmean(ssm_sl_arr3D, axis=0)\n",
        "    ssm_rly_arr2D = np.nanmean(ssm_rly_arr3D, axis=0)\n",
        "    sd_sl_arr2D = np.nanmean(sd_sl_arr3D, axis=0)\n",
        "    sd_rly_arr2D = np.nanmean(sd_rly_arr3D, axis=0)\n",
        "\n",
        "    return ssm_sl_arr2D, ssm_rly_arr2D, sd_sl_arr2D, sd_rly_arr2D, miu_sl_arr1D, miu_rly_arr1D"
      ],
      "metadata": {
        "id": "kWSQFYfbPda0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reorgnize ML results - perform full CDF matching\n",
        "ML1_sl_arr_corrected, ML2_sl_arr_corrected, ML3_sl_arr_corrected, \\\n",
        "ML1_rly_arr_corrected, ML2_rly_arr_corrected, ML3_rly_arr_corrected = correct_2_ML3(ML1_sly_y_pred_all_arr,\n",
        "                                                                                    ML2_sly_y_pred_all_arr,\n",
        "                                                                                    ML3_sly_y_pred_all_arr,\n",
        "                                                                                    ML1_rly_y_pred_all_arr,\n",
        "                                                                                    ML2_rly_y_pred_all_arr,\n",
        "                                                                                    ML3_rly_y_pred_all_arr)\n",
        "\n",
        "ssm_sl_arr, ssm_rly_arr, sd_sl_arr, \\\n",
        "sd_rly_arr, miu_sl_arr, miu_rly_arr = calculate_sd_and_miu(ML1_sl_arr_corrected,\n",
        "                                                          ML2_sl_arr_corrected,\n",
        "                                                          ML3_sl_arr_corrected,\n",
        "                                                          ML1_rly_arr_corrected,\n",
        "                                                          ML2_rly_arr_corrected,\n",
        "                                                          ML3_rly_arr_corrected)\n",
        "\n",
        "ssm_sl_arr2D, ssm_rly_arr2D, sd_sl_arr2D, \\\n",
        "sd_rly_arr2D, miu_sl_arr1D, miu_rly_arr1D = reorgnize_2_2d(ssm_sl_arr,\n",
        "                                                           ssm_rly_arr,\n",
        "                                                           sd_sl_arr,\n",
        "                                                           sd_rly_arr,\n",
        "                                                           miu_sl_arr,\n",
        "                                                           miu_rly_arr,\n",
        "                                                           refer_id_arr)\n",
        "\n",
        "del ssm_sl_arr, ssm_rly_arr, sd_sl_arr, sd_rly_arr, miu_sl_arr, miu_rly_arr"
      ],
      "metadata": {
        "id": "cDVgTRkvQbps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a dot polt for ssm_sl_arr2D (surface layer soil moisture) and ssm_rly_arr2D (rootzone soil moisture)  which both have a shape of (61344, 1), x from 1 to 61344.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming ssm_sl_arr2D and ssm_rly_arr2D are defined and have shape (61344, 1)\n",
        "x = range(1, 61345)  # x-axis values from 1 to 61344\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "plt.plot(x, ssm_sl_arr2D, label='Surface Layer Soil Moisture (ssm_sl_arr2D)', marker='o', linestyle='-', markersize=2)\n",
        "plt.plot(x, ssm_rly_arr2D, label='Rootzone Soil Moisture (ssm_rly_arr2D)', marker='x', linestyle='--', markersize=2)\n",
        "\n",
        "plt.xlabel('X-axis (1 to 61344)')\n",
        "plt.ylabel('Soil Moisture (m3/m3)')\n",
        "plt.title('Soil Moisture')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7EhjAN-uSHZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssm_sl_arr2D.shape"
      ],
      "metadata": {
        "id": "rYuWKW8LSATc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvHgldrH_Cqu"
      },
      "source": [
        "## Run EnKF to integrate ML estimates into a process model, Layered Green and Ampt infitration with Redistribution\n",
        "\n",
        "Note that since LGRA model is coded in c/c++, here, the code only includes the data preparing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TubIS8hiJg7N"
      },
      "source": [
        "### Run EnKF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare for inputs - driving forces (precipitation and potential evaporation)\n",
        "alpha = 1.3\n",
        "rho_w = 1000\n",
        "\n",
        "def e_s(T):\n",
        "    return(611*np.exp((17.27*T)/(273.3+T)))\n",
        "\n",
        "def delta(T):\n",
        "    return((4098*e_s(T))/((273.3+T)**2))\n",
        "\n",
        "def l_v(T):\n",
        "    return(2500-2.36*T)\n",
        "\n",
        "def gamma(T):\n",
        "    return(66.8)\n",
        "    #return(1.005*101325*pressure/(0.622*l_v(T)))\n",
        "    #note that gamma is often given as the constant 66.8 pascals/decree C. the other version, commented out, takes temperature and pressure (input pressure in atm, and temp as deg C) as inputs.\n",
        "\n",
        "def E_r(R_n,T):\n",
        "    return(R_n/(l_v(T)*1000*rho_w))\n",
        "\n",
        "def E_PT(R_n,T):\n",
        "    return(E_r(R_n,T)*alpha*delta(T)/(delta(T)+gamma(T)))\n",
        "\n",
        "\n",
        "\n",
        "prec_arr = era5hour_arr[0,:,:]*1000  # hourly precipitation, m to mm\n",
        "prec_arr[prec_arr<1e-2] = 0\n",
        "\n",
        "DSR_arr = era5hour_arr[2,:,:]/3600  # hourly DSR, J to w/m2\n",
        "temp_arr = era5hour_arr[5,:,:] - 273.15  # K to degC\n",
        "\n",
        "PET_arr = 3.6e6*E_PT(DSR_arr, temp_arr)  # hourly potential ET, mm\n",
        "PET_arr[PET_arr<1e-6] = 0\n",
        "\n",
        "np.savetxt(f\"prec_arr.csv\", prec_arr, delimiter=\",\")\n",
        "np.savetxt(f\"PET_arr.csv\", PET_arr, delimiter=\",\")"
      ],
      "metadata": {
        "id": "_aYe7NyDjM-Z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare for inputs - ML estimates\n",
        "np.savetxt(f\"ssm_sl_arr2D.csv\", ssm_sl_arr2D, delimiter=\",\")\n",
        "np.savetxt(f\"ssm_rly_arr2D.csv\", ssm_rly_arr2D, delimiter=\",\")\n",
        "np.savetxt(f\"sd_sl_arr2D.csv\", sd_sl_arr2D, delimiter=\",\")\n",
        "np.savetxt(f\"sd_rly_arr2D.csv\", sd_rly_arr2D, delimiter=\",\")"
      ],
      "metadata": {
        "id": "oSMpvGW0rJXQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare for inputs - soil names (USDA soil taxonomy)\n",
        "pm_clay_arr = np.column_stack((clay_0_5_arr, clay_5_15_arr, clay_15_30_arr, clay_30_60_arr, clay_60_100_arr))\n",
        "pm_sand_arr = np.column_stack((sand_0_5_arr, sand_5_15_arr, sand_15_30_arr, sand_30_60_arr, sand_60_100_arr))\n",
        "pm_silt_arr = 100 - pm_clay_arr - pm_sand_arr\n",
        "\n",
        "mask_sand = np.logical_and(pm_sand_arr > 85, (pm_silt_arr + 1.5 * pm_clay_arr) < 15)\n",
        "mask_loamy_sand = np.logical_and.reduce([\n",
        "    pm_sand_arr >= 70,\n",
        "    pm_sand_arr < 91,\n",
        "    (pm_silt_arr + 1.5 * pm_clay_arr) >= 15,\n",
        "    (pm_silt_arr + 2 * pm_clay_arr) < 30\n",
        "])\n",
        "mask_sandy_loam = np.logical_or(\n",
        "    np.logical_and.reduce([\n",
        "        pm_clay_arr >= 7,\n",
        "        pm_clay_arr < 20,\n",
        "        pm_sand_arr > 52,\n",
        "        (pm_silt_arr + 2 * pm_clay_arr) >= 30\n",
        "    ]),\n",
        "    np.logical_and.reduce([\n",
        "        pm_clay_arr < 7,\n",
        "        pm_silt_arr < 50,\n",
        "        pm_sand_arr > 43\n",
        "    ])\n",
        ")\n",
        "mask_loam = np.logical_and.reduce([\n",
        "    pm_clay_arr >= 7,\n",
        "    pm_clay_arr < 27,\n",
        "    pm_silt_arr >= 28,\n",
        "    pm_silt_arr < 50,\n",
        "    pm_sand_arr <= 52\n",
        "])\n",
        "mask_silt_loam = np.logical_or(\n",
        "    np.logical_and(pm_silt_arr >= 50, np.logical_and(pm_clay_arr >= 12, pm_clay_arr < 27)),\n",
        "    np.logical_and(pm_clay_arr < 12, np.logical_and(pm_silt_arr >= 50, pm_silt_arr < 80))\n",
        ")\n",
        "mask_silt = np.logical_and(pm_silt_arr >= 80, pm_clay_arr < 12)\n",
        "mask_sandy_clay_loam = np.logical_and.reduce([\n",
        "    pm_clay_arr >= 20,\n",
        "    pm_clay_arr < 35,\n",
        "    pm_silt_arr < 28,\n",
        "    pm_sand_arr > 45\n",
        "])\n",
        "mask_clay_loam = np.logical_and.reduce([\n",
        "    pm_clay_arr >= 27,\n",
        "    pm_clay_arr < 40,\n",
        "    pm_sand_arr > 20,\n",
        "    pm_sand_arr < 46\n",
        "])\n",
        "mask_silt_clay_loam = np.logical_and.reduce([\n",
        "    pm_clay_arr >= 27,\n",
        "    pm_clay_arr < 40,\n",
        "    pm_sand_arr <= 20\n",
        "])\n",
        "mask_sandy_clay = np.logical_and(pm_clay_arr >= 35, pm_sand_arr >= 45)\n",
        "mask_silty_clay = np.logical_and(pm_clay_arr >= 40, pm_silt_arr >= 40)\n",
        "mask_clay = np.logical_and.reduce([\n",
        "    pm_clay_arr >= 40,\n",
        "    pm_sand_arr <= 45,\n",
        "    pm_silt_arr < 40\n",
        "])\n",
        "\n",
        "#%%\n",
        "\n",
        "soil_names_arr = np.full(pm_sand_arr.shape, 'abc', dtype='<U20')\n",
        "\n",
        "\n",
        "soil_names_arr[mask_sand]=\"sand\"\n",
        "\n",
        "soil_names_arr[mask_loamy_sand]=\"loamy_sand\"\n",
        "soil_names_arr[mask_sandy_loam]=\"sandy_loam\"\n",
        "soil_names_arr[mask_loam]=\"loam\"\n",
        "soil_names_arr[mask_silt_loam]=\"silt_loam\"\n",
        "soil_names_arr[mask_silt]=\"silt\"\n",
        "soil_names_arr[mask_sandy_clay_loam]=\"sandy_clay_loam\"\n",
        "soil_names_arr[mask_clay_loam]=\"clay_loam\"\n",
        "soil_names_arr[mask_silt_clay_loam]=\"silty_clay_loam\"\n",
        "soil_names_arr[mask_sandy_clay]=\"sandy_clay\"\n",
        "soil_names_arr[mask_silty_clay]=\"silty_clay\"\n",
        "soil_names_arr[mask_clay]=\"clay\"\n",
        "\n",
        "np.savetxt(f\"soil_names.csv\", soil_names_arr, delimiter=\",\", fmt=\"%s\")"
      ],
      "metadata": {
        "id": "RbiDanvEuyWv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title compile c++ program - do it once\n",
        "# !unzip LGARTOKF_forPipeline.zip -d LGARTOKF_forPipeline\n",
        "# %cd LGARTOKF_forPipeline/LGARTOKF_forPipeline\n",
        "\n",
        "# !g++ -g -std=c++17 \\\n",
        "# bmi_main_lgar.cxx \\\n",
        "# src/aet.cxx \\\n",
        "# src/bmi_lgar.cxx \\\n",
        "# src/lgar.cxx \\\n",
        "# src/linked_list.cxx \\\n",
        "# src/mem_funcs.cxx \\\n",
        "# src/soil_funcs.cxx \\\n",
        "# src/util_funcs.cxx \\\n",
        "# giuh/giuh.cxx \\\n",
        "# -Iinclude -Ibmi -Igiuh \\\n",
        "# -o LGAR_EnKF\n",
        "\n",
        "# %cd /content"
      ],
      "metadata": {
        "id": "iGknRkzfqwcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title compile c++ program - do it once\n",
        "# %cd /content\n",
        "# !unzip LGARTOKF_original.zip -d LGARTOKF_original\n",
        "# %cd LGARTOKF_original/LGARTOKF_original\n",
        "\n",
        "# !g++ -g -std=c++17 \\\n",
        "# bmi_main_lgar.cxx \\\n",
        "# src/aet.cxx \\\n",
        "# src/bmi_lgar.cxx \\\n",
        "# src/lgar.cxx \\\n",
        "# src/linked_list.cxx \\\n",
        "# src/mem_funcs.cxx \\\n",
        "# src/soil_funcs.cxx \\\n",
        "# src/util_funcs.cxx \\\n",
        "# giuh/giuh.cxx \\\n",
        "# -Iinclude -Ibmi -Igiuh \\\n",
        "# -o LGAR\n",
        "\n",
        "# %cd /content"
      ],
      "metadata": {
        "id": "tUhGx63Is2Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run original LGAR model\n",
        "!chmod +x LGAR\n",
        "!./LGAR  --filename_soilname soil_names.csv --filename_prec prec_arr.csv --filename_pet PET_arr.csv --settings_path settingInfo.txt --filename_soil_params vG_default_params_HYDRUS.csv"
      ],
      "metadata": {
        "id": "6mfPG0XCup1k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # create a scatter plot for  LGAR_sm_sly_0 and LGAR_sm_rly_0, which both have a shape of (61344, ), and x is from 1 to 61344\n",
        "sm_LGAR_0 = pd.read_csv(f'Original_LGAR_{idd}.csv', header=None)\n",
        "LGAR_sm_sly_0 = sm_LGAR_0.iloc[:, 1].to_numpy()\n",
        "LGAR_sm_rly_0 = sm_LGAR_0.iloc[:, 2].to_numpy()\n",
        "\n",
        "x = range(1, len(LGAR_sm_sly_0) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, LGAR_sm_sly_0, label='LGAR_sm_sly_0', color='r') #\n",
        "plt.plot(x, LGAR_sm_rly_0, label='LGAR_sm_rly_0', color='b') #\n",
        "\n",
        "plt.xlabel('X-axis (1 to 61344)')\n",
        "plt.ylabel('Soil Moisture')\n",
        "plt.title('Scatter Plot of LGAR Soil Moisture')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8t3gEZAucxq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Do CDF matching: match ML to original model simulation\n",
        "\n",
        "# ML results\n",
        "sm_sl_arr2D_LGARCorrected = np.zeros(ssm_sl_arr2D.shape)\n",
        "sm_rly_arr2D_LGARCorrected = np.zeros(ssm_rly_arr2D.shape)\n",
        "sm_sl_arr2D_LGARCorrected[:] = np.nan\n",
        "sm_rly_arr2D_LGARCorrected[:] = np.nan\n",
        "\n",
        "for i in range(len(refer_id_arr)):\n",
        "  idd = refer_id_arr[i]\n",
        "  print(idd)\n",
        "  # orginal models simulations\n",
        "  site_sm_LGAR = pd.read_csv(f'Original_LGAR_{idd}.csv', header=None)\n",
        "  LGAR_sm_sly_site = site_sm_LGAR.iloc[:, 1].to_numpy()\n",
        "  LGAR_sm_rly_site = site_sm_LGAR.iloc[:, 2].to_numpy()\n",
        "  print(f'LGAR at surface layer, maximum SM: {np.nanmax(LGAR_sm_sly_site)}, minimum SM: {np.nanmin(LGAR_sm_sly_site)}')\n",
        "  print(f'LGAR at rootzone, maximum SM: {np.nanmax(LGAR_sm_rly_site)}, minimum SM: {np.nanmin(LGAR_sm_rly_site)}')\n",
        "\n",
        "  # ML results\n",
        "\n",
        "  ML_sm_sl_site = ssm_sl_arr2D[:,i]\n",
        "  ML_sm_rly_site = ssm_rly_arr2D[:,i]\n",
        "\n",
        "  print(f'ML at surface layer, maximum SM: {np.nanmax(ML_sm_sl_site)}, minimum SM: {np.nanmin(ML_sm_sl_site)}')\n",
        "  print(f'ML at rootzone, maximum SM: {np.nanmax(ML_sm_rly_site)}, minimum SM: {np.nanmin(ML_sm_rly_site)}')\n",
        "\n",
        "  nanmask_sly = np.isnan(ML_sm_sl_site)\n",
        "  nanmask_rly = np.isnan(ML_sm_rly_site)\n",
        "\n",
        "  sm_sl_arr2D_LGARCorrected[~nanmask_sly, i] = cdf_matching(ML_sm_sl_site[~nanmask_sly], LGAR_sm_sly_site)\n",
        "  sm_rly_arr2D_LGARCorrected[~nanmask_rly, i] = cdf_matching(ML_sm_rly_site[~nanmask_rly], LGAR_sm_rly_site)\n",
        "\n",
        "  print(f'cdf-ed ML at surface layer, maximum SM: {np.nanmax(sm_sl_arr2D_LGARCorrected[:,i])}, minimum SM: {np.nanmin(sm_sl_arr2D_LGARCorrected[:,i])}')\n",
        "  print(f'cdf-ed ML at rootzone, maximum SM: {np.nanmax(sm_rly_arr2D_LGARCorrected[:,i])}, minimum SM: {np.nanmin(sm_rly_arr2D_LGARCorrected[:,i])}')\n",
        "\n",
        "\n",
        "np.savetxt(f\"sm_sl_arr2D_LGARCorrected.csv\", sm_sl_arr2D_LGARCorrected, delimiter=\",\")\n",
        "np.savetxt(f\"sm_rly_arr2D_LGARCorrected.csv\", sm_rly_arr2D_LGARCorrected, delimiter=\",\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3QWS-fTnvM6e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming ssm_sl_arr2D and ssm_rly_arr2D are defined and have shape (61344, 1)\n",
        "x = range(1, 61345)  # x-axis values from 1 to 61344\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "plt.plot(x, sm_sl_arr2D_LGARCorrected, label='Surface Layer Soil Moisture (ssm_sl_arr2D)', marker='o', linestyle='-', markersize=2)\n",
        "plt.plot(x, sm_rly_arr2D_LGARCorrected, label='Rootzone Soil Moisture (ssm_rly_arr2D)', marker='x', linestyle='--', markersize=2)\n",
        "\n",
        "plt.xlabel('X-axis (1 to 61344)')\n",
        "plt.ylabel('Soil Moisture (m3/m3)')\n",
        "plt.title('Soil Moisture')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kIBma9WtOCE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run EnKF-LGAR\n",
        "!chmod +x LGAR_EnKF\n",
        "!./LGAR_EnKF --filename_soilname soil_names.csv --filename_prec prec_arr.csv --filename_pet PET_arr.csv --filename_sm_sly sm_sl_arr2D_LGARCorrected.csv --filename_sm_rly sm_rly_arr2D_LGARCorrected.csv --filename_sd_sly sd_sl_arr2D.csv --filename_sd_rly sd_rly_arr2D.csv --settings_path settingInfo.txt  --filename_soil_params vG_default_params_HYDRUS.csv"
      ],
      "metadata": {
        "id": "rOoyGWixyJ_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CDF match back to ML estimates\n",
        "for i in range(len(refer_id_arr)):\n",
        "  idd = refer_id_arr[i]\n",
        "  print(idd)\n",
        "  df_enkf = pd.read_csv(f'EnKF_LGAR_{idd}.csv')\n",
        "  # ML results\n",
        "  ML_sm_sl_site = ssm_sl_arr2D[:,i]\n",
        "  ML_sm_rly_site = ssm_rly_arr2D[:,i]\n",
        "  nanmask_sly = np.isnan(ML_sm_sl_site)\n",
        "  nanmask_rly = np.isnan(ML_sm_rly_site)\n",
        "\n",
        "  df_enkf['ml_sm_sly'] = ML_sm_sl_site\n",
        "  df_enkf['ml_sm_rly'] = ML_sm_rly_site\n",
        "\n",
        "  df_enkf['forcast_sm_sly'] = cdf_matching(df_enkf['forcast_sm_sly'].to_numpy(), ML_sm_sl_site[~nanmask_rly])\n",
        "  df_enkf['forcast_sm_rly'] = cdf_matching(df_enkf['forcast_sm_rly'].to_numpy(), ML_sm_rly_site[~nanmask_rly])\n",
        "  df_enkf['analysis_sm_sly'] = cdf_matching(df_enkf['analysis_sm_sly'].to_numpy(), ML_sm_sl_site[~nanmask_rly])\n",
        "  df_enkf['analysis_sm_rly'] = cdf_matching(df_enkf['analysis_sm_rly'].to_numpy(), ML_sm_rly_site[~nanmask_rly])\n",
        "\n",
        "  df_enkf.to_csv(f'EnKF_LGAR_{idd}.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qMi-R1rLvW5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a plot:\n",
        "# x=df_enkf['time']\n",
        "# y1=df_enkf['forcast_sm_rly'], whic is a line\n",
        "# y2=df_enkf['analysis_sm_rly'], whic is a line\n",
        "# y3=df_enkf['ml_sm_rly'], whic is a scatter\n",
        "\n",
        "df_enkf = pd.read_csv(f'EnKF_LGAR_0.csv')\n",
        "x = df_enkf['time'].to_numpy()\n",
        "y1 = df_enkf['forcast_sm_rly'].to_numpy()\n",
        "y1_sigma = df_enkf['forcast_sd_rly'].to_numpy()\n",
        "y1_upper = y1 + y1_sigma\n",
        "y1_lower = y1 - y1_sigma\n",
        "y2 = df_enkf['analysis_sm_rly'].to_numpy()\n",
        "y2_sigma = df_enkf['analysis_sd_rly'].to_numpy()\n",
        "y2_upper = y2 + y2_sigma\n",
        "y2_lower = y2 - y2_sigma\n",
        "y3 = df_enkf['ml_sm_rly'].to_numpy()\n",
        "y3_sigma = df_enkf['ml_sd_rly'].to_numpy()\n",
        "y3_upper = y3 + y3_sigma\n",
        "y3_lower = y3 - y3_sigma\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x[31200:31400], y1[31200:31400], label='Forecast', color='#f47721')\n",
        "plt.plot(x[31200:31400], y2[31200:31400], label='Analysis', color='#7ac143')\n",
        "plt.scatter(x[31200:31400], y3[31200:31400], label='ML', marker='o', s=10) # Adjust marker size (s) as needed\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Soil Moisture')\n",
        "plt.title('Soil Moisture Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nWBFEpoweLHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plot with uncertainty\n",
        "\n",
        "# Load data\n",
        "df_enkf = pd.read_csv('EnKF_LGAR_0.csv')\n",
        "\n",
        "# Define range for slicing\n",
        "range_low = 31000    # Change this to set the start index\n",
        "range_up = 32000   # Change this to set the end index\n",
        "\n",
        "# Slice x and all y variables\n",
        "x = df_enkf['time'].to_numpy()[range_low:range_up]\n",
        "y1 = df_enkf['forcast_sm_rly'].to_numpy()[range_low:range_up]\n",
        "y1_sigma = df_enkf['forcast_sd_rly'].to_numpy()[range_low:range_up]\n",
        "y1_upper = y1 + y1_sigma\n",
        "y1_lower = y1 - y1_sigma\n",
        "\n",
        "y2 = df_enkf['analysis_sm_rly'].to_numpy()[range_low:range_up]\n",
        "y2_sigma = df_enkf['analysis_sd_rly'].to_numpy()[range_low:range_up]\n",
        "y2_upper = y2 + y2_sigma\n",
        "y2_lower = y2 - y2_sigma\n",
        "\n",
        "y3 = df_enkf['ml_sm_rly'].to_numpy()[range_low:range_up]\n",
        "y3_sigma = df_enkf['ml_sd_rly'].to_numpy()[range_low:range_up]\n",
        "y3_upper = y3 + y3_sigma\n",
        "y3_lower = y3 - y3_sigma\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.fill_between(x, y1_upper, y1_lower, color='#f47721', alpha=0.3, label='Forecast Uncertainty')\n",
        "plt.plot(x, y1, color='#f47721', label='Forecast')\n",
        "\n",
        "plt.fill_between(x, y2_upper, y2_lower, color='#7ac143', alpha=0.3, label='Analysis Uncertainty')\n",
        "plt.plot(x, y2, color='#7ac143', label='Analysis')\n",
        "\n",
        "plt.errorbar(\n",
        "    x, y3,\n",
        "    yerr=[y3 - y3_lower, y3_upper - y3],\n",
        "    fmt='o',\n",
        "    color='blue',\n",
        "    ecolor='lightblue',\n",
        "    elinewidth=1,\n",
        "    # alpha=0.5,\n",
        "    markersize=3,\n",
        "    capsize=2,\n",
        "    label='ML with Uncertainty'\n",
        ")\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Soil Moisture')\n",
        "plt.title('Soil Moisture Comparison')\n",
        "plt.ylim(0.2, 0.6)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "N9Zb8fLlLlkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2RUHFfXfLo7e",
        "1JXbDpQOLE7O"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}