{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cshgiser/HRSM/blob/main/HRSM_MAPPING_Sec2-2_ML_pipeline_area.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok1qkt3pvrpi"
      },
      "source": [
        "## Initial settings (authorize, connect GD, install packages, etc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q1lIe_yUAiRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2YjXZTBuyY8"
      },
      "outputs": [],
      "source": [
        "# @title get GEE authorization\n",
        "import ee\n",
        "\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "# Initialize the library.\n",
        "ee.Initialize(project='ee-scai62')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFzcQBXFzb3P"
      },
      "outputs": [],
      "source": [
        "# @title Connect to my Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr9sC06bziKw"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/' ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWGkiKt518nz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Install necessary packages\n",
        "!pip install folium\n",
        "!pip install geopandas\n",
        "!pip install netCDF4\n",
        "!pip install --upgrade xee\n",
        "!pip install rasterio\n",
        "!pip install ipywidgets\n",
        "!pip install permetrics==2.0.0\n",
        "# !pip install pyDEM\n",
        "# !pip install richdem\n",
        "# !pip install pyflwdir\n",
        "!pip install mapie\n",
        "!pip install scikit-learn==1.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku6RglgO2FUR"
      },
      "outputs": [],
      "source": [
        "# @title Import packages\n",
        "from mapie.regression import SplitConformalRegressor, ConformalizedQuantileRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "import folium\n",
        "from folium import Figure\n",
        "import geopandas as gpd\n",
        "import json\n",
        "import geemap\n",
        "import numpy as np\n",
        "from shapely.geometry import mapping\n",
        "import pandas as pd\n",
        "import netCDF4 as nc\n",
        "import xarray\n",
        "import rasterio\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from rasterio.crs import CRS\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import joblib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from permetrics import RegressionMetric\n",
        "import osgeo.gdal as gdal\n",
        "# import richdem as rd\n",
        "# import pyflwdir\n",
        "from osgeo import osr, gdalconst\n",
        "from scipy.stats import rankdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR7GKJyKwmqJ"
      },
      "source": [
        "## Define study site: set longitude and latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEk-95TB2LUK"
      },
      "outputs": [],
      "source": [
        "# @title Input longitude and latitude\n",
        "\n",
        "# Global variables to store coordinates\n",
        "upper_left = None\n",
        "bottom_right = None\n",
        "\n",
        "def validate_coordinates(lon, lat):\n",
        "    \"\"\"Check if coordinate is inside contiguous USA.\"\"\"\n",
        "    return -125 <= lon <= -66 and 24 <= lat <= 50\n",
        "\n",
        "def on_button_click(b):\n",
        "    global upper_left, bottom_right\n",
        "    try:\n",
        "        lon1 = float(ul_longitude_input.value)\n",
        "        lat1 = float(ul_latitude_input.value)\n",
        "        lon2 = float(br_longitude_input.value)\n",
        "        lat2 = float(br_latitude_input.value)\n",
        "\n",
        "        if (validate_coordinates(lon1, lat1) and\n",
        "            validate_coordinates(lon2, lat2) and\n",
        "            lon1 < lon2 and lat1 > lat2):  # ensure proper rectangle\n",
        "\n",
        "            upper_left = (lon1, lat1)\n",
        "            bottom_right = (lon2, lat2)\n",
        "\n",
        "            output_area.clear_output()\n",
        "            with output_area:\n",
        "                print(f\"Valid rectangle defined:\")\n",
        "                print(f\"  Upper-left  (Lon {lon1}, Lat {lat1})\")\n",
        "                print(f\"  Bottom-right (Lon {lon2}, Lat {lat2})\")\n",
        "\n",
        "        else:\n",
        "            output_area.clear_output()\n",
        "            with output_area:\n",
        "                print(\"Invalid coordinates. Ensure they are inside the contiguous USA \"\n",
        "                      \"and that Upper-left is above and left of Bottom-right.\")\n",
        "\n",
        "    except ValueError:\n",
        "        output_area.clear_output()\n",
        "        with output_area:\n",
        "            print(\"Invalid input. Please enter numeric values.\")\n",
        "\n",
        "# Widgets for inputs\n",
        "ul_longitude_input = widgets.Text(placeholder=\"UL Longitude\")\n",
        "ul_latitude_input = widgets.Text(placeholder=\"UL Latitude\")\n",
        "br_longitude_input = widgets.Text(placeholder=\"BR Longitude\")\n",
        "br_latitude_input = widgets.Text(placeholder=\"BR Latitude\")\n",
        "\n",
        "submit_button = widgets.Button(description=\"OK\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "submit_button.on_click(on_button_click)\n",
        "\n",
        "# Display layout\n",
        "display(widgets.HBox([widgets.Label(\"Upper-left Longitude:\"), ul_longitude_input]))\n",
        "display(widgets.HBox([widgets.Label(\"Upper-left Latitude:\"), ul_latitude_input]))\n",
        "display(widgets.HBox([widgets.Label(\"Bottom-right Longitude:\"), br_longitude_input]))\n",
        "display(widgets.HBox([widgets.Label(\"Bottom-right Latitude:\"), br_latitude_input]))\n",
        "display(submit_button)\n",
        "display(output_area)\n",
        "\n",
        "##  Upper-left  (Lon -90.0, Lat 40.0)\n",
        "##  Bottom-right (Lon -89.0, Lat 39.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define polygon geometry from upper-left and bottom-right points\n",
        "# Extract coordinates\n",
        "ul_lon, ul_lat = upper_left\n",
        "br_lon, br_lat = bottom_right\n",
        "\n",
        "# Define polygon coordinates in counter-clockwise order\n",
        "polygon_coords = [\n",
        "    [ul_lon, ul_lat],        # upper-left\n",
        "    [br_lon, ul_lat],        # upper-right\n",
        "    [br_lon, br_lat],        # bottom-right\n",
        "    [ul_lon, br_lat],        # bottom-left\n",
        "    [ul_lon, ul_lat]         # close the polygon\n",
        "]\n",
        "\n",
        "# Create an Earth Engine Polygon\n",
        "polygon = ee.Geometry.Polygon([polygon_coords])\n",
        "\n",
        "# Wrap in a FeatureCollection\n",
        "feature_collection = ee.FeatureCollection([\n",
        "    ee.Feature(polygon, {\"id_num\": 0})\n",
        "])\n",
        "\n",
        "print(\"Polygon FeatureCollection defined.\")"
      ],
      "metadata": {
        "id": "P81Z0sDWnrXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_collection.getInfo()"
      ],
      "metadata": {
        "id": "nImn7d2OpIz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZEEwnoR3tvw"
      },
      "outputs": [],
      "source": [
        "# @title Visualization - spatial distribution of sites\n",
        "import folium\n",
        "from folium import Figure\n",
        "\n",
        "fig = Figure(width=800, height=600)\n",
        "m = folium.Map(location=[42, -95.25], zoom_start=4)\n",
        "\n",
        "\n",
        "roi_geojson = feature_collection.getInfo()\n",
        "folium.TileLayer(\n",
        "    tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "    attr='Google Satellite',\n",
        "    name='Satellite',\n",
        "    overlay=True\n",
        ").add_to(m)\n",
        "folium.GeoJson(roi_geojson).add_to(m)\n",
        "\n",
        "fig.add_child(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHUbWlcdw0UD"
      },
      "source": [
        "## Read ee.images (constant images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title read images to array\n",
        "\n",
        "# Landcover\n",
        "LC = ee.ImageCollection('USGS/NLCD_RELEASES/2019_REL/NLCD').filterDate('2016-01-01', '2022-12-31').select('landcover').first()\n",
        "lc_proj = LC.projection()\n",
        "\n",
        "#polaris\n",
        "bd_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_0_5').first().rename('bd_0_5')\n",
        "bd_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_5_15').first().rename('bd_5_15')\n",
        "bd_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_15_30').first().rename('bd_15_30')\n",
        "bd_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_30_60').first().rename('bd_30_60')\n",
        "bd_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_60_100').first().rename('bd_60_100')\n",
        "bd_0_100 = bd_0_5.multiply(0.05).add(bd_5_15.multiply(0.1)).add(bd_15_30.multiply(0.15)).add(bd_30_60.multiply(0.3)).add(bd_60_100.multiply(0.4)).rename('bd_0_100')\n",
        "\n",
        "clay_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_0_5').first().rename('clay_0_5')\n",
        "clay_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_5_15').first().rename('clay_5_15')\n",
        "clay_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_15_30').first().rename('clay_15_30')\n",
        "clay_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_30_60').first().rename('clay_30_60')\n",
        "clay_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_60_100').first().rename('clay_60_100')\n",
        "clay_0_100 = clay_0_5.multiply(0.05).add(clay_5_15.multiply(0.1)).add(clay_15_30.multiply(0.15)).add(clay_30_60.multiply(0.3)).add(clay_60_100.multiply(0.4)).rename('clay_0_100')\n",
        "\n",
        "ksat_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_0_5').first().rename('ksat_0_5')\n",
        "ksat_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_5_15').first().rename('ksat_5_15')\n",
        "ksat_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_15_30').first().rename('ksat_15_30')\n",
        "ksat_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_30_60').first().rename('ksat_30_60')\n",
        "ksat_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_60_100').first().rename('ksat_60_100')\n",
        "\n",
        "\n",
        "sand_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_0_5').first().rename('sand_0_5')\n",
        "sand_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_5_15').first().rename('sand_5_15')\n",
        "sand_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_15_30').first().rename('sand_15_30')\n",
        "sand_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_30_60').first().rename('sand_30_60')\n",
        "sand_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_60_100').first().rename('sand_60_100')\n",
        "sand_0_100 = sand_0_5.multiply(0.05).add(sand_5_15.multiply(0.1)).add(sand_15_30.multiply(0.15)).add(sand_30_60.multiply(0.3)).add(sand_60_100.multiply(0.4)).rename('sand_0_100')\n",
        "\n",
        "silt_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_0_5').first().rename('silt_0_5')\n",
        "silt_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_5_15').first().rename('silt_5_15')\n",
        "silt_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_15_30').first().rename('silt_15_30')\n",
        "silt_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_30_60').first().rename('silt_30_60')\n",
        "silt_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_60_100').first().rename('silt_60_100')\n",
        "silt_0_100 = silt_0_5.multiply(0.05).add(silt_5_15.multiply(0.1)).add(silt_15_30.multiply(0.15)).add(silt_30_60.multiply(0.3)).add(silt_60_100.multiply(0.4)).rename('silt_0_100')\n",
        "\n",
        "polaris = bd_0_5.addBands(bd_0_100)\\\n",
        "          .addBands(ksat_0_5).addBands(ksat_0_5)\\\n",
        "          .addBands(clay_0_5).addBands(clay_0_100)\\\n",
        "          .addBands(sand_0_5).addBands(sand_0_100)\\\n",
        "          .addBands(silt_0_5).addBands(silt_0_100)\n",
        "\n",
        "polaris_ksat = ksat_0_5.addBands(ksat_5_15).addBands(ksat_15_30).addBands(ksat_30_60).addBands(ksat_60_100)"
      ],
      "metadata": {
        "id": "G36xwW7L7dT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region = feature_collection.geometry()\n",
        "\n",
        "print(\"land cover:\")\n",
        "LC_100m = LC.reproject(crs='EPSG:4326', scale=100)\n",
        "LC_arr = geemap.ee_to_numpy(\n",
        "    LC_100m,\n",
        "    region=region,\n",
        "    scale=100,\n",
        ")\n",
        "print(LC_arr.shape)\n",
        "\n",
        "print(\"polaris:\")\n",
        "polaris_100m = polaris.reproject(crs='EPSG:4326', scale=100)\n",
        "polaris_arr = geemap.ee_to_numpy(\n",
        "    polaris_100m,\n",
        "    region=region,\n",
        "    scale=100,\n",
        ")\n",
        "print(polaris_arr.shape)\n",
        "\n",
        "polaris_ksat_100m = polaris_ksat.reproject(crs='EPSG:4326', scale=100)\n",
        "polaris_ksat_arr = geemap.ee_to_numpy(\n",
        "    polaris_ksat_100m,\n",
        "    region=region,\n",
        "    scale=100,\n",
        ")\n",
        "print(polaris_ksat_arr.shape)"
      ],
      "metadata": {
        "id": "MD-_OLiSAplH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert units of soil properties for ML inputs\n",
        "\n",
        "polaris_arr[:, :, 2] = np.power(10, polaris_arr[:, :, 2])  # convert to cm/hr\n",
        "polaris_ksat_arr[:, :, 0] = np.power(10, polaris_ksat_arr[:, :, 0])\n",
        "polaris_ksat_arr[:, :, 1] = np.power(10, polaris_ksat_arr[:, :, 1])\n",
        "polaris_ksat_arr[:, :, 2] = np.power(10, polaris_ksat_arr[:, :, 2])\n",
        "polaris_ksat_arr[:, :, 3] = np.power(10, polaris_ksat_arr[:, :, 3])\n",
        "polaris_ksat_arr[:, :, 4] = np.power(10, polaris_ksat_arr[:, :, 4])\n",
        "\n",
        "d0_5 = 0.05\n",
        "d5_15 = 0.10\n",
        "d15_30 = 0.15\n",
        "d30_60 = 0.30\n",
        "d60_100 = 0.40\n",
        "d_total = d0_5 + d5_15 + d15_30 + d30_60 + d60_100\n",
        "polaris_arr[:, :, 3] = d_total/(d0_5/polaris_ksat_arr[:, :, 0] +\n",
        "                                d5_15/polaris_ksat_arr[:, :, 1] +\n",
        "                                d15_30/polaris_ksat_arr[:, :, 2] +\n",
        "                                d30_60/polaris_ksat_arr[:, :, 3] +\n",
        "                                d60_100/polaris_ksat_arr[:, :, 4])"
      ],
      "metadata": {
        "id": "7p8p8jH2GWq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max ksat at surface: \", polaris_arr[:, :, 2].max())\n",
        "print(\"min ksat at surface: \", polaris_arr[:, :, 2].min())\n",
        "print(\"max ksat at rootzone: \", polaris_arr[:, :, 3].max())\n",
        "print(\"min ksat at rootzone: \", polaris_arr[:, :, 3].min())\n",
        "print(\"number of nan values in soil property data: \", np.count_nonzero(np.isnan(polaris_arr)))"
      ],
      "metadata": {
        "id": "jILzEu3elDip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load DEM and read to array"
      ],
      "metadata": {
        "id": "WD2zr2gW7eD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dem_image = ee.Image('projects/ee-scai62/assets/COUNS_dem100m')\n",
        "TWII_image = ee.Image('projects/ee-scai62/assets/CONUS_dem100m_TWII')\n",
        "aspect_image = ee.Image('projects/ee-scai62/assets/CONUS_dem100m_aspect')\n",
        "slopeDegree_image = ee.Image('projects/ee-scai62/assets/CONUS_dem100m_slopeDegree')\n",
        "\n",
        "dem_image = dem_image.reproject(lc_proj)\n",
        "TWII_image = TWII_image.reproject(lc_proj)\n",
        "aspect_image = aspect_image.reproject(lc_proj)\n",
        "slopeDegree_image = slopeDegree_image.reproject(lc_proj)\n",
        "\n",
        "dem_image = dem_image.reproject(crs='EPSG:4326', scale=100)\n",
        "TWII_image = TWII_image.reproject(crs='EPSG:4326', scale=100)\n",
        "aspect_image = aspect_image.reproject(crs='EPSG:4326', scale=100)\n",
        "slopeDegree_image = slopeDegree_image.reproject(crs='EPSG:4326', scale=100)"
      ],
      "metadata": {
        "id": "1TLBC8YnDr2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dem_arr = geemap.ee_to_numpy(dem_image, region=region, scale=100)\n",
        "dem_arr[dem_arr<-1000] = np.nan\n",
        "\n",
        "twi_arr = geemap.ee_to_numpy(TWII_image, region=region, scale=100)\n",
        "\n",
        "aspect_arr = geemap.ee_to_numpy(aspect_image, region=region, scale=100)\n",
        "sinAsp_arr = np.sin(aspect_arr/180*np.pi)\n",
        "sinAsp_arr[aspect_arr==-1] = 0\n",
        "cosAsp_arr = np.cos(aspect_arr/180*np.pi)\n",
        "cosAsp_arr[aspect_arr == -1] = 0\n",
        "\n",
        "slope_arr = geemap.ee_to_numpy(slopeDegree_image, region=region, scale=100)"
      ],
      "metadata": {
        "id": "lT4nX9i3orL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Duplicate the last column of dem_arr to match the width of other arrays\n",
        "# last_column_dem = dem_arr[:, -1:, :]\n",
        "# dem_arr = np.concatenate((dem_arr, last_column_dem), axis=1)\n",
        "\n",
        "# last_column_twi = twi_arr[:, -1:, :]\n",
        "# twi_arr = np.concatenate((twi_arr, last_column_twi), axis=1)\n",
        "\n",
        "# last_column_sinAsp = sinAsp_arr[:, -1:, :]\n",
        "# sinAsp_arr = np.concatenate((sinAsp_arr, last_column_sinAsp), axis=1)\n",
        "\n",
        "# last_column_cosAsp = cosAsp_arr[:, -1:, :]\n",
        "# cosAsp_arr = np.concatenate((cosAsp_arr, last_column_cosAsp), axis=1)\n",
        "\n",
        "# last_column_slope = slope_arr[:, -1:, :]\n",
        "# slope_arr = np.concatenate((slope_arr, last_column_slope), axis=1)\n",
        "\n",
        "\n",
        "# # Print the new shapes to verify\n",
        "# print(\"New dem_arr shape: \", dem_arr.shape)\n",
        "# print(\"New twi_arr shape: \", twi_arr.shape)\n",
        "# print(\"New sinAsp_arr shape: \", sinAsp_arr.shape)\n",
        "# print(\"New cosAsp_arr shape: \", cosAsp_arr.shape)\n",
        "# print(\"New slope_arr shape: \", slope_arr.shape)\n",
        "\n",
        "# dem_arr = dem_arr[:LC_arr.shape[0], :LC_arr.shape[1], 0]\n",
        "# twi_arr = twi_arr[:LC_arr.shape[0], :LC_arr.shape[1], 0]\n",
        "# sinAsp_arr = sinAsp_arr[:LC_arr.shape[0], :LC_arr.shape[1], 0]\n",
        "# cosAsp_arr = cosAsp_arr[:LC_arr.shape[0], :LC_arr.shape[1], 0]\n",
        "# slope_arr = slope_arr[:LC_arr.shape[0], :LC_arr.shape[1], 0]\n",
        "\n",
        "# print(\"dem_arr shape: \", dem_arr.shape)\n",
        "# print(\"twi_arr shape: \", twi_arr.shape)\n",
        "# print(\"sinAsp_arr shape: \", sinAsp_arr.shape)\n",
        "# print(\"cosAsp_arr shape: \", cosAsp_arr.shape)\n",
        "# print(\"slope_arr shape: \", slope_arr.shape)"
      ],
      "metadata": {
        "id": "elDvqjueaHbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irpDaBGt7m-W"
      },
      "source": [
        "## Define study area and period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRJHKFkR7mgg"
      },
      "outputs": [],
      "source": [
        "# prompt: get the boundary of feature_collection\n",
        "region = feature_collection.geometry()\n",
        "\n",
        "start_date = '2024-01-01'\n",
        "end_date = '2025-01-01'\n",
        "\n",
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "\n",
        "num_hours = int((end_datetime - start_datetime).total_seconds()/3600)\n",
        "print(num_hours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QL9ACxF7h2t"
      },
      "source": [
        "## Get ee imagecollections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5HQkf4L7ijj"
      },
      "outputs": [],
      "source": [
        "# @title Era5-land\n",
        "start_date_era5 = (datetime.strptime(start_date, '%Y-%m-%d')- timedelta(hours=120+24)).strftime('%Y-%m-%d')\n",
        "\n",
        "era5hour_short = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\")\\\n",
        "                    .filterBounds(region).filterDate(start_date_era5, end_date)\\\n",
        "                    .map(lambda img: img.addBands(\n",
        "                        img.select('u_component_of_wind_10m').hypot(img.select('v_component_of_wind_10m')).rename('wind_10m')\n",
        "                        )).select([\n",
        "                            'temperature_2m',    # 2m air temperature, K\n",
        "                            'dewpoint_temperature_2m',  # 2m dew point temperature, K\n",
        "                            'wind_10m',  # wind speed, m/s\n",
        "                            'surface_pressure',  # atmospheric surface pressure, Pa\n",
        "                            ])\n",
        "\n",
        "\n",
        "era5hour_mid = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").filterBounds(region).filterDate(start_date_era5, end_date).select([\n",
        "    'surface_solar_radiation_downwards_hourly',  # solar radiation, J\n",
        "    'surface_thermal_radiation_downwards_hourly',  # thermal radiation, J\n",
        "    ])\n",
        "\n",
        "\n",
        "era5hour_long = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").filterBounds(region).filterDate(start_date_era5, end_date).select([\n",
        "    'total_evaporation_hourly',  # total evaporation, m\n",
        "    'total_precipitation_hourly',   # precipitation, m\n",
        "    ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L--AZmxi8Kx-"
      },
      "outputs": [],
      "source": [
        "# @title HLSL30\n",
        "\n",
        "###note: no scale factor for HLSL30 product.\n",
        "\n",
        "def bitwiseExtract(img, fromBit, toBit):\n",
        "  maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
        "  mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
        "  return img.rightShift(fromBit).bitwiseAnd(mask)\n",
        "\n",
        "# remove low quality data\n",
        "def maskHLSL30(image):\n",
        "  qcDay = image.select('Fmask')\n",
        "  cloud = bitwiseExtract(qcDay, 1, 1).eq(0)\n",
        "  cloudshadow = bitwiseExtract(qcDay, 3, 3).eq(0)\n",
        "  snowice = bitwiseExtract(qcDay, 4, 4).eq(0)\n",
        "  water = bitwiseExtract(qcDay, 5, 5).eq(0)\n",
        "  aerosol = bitwiseExtract(qcDay, 6, 7).lte(2)\n",
        "  mask = cloud.And(cloudshadow).And(snowice).And(water).And(aerosol)\n",
        "\n",
        "  return    image.updateMask(mask).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "\n",
        "HLSL30 = ee.ImageCollection(\"NASA/HLS/HLSL30/v002\").filterBounds(region)\\\n",
        "    .filterDate(start_date, end_date).map(maskHLSL30) \\\n",
        "\t\t.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B9', 'B10', 'B11'])\n",
        "\n",
        "\n",
        "\n",
        "HLSL30_timestamps = HLSL30.aggregate_array('system:time_start')\n",
        "HLSL30_datetime_list = HLSL30_timestamps.map(\n",
        "    lambda t: ee.Date(t).format('YYYY-MM-dd HH')\n",
        ")\n",
        "HLSL30_datetime_list_py = HLSL30_datetime_list.getInfo()\n",
        "print(HLSL30_datetime_list_py[:10])  # preview first 10\n",
        "print(\"Total unique timestamps:\", len(set(HLSL30_datetime_list_py)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8OPoVwv8NSC"
      },
      "outputs": [],
      "source": [
        "# @title Sentinel-2\n",
        "# Sentinel-2 images\n",
        "\n",
        "# old version\n",
        "# def maskSentinel2(img):\n",
        "#   cloudOpaqueBitMask = (1 << 10);\n",
        "#   cloudCirrusMask = (1 << 11);\n",
        "#   # Get the pixel QA band.\n",
        "#   qa = img.select('QA60')\n",
        "#   # Both flags should be set to zero, indicating clear conditions.\n",
        "#   mask = qa.bitwiseAnd(cloudOpaqueBitMask).eq(0) \\\n",
        "#                 .And(qa.bitwiseAnd(cloudCirrusMask).eq(0))\n",
        "#   return img.updateMask(mask).copyProperties(img, ['system:time_start'])                  #.multiply(0.0001).toFloat().copyProperties(img, ['mydate'])  # after applying updateMask(). all properties will be lost\n",
        "\n",
        "\n",
        "def maskSentinel2(img):\n",
        "  # Get the pixel QA band.\n",
        "  scl = img.select('SCL')\n",
        "  mask = scl.neq(8).And(scl.neq(9)).And(scl.neq(10)).And(scl.neq(11))\n",
        "  return img.updateMask(mask).copyProperties(img, ['system:time_start'])\n",
        "\n",
        "\n",
        "def add_date(img):\n",
        "  date_start = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd-HH')\n",
        "  return img.set('mydate', date_start)\n",
        "\n",
        "\n",
        "Sentinel2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\\\n",
        "            .filterDate(start_date, end_date).filterBounds(region)\\\n",
        "            .map(maskSentinel2)\\\n",
        "            .select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'])\n",
        "\n",
        "\n",
        "# Sentinel2 = Sentinel2.map(add_date)\n",
        "\n",
        "\n",
        "\n",
        "Sentinel2_timestamps = Sentinel2.aggregate_array('system:time_start')\n",
        "Sentinel2_datetime_list = Sentinel2_timestamps.map(\n",
        "    lambda t: ee.Date(t).format('YYYY-MM-dd HH')\n",
        ")\n",
        "Sentinel2_datetime_list_py = Sentinel2_datetime_list.getInfo()\n",
        "print(Sentinel2_datetime_list_py[:10])  # preview first 10\n",
        "print(\"Total unique timestamps:\", len(set(Sentinel2_datetime_list_py)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eol3scCY8Prg"
      },
      "outputs": [],
      "source": [
        "# @title Sentinel-1\n",
        "\n",
        "def preprocess_vv(image):\n",
        "    vv_masked = image.updateMask(image.gt(-20).And(image.lt(-5)))\n",
        "    vv_filtered = vv_masked.convolve(ee.Kernel.gaussian(3))\n",
        "    return vv_filtered #.rename('VV').copyProperties(image, ['system:time_start'])\n",
        "\n",
        "# Define preprocessing for VH\n",
        "def preprocess_vh(image):\n",
        "    vh_masked = image.updateMask(image.gt(-30).And(image.lt(-10)))\n",
        "    vh_filtered = vh_masked.convolve(ee.Kernel.gaussian(3))\n",
        "    return vh_filtered #.rename('VH').copyProperties(image, ['system:time_start'])\n",
        "\n",
        "\n",
        "def merge_bands(image):\n",
        "    vv = image.select('VV')\n",
        "    vh = image.select('VH')\n",
        "    angle = image.select('angle')\n",
        "\n",
        "    vv_prep = preprocess_vv(vv)      # Apply mask + smoothing to VV\n",
        "    vh_prep = preprocess_vh(vh)      # Apply mask + smoothing to VH\n",
        "\n",
        "    merged = vv_prep.addBands(vh_prep).addBands(angle.rename('angle'))\n",
        "\n",
        "    return merged.copyProperties(image, ['system:time_start'])\n",
        "\n",
        "def to_float(image):\n",
        "    all_bands = image.bandNames()\n",
        "    return image.select(all_bands).float().copyProperties(image, ['system:time_start'])\n",
        "\n",
        "Sentinel1 = (\n",
        "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filterDate(start_date, end_date)\n",
        "    .filterBounds(region)\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
        "    # .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .sort('SLC_Processing_start')\n",
        "    .map(merge_bands).map(to_float)\n",
        ")\n",
        "\n",
        "\n",
        "Sentinel1_timestamps = Sentinel1.aggregate_array('system:time_start')\n",
        "Sentinel1_datetime_list = Sentinel1_timestamps.map(\n",
        "    lambda t: ee.Date(t).format('YYYY-MM-dd HH')\n",
        ")\n",
        "Sentinel1_datetime_list_py = Sentinel1_datetime_list.getInfo()\n",
        "print(Sentinel1_datetime_list_py[:10])  # preview first 10\n",
        "print(\"Total unique timestamps:\", len(set(Sentinel1_datetime_list_py)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run ML models"
      ],
      "metadata": {
        "id": "D05Cio2jRelm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load ML models\n",
        "ML1_sly_scaler = joblib.load('./StartFolder/TrainedML/ML1_sly_scaler.joblib')\n",
        "ML1_rly_scaler = joblib.load('./StartFolder/TrainedML/ML1_rly_scaler.joblib')\n",
        "ML2_sly_scaler = joblib.load('./StartFolder/TrainedML/ML2_sly_scaler.joblib')\n",
        "ML2_rly_scaler = joblib.load('./StartFolder/TrainedML/ML2_rly_scaler.joblib')\n",
        "ML3_sly_scaler = joblib.load('./StartFolder/TrainedML/ML3_sly_scaler.joblib')\n",
        "ML3_rly_scaler = joblib.load('./StartFolder/TrainedML/ML3_rly_scaler.joblib')\n",
        "# ML1-sly\n",
        "ML1_sly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML1_sly_lgb_mean.joblib')\n",
        "ML1_sly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML1_sly_mapie_cqr.joblib')\n",
        "# ML1 - rly\n",
        "ML1_rly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML1_rly_lgb_mean.joblib')\n",
        "ML1_rly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML1_rly_mapie_cqr.joblib')\n",
        "# ML2-sly\n",
        "ML2_sly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML2_sly_lgb_mean.joblib')\n",
        "ML2_sly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML2_sly_mapie_cqr.joblib')\n",
        "# ML2-rly\n",
        "ML2_rly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML2_rly_lgb_mean.joblib')\n",
        "ML2_rly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML2_rly_mapie_cqr.joblib')\n",
        "# ML3-sly\n",
        "ML3_sly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML3_sly_lgb_mean.joblib')\n",
        "ML3_sly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML3_sly_mapie_cqr.joblib')\n",
        "# ML3-rly\n",
        "ML3_rly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML3_rly_lgb_mean.joblib')\n",
        "ML3_rly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML3_rly_mapie_cqr.joblib')"
      ],
      "metadata": {
        "id": "IOn_CrOvyIa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean:\", ML1_sly_scaler.mean_)\n",
        "# print(\"Standard Deviation:\", ML1_sly_scaler.scale_)\n",
        "# print(\"Mean:\", ML1_rly_scaler.mean_)\n",
        "# print(\"Standard Deviation:\", ML1_rly_scaler.scale_)\n",
        "print(\"Mean:\", ML2_sly_scaler.mean_)\n",
        "# print(\"Standard Deviation:\", ML2_sly_scaler.scale_)\n",
        "# print(\"Mean:\", ML2_rly_scaler.mean_)\n",
        "# print(\"Standard Deviation:\", ML2_rly_scaler.scale_)\n",
        "print(\"Mean:\", ML3_sly_scaler.mean_)\n",
        "# print(\"Standard Deviation:\", ML3_sly_scaler.scale_)\n",
        "# print(\"Mean:\", ML3_rly_scaler.mean_)\n",
        "# print(\"Standard Deviation:\", ML3_rly_scaler.scale_)"
      ],
      "metadata": {
        "id": "4qTho-Hk6uQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_weights(n):\n",
        "    weights = 1.0 / np.arange(n, 0, -1)      # reversed sequence [n ... 1]\n",
        "    weights = weights / weights.sum()        # normalize\n",
        "    return weights, ee.List(weights.tolist())\n",
        "\n",
        "# Short-term (24h)\n",
        "short_term_weights, ee_short_term_weights = make_weights(24)\n",
        "# print(\"Short-term weights:\", short_term_weights)\n",
        "\n",
        "# Mid-term (72h)\n",
        "mid_term_weights, ee_mid_term_weights = make_weights(72)\n",
        "# print(\"Mid-term weights:\", mid_term_weights)\n",
        "\n",
        "# Long-term (120h)\n",
        "long_term_weights, ee_long_term_weights = make_weights(120)\n",
        "# print(\"Long-term weights:\", long_term_weights)\n",
        "\n",
        "def weighted_sum(imgcol, weights):\n",
        "    # Ensure both image list and weights are same size\n",
        "    img_list = imgcol.toList(imgcol.size())\n",
        "    zipped   = img_list.zip(weights)\n",
        "\n",
        "    weighted_list = zipped.map(\n",
        "        lambda pair: ee.Image(ee.List(pair).get(0)).multiply(ee.Number(ee.List(pair).get(1)))\n",
        "    )\n",
        "\n",
        "    return ee.ImageCollection(weighted_list).sum()\n"
      ],
      "metadata": {
        "id": "b8blbTxT-O--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows, cols = LC_arr.shape[:2]# shape of your land cover array\n",
        "# Create 1D coordinate vectors\n",
        "lon_1d = np.linspace(ul_lon, br_lon, num=cols)\n",
        "lat_1d = np.linspace(ul_lat, br_lat, num=rows)\n",
        "# Make 2D coordinate grids\n",
        "lon_arr, lat_arr = np.meshgrid(lon_1d, lat_1d)\n",
        "\n",
        "LCmask = LC_arr.reshape(-1) == 82  # only cropland\n",
        "\n",
        "\n",
        "HLSL30_set = set(HLSL30_datetime_list_py)\n",
        "S2_set = set(Sentinel2_datetime_list_py)\n",
        "S1_set = set(Sentinel1_datetime_list_py)"
      ],
      "metadata": {
        "id": "waXAK4EY-zr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.transform import from_origin\n",
        "\n",
        "def arr_2_tif(y_hat_2d, lat_arr, lon_arr, filename, epsg=4326):\n",
        "    \"\"\"\n",
        "    Save a 2D result array as a GeoTIFF given latitude and longitude grids.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_hat_2d : np.ndarray\n",
        "        2D array with results (rows x cols).\n",
        "    lat_arr : np.ndarray\n",
        "        2D array of latitudes (same shape as y_hat_2d).\n",
        "    lon_arr : np.ndarray\n",
        "        2D array of longitudes (same shape as y_hat_2d).\n",
        "    filename : str\n",
        "        Output file name (e.g., \"result.tif\").\n",
        "    epsg : int, optional\n",
        "        EPSG code for the CRS (default = 4326, WGS84).\n",
        "    \"\"\"\n",
        "\n",
        "    rows, cols = y_hat_2d.shape\n",
        "\n",
        "    # Derive resolution from first differences\n",
        "    res_lat = abs(lat_arr[1, 0] - lat_arr[0, 0])   # pixel height\n",
        "    res_lon = abs(lon_arr[0, 1] - lon_arr[0, 0])   # pixel width\n",
        "\n",
        "    # Upper-left corner (rasterio expects top-left origin)\n",
        "    ul_lat = lat_arr[0, 0]\n",
        "    ul_lon = lon_arr[0, 0]\n",
        "\n",
        "    # Create affine transform\n",
        "    transform = from_origin(ul_lon, ul_lat, res_lon, res_lat)\n",
        "\n",
        "    # Write GeoTIFF\n",
        "    with rasterio.open(\n",
        "        filename,\n",
        "        \"w\",\n",
        "        driver=\"GTiff\",\n",
        "        height=rows,\n",
        "        width=cols,\n",
        "        count=1,\n",
        "        dtype=y_hat_2d.dtype,\n",
        "        crs=f\"EPSG:{epsg}\",\n",
        "        transform=transform,\n",
        "    ) as dst:\n",
        "        dst.write(y_hat_2d, 1)"
      ],
      "metadata": {
        "id": "ZkP2Q5KOtNrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rrrows, cccols = LC_arr.shape[:2]\n",
        "\n",
        "current_dt = start_datetime\n",
        "while current_dt <= end_datetime:\n",
        "  dt_str = current_dt.strftime('%Y-%m-%d %H')  # match your lists\n",
        "  dt_str_4_filename = current_dt.strftime('%Y%m%d%H')\n",
        "  doy = current_dt.timetuple().tm_yday\n",
        "\n",
        "  # HLSL30\n",
        "  if dt_str in HLSL30_set:\n",
        "    print(\"------HLSL30------\", current_dt)\n",
        "    img = HLSL30.filterDate(current_dt, current_dt + timedelta(hours=1)).reduce(ee.Reducer.mean()).unmask(-9999)\n",
        "    if img:\n",
        "        if os.path.exists(f'HLSL30_surface_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif'):\n",
        "          current_dt += timedelta(hours=1)\n",
        "          continue\n",
        "\n",
        "        HLSL30_arr = geemap.ee_to_numpy(img, region=region, scale=100) # , default_value=-999\n",
        "        HLSL30_arr[HLSL30_arr<-9998] = np.nan\n",
        "\n",
        "        # ---- Short term (24h) ----\n",
        "        era5hour_short_last24 = era5hour_short.filterDate(\n",
        "            current_dt - timedelta(hours=23), current_dt\n",
        "        )\n",
        "        weighted_era5hour_short = weighted_sum(era5hour_short_last24, ee_short_term_weights)\n",
        "        era5hour_short_arr = geemap.ee_to_numpy(weighted_era5hour_short, region=region, scale=100)\n",
        "        # print(\"Short term shape:\", era5hour_short_arr.shape)\n",
        "\n",
        "        # ---- Mid term (72h) ----\n",
        "        era5hour_mid_last72 = era5hour_mid.filterDate(\n",
        "            current_dt - timedelta(hours=71), current_dt\n",
        "        )\n",
        "        weighted_era5hour_mid = weighted_sum(era5hour_mid_last72, ee_mid_term_weights)\n",
        "        era5hour_mid_arr = geemap.ee_to_numpy(weighted_era5hour_mid, region=region, scale=100)\n",
        "        # print(\"Mid term shape:\", era5hour_mid_arr.shape)\n",
        "\n",
        "        # ---- Long term (120h) ----\n",
        "        era5hour_long_last120 = era5hour_long.filterDate(\n",
        "            current_dt - timedelta(hours=119), current_dt\n",
        "        )\n",
        "        weighted_era5hour_long = weighted_sum(era5hour_long_last120, ee_long_term_weights)\n",
        "        era5hour_long_arr = geemap.ee_to_numpy(weighted_era5hour_long, region=region, scale=100)\n",
        "        # print(\"Long term shape:\", era5hour_long_arr.shape)\n",
        "\n",
        "        X = np.full((len(LC_arr.reshape(-1)), 30), np.nan)\n",
        "        X = X.astype(np.float32)\n",
        "        X[:, 0] = np.ones(LC_arr.size, dtype=np.float32) * doy\n",
        "        X[:, 1] = lat_arr.reshape(-1)[:]\n",
        "        X[:, 2] = lon_arr.reshape(-1)[:]\n",
        "        X[:, 3] = dem_arr.reshape(-1)[:]\n",
        "        X[:, 4] = slope_arr.reshape(-1)[:]\n",
        "        X[:, 5] = sinAsp_arr.reshape(-1)[:]\n",
        "        X[:, 6] = cosAsp_arr.reshape(-1)[:]\n",
        "        X[:, 7] = twi_arr.reshape(-1)[:]\n",
        "        X[:, 8] = polaris_arr[:,:,0].reshape(-1)[:]\n",
        "        X[:, 9] = polaris_arr[:,:,6].reshape(-1)[:]\n",
        "        X[:, 10] = polaris_arr[:,:,4].reshape(-1)[:]\n",
        "        X[:, 11] = polaris_arr[:,:,2].reshape(-1)[:]\n",
        "        X[:, 12] = era5hour_short_arr[:,:,1].reshape(-1)[:]\n",
        "        X[:, 13] = era5hour_short_arr[:,:,0].reshape(-1)[:]\n",
        "        X[:, 14] = era5hour_mid_arr[:,:,0].reshape(-1)[:]\n",
        "        X[:, 15] = era5hour_mid_arr[:,:,1].reshape(-1)[:]\n",
        "        X[:, 16] = era5hour_long_arr[:,:,0].reshape(-1)[:]\n",
        "        X[:, 17] = era5hour_short_arr[:,:,2].reshape(-1)[:]\n",
        "        X[:, 18] = era5hour_short_arr[:,:,3].reshape(-1)[:]\n",
        "        X[:, 19] = era5hour_long_arr[:,:,1].reshape(-1)[:][:]\n",
        "        B2 = HLSL30_arr[:, :, 0].reshape(-1)  # B2\n",
        "        B3 = HLSL30_arr[:, :, 1].reshape(-1)  # B3\n",
        "        B4 = HLSL30_arr[:, :, 2].reshape(-1)  # B4\n",
        "        X[:, 20] = HLSL30_arr[:, :, 3].reshape(-1) # B5\n",
        "        X[:, 21] = HLSL30_arr[:, :, 4].reshape(-1)  # B6\n",
        "        X[:, 22] = HLSL30_arr[:, :, 5].reshape(-1)  # B7\n",
        "        X[:, 23] = HLSL30_arr[:, :, 7].reshape(-1)  # B10\n",
        "        X[:, 24] = HLSL30_arr[:, :, 8].reshape(-1)  # B11\n",
        "        X[:, 25] = (X[:, 20]- B4) / (X[:, 20] + B4)  # NDVI\n",
        "        X[:, 26] = (B3 - X[:, 20]) / (B3 + X[:, 20])  # NDWI\n",
        "        X[:, 27] = -0.3599 * B2 - 0.3533 * B3 - 0.4734 * B4 + 0.6633 *X[:, 20] - 0.0087 * X[:, 21] - 0.2856 * X[:, 22]  # greenness\n",
        "        X[:, 28] = 0.3510 * B2 + 0.3813 * B3 + 0.3437 * B4 + 0.7196 * X[:, 20] + 0.2396 * X[:, 21] + 0.1949 * X[:, 22]  # brightness\n",
        "        X[:, 29] = 0.2578 * B2 + 0.2305 * B3 + 0.0883 * B4 + 0.1071 * X[:, 20] - 0.7611 * X[:, 21] - 0.5308 * X[:, 22]  # wetness\n",
        "\n",
        "\n",
        "        startt = time.time()\n",
        "\n",
        "        X_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isinf(X), axis=1) & LCmask\n",
        "        print(\"surface layer\")\n",
        "        print(f'valid pixels: {np.count_nonzero(X_mask)}/{len(X_mask)}')\n",
        "        if np.count_nonzero(X_mask) != 0:\n",
        "          ML3_sly_y_hat = np.full((X.shape[0]), np.nan)\n",
        "          # ML3_sly_y_up_hat = np.full((X.shape[0]), np.nan)\n",
        "          # ML3_sly_y_low_hat = np.full((X.shape[0]), np.nan)\n",
        "          ML3_sly_y_hat[X_mask] = ML3_sly_lgb_mean.predict(ML3_sly_scaler.transform(X[X_mask, :]))\n",
        "          # y_pred_cqr, y_pis_cqr = ML3_sly_mapie_cqr.predict_interval(ML3_sly_scaler.transform(X[X_mask, :]))\n",
        "          # ML3_sly_y_up_hat[X_mask] = y_pis_cqr[:, 1, 0]\n",
        "          # ML3_sly_y_low_hat[X_mask] = y_pis_cqr[:, 0, 0]\n",
        "\n",
        "          ML3_sly_y_hat_2d = ML3_sly_y_hat.reshape(rrrows, cccols)\n",
        "          arr_2_tif(ML3_sly_y_hat_2d, lat_arr, lon_arr, f'HLSL30_surface_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif')\n",
        "\n",
        "        endt = time.time()\n",
        "        print(f\"Elapsed time: {endt - startt:.4f} seconds\")\n",
        "\n",
        "\n",
        "        X[:, 8] = polaris_arr[:,:,1].reshape(-1)[:]\n",
        "        X[:, 9] = polaris_arr[:,:,7].reshape(-1)[:]\n",
        "        X[:, 10] = polaris_arr[:,:,5].reshape(-1)[:]\n",
        "        X[:, 11] = polaris_arr[:,:,3].reshape(-1)[:]\n",
        "\n",
        "\n",
        "        startt = time.time()\n",
        "\n",
        "        X_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isinf(X), axis=1) & LCmask\n",
        "        print(\"rootzone\")\n",
        "        print(f'valid pixels: {np.count_nonzero(X_mask)}/{len(X_mask)}')\n",
        "        if np.count_nonzero(X_mask) != 0:\n",
        "          ML3_rly_y_hat = np.full((X.shape[0]), np.nan)\n",
        "          # ML3_rly_y_up_hat = np.full((X.shape[0]), np.nan)\n",
        "          # ML3_rly_y_low_hat = np.full((X.shape[0]), np.nan)\n",
        "          ML3_rly_y_hat[X_mask] = ML3_rly_lgb_mean.predict(ML3_rly_scaler.transform(X[X_mask, :]))\n",
        "          # y_pred_cqr, y_pis_cqr = ML3_rly_mapie_cqr.predict_interval(ML3_rly_scaler.transform(X[X_mask, :]))\n",
        "          # ML3_rly_y_up_hat[X_mask] = y_pis_cqr[:, 1, 0]\n",
        "          # ML3_rly_y_low_hat[X_mask] = y_pis_cqr[:, 0, 0]\n",
        "\n",
        "          ML3_rly_y_hat_2d = ML3_rly_y_hat.reshape(rrrows, cccols)\n",
        "          arr_2_tif(ML3_rly_y_hat_2d, lat_arr, lon_arr, f'HLSL30_rootzone_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif')\n",
        "\n",
        "        endt = time.time()\n",
        "        print(f\"Elapsed time: {endt - startt:.4f} seconds\")\n",
        "\n",
        "        # print(ML3_sly_y_hat.shape)\n",
        "        # print(ML3_sly_y_hat)\n",
        "\n",
        "\n",
        "  current_dt += timedelta(hours=1)\n",
        "\n",
        "\n",
        "  ##########################################"
      ],
      "metadata": {
        "id": "Co7BL4Kr7Jvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rrrows, cccols = LC_arr.shape[:2]\n",
        "\n",
        "current_dt = start_datetime\n",
        "while current_dt <= end_datetime:\n",
        "  dt_str = current_dt.strftime('%Y-%m-%d %H')  # match your lists\n",
        "  dt_str_4_filename = current_dt.strftime('%Y%m%d%H')\n",
        "  doy = current_dt.timetuple().tm_yday\n",
        "\n",
        "\n",
        "  # Sentinel2\n",
        "  if dt_str in S2_set:\n",
        "    print(\"------Sentinel2------\", current_dt)\n",
        "    img = Sentinel2.filterDate(current_dt, current_dt + timedelta(hours=1)).reduce(ee.Reducer.mean()).unmask(-9999)\n",
        "    if img:\n",
        "      if os.path.exists(f'Sentinel2_surface_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif'):\n",
        "        current_dt += timedelta(hours=1)\n",
        "        continue\n",
        "\n",
        "      sentinel2_arr = geemap.ee_to_numpy(img, region=region, scale=100)\n",
        "      sentinel2_arr[sentinel2_arr<-9998] = np.nan\n",
        "\n",
        "      # ---- Short term (24h) ----\n",
        "      era5hour_short_last24 = era5hour_short.filterDate(\n",
        "          current_dt - timedelta(hours=23), current_dt\n",
        "      )\n",
        "      weighted_era5hour_short = weighted_sum(era5hour_short_last24, ee_short_term_weights)\n",
        "      era5hour_short_arr = geemap.ee_to_numpy(weighted_era5hour_short, region=region, scale=100)\n",
        "\n",
        "      # ---- Mid term (72h) ----\n",
        "      era5hour_mid_last72 = era5hour_mid.filterDate(\n",
        "          current_dt - timedelta(hours=71), current_dt\n",
        "      )\n",
        "      weighted_era5hour_mid = weighted_sum(era5hour_mid_last72, ee_mid_term_weights)\n",
        "      era5hour_mid_arr = geemap.ee_to_numpy(weighted_era5hour_mid, region=region, scale=100)\n",
        "\n",
        "      # ---- Long term (120h) ----\n",
        "      era5hour_long_last120 = era5hour_long.filterDate(\n",
        "          current_dt - timedelta(hours=119), current_dt\n",
        "      )\n",
        "      weighted_era5hour_long = weighted_sum(era5hour_long_last120, ee_long_term_weights)\n",
        "      era5hour_long_arr = geemap.ee_to_numpy(weighted_era5hour_long, region=region, scale=100)\n",
        "\n",
        "\n",
        "      X = np.full((len(LC_arr.reshape(-1)), 32), np.nan)\n",
        "      X = X.astype(np.float32)\n",
        "      X[:, 0] = np.ones(LC_arr.size, dtype=np.float32) * doy\n",
        "      X[:, 1] = lat_arr.reshape(-1)[:]\n",
        "      X[:, 2] = lon_arr.reshape(-1)[:]\n",
        "      X[:, 3] = dem_arr.reshape(-1)[:]\n",
        "      X[:, 4] = slope_arr.reshape(-1)[:]\n",
        "      X[:, 5] = sinAsp_arr.reshape(-1)[:]\n",
        "      X[:, 6] = cosAsp_arr.reshape(-1)[:]\n",
        "      X[:, 7] = twi_arr.reshape(-1)[:]\n",
        "      X[:, 8] = polaris_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 9] = polaris_arr[:,:,6].reshape(-1)[:]\n",
        "      X[:, 10] = polaris_arr[:,:,4].reshape(-1)[:]\n",
        "      X[:, 11] = polaris_arr[:,:,2].reshape(-1)[:]\n",
        "      X[:, 12] = era5hour_short_arr[:,:,1].reshape(-1)[:]\n",
        "      X[:, 13] = era5hour_short_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 14] = era5hour_mid_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 15] = era5hour_mid_arr[:,:,1].reshape(-1)[:]\n",
        "      X[:, 16] = era5hour_long_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 17] = era5hour_short_arr[:,:,2].reshape(-1)[:]\n",
        "      X[:, 18] = era5hour_short_arr[:,:,3].reshape(-1)[:]\n",
        "      X[:, 19] = era5hour_long_arr[:,:,1].reshape(-1)[:][:]\n",
        "      B2 = sentinel2_arr[:, :, 0].reshape(-1) * 0.0001  # B2\n",
        "      B3 = sentinel2_arr[:, :, 1].reshape(-1) * 0.0001  # B3\n",
        "      B4 = sentinel2_arr[:, :, 2].reshape(-1) * 0.0001  # B4\n",
        "      X[:, 20] = sentinel2_arr[:, :, 3].reshape(-1) * 0.0001 # B5\n",
        "      X[:, 21] = sentinel2_arr[:, :, 4].reshape(-1) * 0.0001  # B6\n",
        "      X[:, 22] = sentinel2_arr[:, :, 5].reshape(-1) * 0.0001  # B7\n",
        "      X[:, 23] = sentinel2_arr[:, :, 6].reshape(-1) * 0.0001  # B8\n",
        "      X[:, 24] = sentinel2_arr[:, :, 7].reshape(-1) * 0.0001  # B8A\n",
        "      X[:, 25] = sentinel2_arr[:, :, 8].reshape(-1) * 0.0001  # B11\n",
        "      X[:, 26] = sentinel2_arr[:, :, 9].reshape(-1) * 0.0001  # B12\n",
        "      X[:, 27] = (X[:, 23]- B4) / (X[:, 23] + B4)  # NDVI\n",
        "      X[:, 28] = (B3 - X[:, 23]) / (B3 + X[:, 23])  # NDWI\n",
        "      X[:, 29] = -0.3599 * B2 - 0.3533 * B3 - 0.4734 * B4 + 0.6633 *X[:, 23] - 0.0087 * X[:, 25] - 0.2856 * X[:, 26]  # greenness\n",
        "      X[:, 30] = 0.3510 * B2 + 0.3813 * B3 + 0.3437 * B4 + 0.7196 * X[:, 23] + 0.2396 * X[:, 25] + 0.1949 * X[:, 26]  # brightness\n",
        "      X[:, 31] = 0.2578 * B2 + 0.2305 * B3 + 0.0883 * B4 + 0.1071 * X[:, 23] - 0.7611 * X[:, 25] - 0.5308 * X[:, 26]  # wetness\n",
        "\n",
        "\n",
        "      # row_means = np.mean(X, axis=0)\n",
        "      # print(row_means.shape)\n",
        "      # print(row_means)\n",
        "\n",
        "      startt = time.time()\n",
        "\n",
        "      X_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isinf(X), axis=1) & LCmask\n",
        "      print(\"surface layer\")\n",
        "      print(f'valid pixels: {np.count_nonzero(X_mask)}/{len(X_mask)}')\n",
        "      if np.count_nonzero(X_mask) != 0:\n",
        "        ML2_sly_y_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML2_sly_y_up_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML2_sly_y_low_hat = np.full((X.shape[0]), np.nan)\n",
        "        ML2_sly_y_hat[X_mask] = ML2_sly_lgb_mean.predict(ML2_sly_scaler.transform(X[X_mask, :]))\n",
        "        # y_pred_cqr, y_pis_cqr = ML2_sly_mapie_cqr.predict_interval(ML2_sly_scaler.transform(X[X_mask, :]))\n",
        "        # ML2_sly_y_up_hat[X_mask] = y_pis_cqr[:, 1, 0]\n",
        "        # ML2_sly_y_low_hat[X_mask] = y_pis_cqr[:, 0, 0]\n",
        "\n",
        "        ML2_sly_y_hat_2d = ML2_sly_y_hat.reshape(rrrows, cccols)\n",
        "        arr_2_tif(ML2_sly_y_hat_2d, lat_arr, lon_arr, f'Sentinel2_surface_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif')\n",
        "\n",
        "      endt = time.time()\n",
        "      print(f\"Elapsed time: {endt - startt:.4f} seconds\")\n",
        "\n",
        "      X[:, 8] = polaris_arr[:,:,1].reshape(-1)[:]\n",
        "      X[:, 9] = polaris_arr[:,:,7].reshape(-1)[:]\n",
        "      X[:, 10] = polaris_arr[:,:,5].reshape(-1)[:]\n",
        "      X[:, 11] = polaris_arr[:,:,3].reshape(-1)[:]\n",
        "\n",
        "      startt = time.time()\n",
        "\n",
        "      X_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isinf(X), axis=1) & LCmask\n",
        "      print(\"rootzone\")\n",
        "      print(f'valid pixels: {np.count_nonzero(X_mask)}/{len(X_mask)}')\n",
        "      if np.count_nonzero(X_mask) != 0:\n",
        "        ML2_rly_y_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML2_rly_y_up_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML2_rly_y_low_hat = np.full((X.shape[0]), np.nan)\n",
        "        ML2_rly_y_hat[X_mask] = ML2_rly_lgb_mean.predict(ML2_rly_scaler.transform(X[X_mask, :]))\n",
        "        # y_pred_cqr, y_pis_cqr = ML2_rly_mapie_cqr.predict_interval(ML2_rly_scaler.transform(X[X_mask, :]))\n",
        "        # ML2_rly_y_up_hat[X_mask] = y_pis_cqr[:, 1, 0]\n",
        "        # ML2_rly_y_low_hat[X_mask] = y_pis_cqr[:, 0, 0]\n",
        "\n",
        "        ML2_rly_y_hat_2d = ML2_rly_y_hat.reshape(rrrows, cccols)\n",
        "        arr_2_tif(ML2_rly_y_hat_2d, lat_arr, lon_arr, f'Sentinel2_rootzone_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif')\n",
        "\n",
        "      endt = time.time()\n",
        "      print(f\"Elapsed time: {endt - startt:.4f} seconds\")\n",
        "\n",
        "      # print(ML2_sly_y_hat.shape)\n",
        "      # print(ML2_sly_y_hat)\n",
        "\n",
        "\n",
        "  current_dt += timedelta(hours=1)"
      ],
      "metadata": {
        "id": "bMbiQIpx7JNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rrrows, cccols = LC_arr.shape[:2]\n",
        "########################################\n",
        "\n",
        "current_dt = start_datetime\n",
        "while current_dt <= end_datetime:\n",
        "  dt_str = current_dt.strftime('%Y-%m-%d %H')  # match your lists\n",
        "  dt_str_4_filename = current_dt.strftime('%Y%m%d%H')\n",
        "  doy = current_dt.timetuple().tm_yday\n",
        "\n",
        "  # Sentinel1\n",
        "  if dt_str in S1_set:\n",
        "    print(\"------Sentinel1------\", current_dt)\n",
        "    img = Sentinel1.filterDate(current_dt, current_dt + timedelta(hours=1)).reduce(ee.Reducer.mean()).unmask(-9999)\n",
        "    if img:\n",
        "      if os.path.exists(f'Sentinel1_surface_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif'):\n",
        "        current_dt += timedelta(hours=1)\n",
        "        continue\n",
        "\n",
        "      sentinel1_arr = geemap.ee_to_numpy(img, region=region, scale=100)\n",
        "      sentinel1_arr[sentinel1_arr<-9998] = np.nan\n",
        "\n",
        "      # ---- Short term (24h) ----\n",
        "      era5hour_short_last24 = era5hour_short.filterDate(\n",
        "          current_dt - timedelta(hours=23), current_dt\n",
        "      )\n",
        "      weighted_era5hour_short = weighted_sum(era5hour_short_last24, ee_short_term_weights)\n",
        "      era5hour_short_arr = geemap.ee_to_numpy(weighted_era5hour_short, region=region, scale=100)\n",
        "\n",
        "      # ---- Mid term (72h) ----\n",
        "      era5hour_mid_last72 = era5hour_mid.filterDate(\n",
        "          current_dt - timedelta(hours=71), current_dt\n",
        "      )\n",
        "      weighted_era5hour_mid = weighted_sum(era5hour_mid_last72, ee_mid_term_weights)\n",
        "      era5hour_mid_arr = geemap.ee_to_numpy(weighted_era5hour_mid, region=region, scale=100)\n",
        "\n",
        "      # ---- Long term (120h) ----\n",
        "      era5hour_long_last120 = era5hour_long.filterDate(\n",
        "          current_dt - timedelta(hours=119), current_dt\n",
        "      )\n",
        "      weighted_era5hour_long = weighted_sum(era5hour_long_last120, ee_long_term_weights)\n",
        "      era5hour_long_arr = geemap.ee_to_numpy(weighted_era5hour_long, region=region, scale=100)\n",
        "\n",
        "\n",
        "      X = np.full((len(LC_arr.reshape(-1)), 26), np.nan)\n",
        "      X = X.astype(np.float32)\n",
        "      X[:, 0] = np.ones(LC_arr.size, dtype=np.float32) * doy\n",
        "      X[:, 1] = lat_arr.reshape(-1)[:]\n",
        "      X[:, 2] = lon_arr.reshape(-1)[:]\n",
        "      X[:, 3] = dem_arr.reshape(-1)[:]\n",
        "      X[:, 4] = slope_arr.reshape(-1)[:]\n",
        "      X[:, 5] = sinAsp_arr.reshape(-1)[:]\n",
        "      X[:, 6] = cosAsp_arr.reshape(-1)[:]\n",
        "      X[:, 7] = twi_arr.reshape(-1)[:]\n",
        "      X[:, 8] = polaris_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 9] = polaris_arr[:,:,6].reshape(-1)[:]\n",
        "      X[:, 10] = polaris_arr[:,:,4].reshape(-1)[:]\n",
        "      X[:, 11] = polaris_arr[:,:,2].reshape(-1)[:]\n",
        "      X[:, 12] = era5hour_short_arr[:,:,1].reshape(-1)[:]\n",
        "      X[:, 13] = era5hour_short_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 14] = era5hour_mid_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 15] = era5hour_mid_arr[:,:,1].reshape(-1)[:]\n",
        "      X[:, 16] = era5hour_long_arr[:,:,0].reshape(-1)[:]\n",
        "      X[:, 17] = era5hour_short_arr[:,:,2].reshape(-1)[:]\n",
        "      X[:, 18] = era5hour_short_arr[:,:,3].reshape(-1)[:]\n",
        "      X[:, 19] = era5hour_long_arr[:,:,1].reshape(-1)[:][:]\n",
        "      X[:, 20] = sentinel1_arr[:, :, 0].reshape(-1)\n",
        "      X[:, 21] = sentinel1_arr[:, :, 1].reshape(-1)\n",
        "      X[:, 22] = sentinel1_arr[:, :, 2].reshape(-1)\n",
        "      X[:, 23] = sentinel1_arr[:, :, 0].reshape(-1) / sentinel1_arr[:, :, 1].reshape(-1)\n",
        "      X[:, 24] = (sentinel1_arr[:, :, 0].reshape(-1)**2 + sentinel1_arr[:, :, 1].reshape(-1)**2)/np.sqrt(2)\n",
        "      X[:, 25] = (sentinel1_arr[:, :, 0].reshape(-1) + sentinel1_arr[:, :, 1].reshape(-1)) / (sentinel1_arr[:, :, 0].reshape(-1) - sentinel1_arr[:, :, 1].reshape(-1))\n",
        "\n",
        "\n",
        "      startt = time.time()\n",
        "\n",
        "      X_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isinf(X), axis=1) & LCmask\n",
        "      print(\"surface layer\")\n",
        "      print(f'valid pixels: {np.count_nonzero(X_mask)}/{len(X_mask)}')\n",
        "      if np.count_nonzero(X_mask) != 0:\n",
        "        ML1_sly_y_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML1_sly_y_up_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML1_sly_y_low_hat = np.full((X.shape[0]), np.nan)\n",
        "        ML1_sly_y_hat[X_mask] = ML1_sly_lgb_mean.predict(ML1_sly_scaler.transform(X[X_mask, :]))\n",
        "        # y_pred_cqr, y_pis_cqr = ML1_sly_mapie_cqr.predict_interval(ML1_sly_scaler.transform(X[X_mask, :]))\n",
        "        # ML1_sly_y_up_hat[X_mask] = y_pis_cqr[:, 1, 0]\n",
        "        # ML1_sly_y_low_hat[X_mask] = y_pis_cqr[:, 0, 0]\n",
        "\n",
        "        ML1_sly_y_hat_2d = ML1_sly_y_hat.reshape(rrrows, cccols)\n",
        "        arr_2_tif(ML1_sly_y_hat_2d, lat_arr, lon_arr, f'Sentinel1_surface_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif')\n",
        "\n",
        "      endt = time.time()\n",
        "      print(f\"Elapsed time: {endt - startt:.4f} seconds\")\n",
        "\n",
        "      X[:, 8] = polaris_arr[:,:,1].reshape(-1)[:]\n",
        "      X[:, 9] = polaris_arr[:,:,7].reshape(-1)[:]\n",
        "      X[:, 10] = polaris_arr[:,:,5].reshape(-1)[:]\n",
        "      X[:, 11] = polaris_arr[:,:,3].reshape(-1)[:]\n",
        "\n",
        "\n",
        "      startt = time.time()\n",
        "\n",
        "      X_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isinf(X), axis=1) & LCmask\n",
        "      print(\"rootzone\")\n",
        "      print(f'valid pixels: {np.count_nonzero(X_mask)}/{len(X_mask)}')\n",
        "      if np.count_nonzero(X_mask) != 0:\n",
        "        ML1_rly_y_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML1_rly_y_up_hat = np.full((X.shape[0]), np.nan)\n",
        "        # ML1_rly_y_low_hat = np.full((X.shape[0]), np.nan)\n",
        "        ML1_rly_y_hat[X_mask] = ML1_rly_lgb_mean.predict(ML1_rly_scaler.transform(X[X_mask, :]))\n",
        "        # y_pred_cqr, y_pis_cqr = ML1_rly_mapie_cqr.predict_interval(ML1_rly_scaler.transform(X[X_mask, :]))\n",
        "        # ML1_rly_y_up_hat[X_mask] = y_pis_cqr[:, 1, 0]\n",
        "        # ML1_rly_y_low_hat[X_mask] = y_pis_cqr[:, 0, 0]\n",
        "\n",
        "        ML1_rly_y_hat_2d = ML1_rly_y_hat.reshape(rrrows, cccols)\n",
        "        arr_2_tif(ML1_rly_y_hat_2d, lat_arr, lon_arr, f'Sentinel1_rootzone_{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_{dt_str_4_filename}.tif')\n",
        "\n",
        "      endt = time.time()\n",
        "      print(f\"Elapsed time: {endt - startt:.4f} seconds\")\n",
        "\n",
        "      # print(ML1_sly_y_hat.shape)\n",
        "      # print(ML1_sly_y_hat)\n",
        "\n",
        "\n",
        "  current_dt += timedelta(hours=1)"
      ],
      "metadata": {
        "id": "OhumSY_dReKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Searching files:**\n",
        "\n",
        "Search for all tiff files exported from above ML model and store file names in a list."
      ],
      "metadata": {
        "id": "1YCdGyc4jEdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Search for files\n",
        "tif_files = glob.glob(f'*{ul_lon*10:.0f}{br_lon*10:.0f}{br_lat*10:.0f}{ul_lat*10:.0f}_*.tif')\n",
        "\n",
        "# Print the first and last file names\n",
        "if tif_files:\n",
        "    print(\"First .tif file:\", tif_files[0])\n",
        "    print(\"Last .tif file:\", tif_files[-1])\n",
        "else:\n",
        "    print(\"No .tif files found.\")\n",
        "\n",
        "# Print the total number of specific .tif files\n",
        "print(\"Total number of .tif files:\", len(tif_files))"
      ],
      "metadata": {
        "id": "Ee6jpu7dONSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37f4898"
      },
      "source": [
        "**Downloading files:**\n",
        "\n",
        "To download the files, you can use the following command in a new code cell. This will create a zip file containing all the `.tif` files and then download it to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "714e5079"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "if tif_files:\n",
        "    # Create a zip file of the specific .tif files\n",
        "    zip_filename = 'tif_files.zip'\n",
        "    !zip {zip_filename} {\" \".join(tif_files)}\n",
        "\n",
        "    # Download the zip file\n",
        "    files.download(zip_filename)\n",
        "else:\n",
        "    print(\"No specific .tif files found to download.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIIcpimyFHIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4UnkgRHmFHHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYUpv7u6FHGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQjX0hRLFHFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AAuIqsGFG4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ObLCQiieFGuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1mt711IfzW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4S_ZB3YNrrK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}