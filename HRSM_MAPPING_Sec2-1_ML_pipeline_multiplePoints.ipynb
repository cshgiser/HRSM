{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cshgiser/HRSM/blob/main/HRSM_MAPPING_Sec2-1_ML_pipeline_multiplePoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok1qkt3pvrpi"
      },
      "source": [
        "## Initial settings (authorize, connect GD, install packages, etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2YjXZTBuyY8"
      },
      "outputs": [],
      "source": [
        "# @title get GEE authorization\n",
        "import ee\n",
        "\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "# Initialize the library.\n",
        "ee.Initialize(project='ee-scai62')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFzcQBXFzb3P"
      },
      "outputs": [],
      "source": [
        "# @title Connect to my Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr9sC06bziKw"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/gdrive/My Drive/NIFA_Download/StartFolder/CDL_sites/' ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MWGkiKt518nz"
      },
      "outputs": [],
      "source": [
        "# @title Install necessary packages\n",
        "!pip install folium\n",
        "!pip install geopandas\n",
        "!pip install netCDF4\n",
        "!pip install --upgrade xee\n",
        "!pip install rasterio\n",
        "!pip install ipywidgets\n",
        "!pip install permetrics==2.0.0\n",
        "!pip install mapie\n",
        "!pip install scikit-learn==1.5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku6RglgO2FUR"
      },
      "outputs": [],
      "source": [
        "# @title Import packages\n",
        "from mapie.regression import SplitConformalRegressor, ConformalizedQuantileRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "import folium\n",
        "from folium import Figure\n",
        "import geopandas as gpd\n",
        "import json\n",
        "import geemap\n",
        "import numpy as np\n",
        "from shapely.geometry import mapping\n",
        "import pandas as pd\n",
        "import netCDF4 as nc\n",
        "import xarray\n",
        "import rasterio\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from rasterio.crs import CRS\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import joblib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from permetrics import RegressionMetric\n",
        "import osgeo.gdal as gdal\n",
        "# import richdem as rd\n",
        "# import pyflwdir\n",
        "from osgeo import osr, gdalconst\n",
        "from scipy.stats import rankdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR7GKJyKwmqJ"
      },
      "source": [
        "## Define study site: randomly choose points\n",
        "\n",
        "This is a demo for ML runing pipeline, showing you how to run the trained ML models online for multiple points.\n",
        "\n",
        "In this case study, we are going to generate stratified samples based on the irrigation calssification data and climate region data.\n",
        "\n",
        "Irrigation maps:\n",
        "\n",
        "Xie, Y., & Lark, T. J. (2021). Mapping annual irrigation from Landsat imagery and environmental variables across the conterminous United States. Remote Sensing of Environment, 260, 112445. https://doi.org/10.1016/j.rse.2021.112445\n",
        "\n",
        "climate classification maps:\n",
        "\n",
        "Beck, H. E., Zimmermann, N. E., McVicar, T. R., Vergopolan, N., Berg, A., & Wood, E. F. (2018). Present and future KÃ¶ppen-Geiger climate classification maps at 1-km resolution. Scientific data, 5(1), 1-12. https://doi.org/10.1038/sdata.2018.214\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU5c9zeOU6Nv"
      },
      "outputs": [],
      "source": [
        "#@title Request irrigation data\n",
        "\n",
        "LANID2018_20 = ee.Image(\"users/xyhuwmir/LANID/update/LANID2018-2020\")\n",
        "LANID2016 = ee.Image(\"projects/ee-scai62/assets/lanid2016\")\n",
        "LANID2017 = ee.Image(\"projects/ee-scai62/assets/lanid2017\")\n",
        "LANID2018 = LANID2018_20.select('irMap18')\n",
        "LANID2019 = LANID2018_20.select('irMap19')\n",
        "LANID2020 = LANID2018_20.select('irMap20')\n",
        "LANID_irrigation = (\n",
        "    LANID2018.eq(1)\n",
        "    .And(LANID2019.eq(1))\n",
        "    .And(LANID2020.eq(1))\n",
        "    .And(LANID2016.eq(1))\n",
        "    .And(LANID2017.eq(1))\n",
        ")\n",
        "\n",
        "LANID_binary = ee.Image.constant(2).where(LANID_irrigation.eq(1), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeRm170h7aWN"
      },
      "outputs": [],
      "source": [
        "#@title Request climate data\n",
        "koppen_geiger_COUNS_1991_2020 = ee.Image(\"projects/ee-scai62/assets/koppen_geiger_COUNS_1991_2020\")\n",
        "\n",
        "\"\"\"\n",
        "14: Cfa  Temperate, no dry season, hot summer  [200 255 80]\n",
        "25: Dfa  Cold, no dry season, hot summer       [0 255 255]\n",
        "\n",
        "4:  BWh  Arid, desert, hot                     [255 0 0]\n",
        "5:  BWk  Arid, desert, cold                    [255 150 150]\n",
        "6:  BSh  Arid, steppe, hot                     [245 165 0]\n",
        "7:  BSk  Arid, steppe, cold                    [255 220 100]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "koppen_reclassified = (\n",
        "    koppen_geiger_COUNS_1991_2020\n",
        "    .where(koppen_geiger_COUNS_1991_2020.eq(14), 1)\n",
        "    .where(\n",
        "        koppen_geiger_COUNS_1991_2020.eq(4)\n",
        "        .Or(koppen_geiger_COUNS_1991_2020.eq(5))\n",
        "        .Or(koppen_geiger_COUNS_1991_2020.eq(6))\n",
        "        .Or(koppen_geiger_COUNS_1991_2020.eq(7)),\n",
        "        2\n",
        "    )\n",
        "    .where(\n",
        "        koppen_geiger_COUNS_1991_2020.neq(14)\n",
        "        .And(koppen_geiger_COUNS_1991_2020.neq(4))\n",
        "        .And(koppen_geiger_COUNS_1991_2020.neq(5))\n",
        "        .And(koppen_geiger_COUNS_1991_2020.neq(6))\n",
        "        .And(koppen_geiger_COUNS_1991_2020.neq(7)),\n",
        "        0\n",
        "    )\n",
        ")\n",
        "# print(koppen_reclassified.getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEFqDOsubag-"
      },
      "outputs": [],
      "source": [
        "# @title Combine two classification layers\n",
        "final_image = LANID_binary.multiply(100).add(koppen_reclassified)\n",
        "band_names = final_image.bandNames()\n",
        "print(band_names.getInfo())\n",
        "\n",
        "valid_classes = ee.List([101, 102, 201, 202])\n",
        "\n",
        "masked_image = final_image.updateMask(\n",
        "    final_image.select('constant').remap(valid_classes, ee.List.repeat(1, valid_classes.size()))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh20Dj1VTC9Z"
      },
      "outputs": [],
      "source": [
        "# @title The next cell is too slow, you can use this alternative: export csv file to google drive first, then load it here\n",
        "\n",
        "# region = ee.FeatureCollection('projects/ee-scai62/assets/openET_states')\n",
        "# scale = masked_image.projection().nominalScale()\n",
        "\n",
        "# random_points = masked_image.stratifiedSample(\n",
        "#     numPoints=200,\n",
        "#     classBand='constant',\n",
        "#     region=region.geometry(),\n",
        "#     scale=scale,\n",
        "#     geometries=True,\n",
        "#     classValues=[101, 102, 201, 202],\n",
        "#     classPoints=[50, 50, 50, 50],\n",
        "#     seed=42\n",
        "# )\n",
        "\n",
        "# def add_coords(feature):\n",
        "#     coords = feature.geometry().coordinates()\n",
        "#     return feature.set({\n",
        "#         'longitude': coords.get(0),\n",
        "#         'latitude': coords.get(1)\n",
        "#     })\n",
        "\n",
        "# random_points = random_points.map(add_coords)\n",
        "\n",
        "\n",
        "# # Export random_points to Google Drive as CSV\n",
        "# export_task = ee.batch.Export.table.toDrive(\n",
        "#     collection=random_points,\n",
        "#     description='random_points_irrigation_climate',\n",
        "#     # folder='GEE_exports',   # optional: folder name in your Drive\n",
        "#     fileNamePrefix='random_points_irrigation_climate',\n",
        "#     fileFormat='CSV'\n",
        "# )\n",
        "\n",
        "# # Start the export task\n",
        "# export_task.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KSMwt2ICm_d"
      },
      "outputs": [],
      "source": [
        "# @title Generate random points within  classes\n",
        "import os\n",
        "\n",
        "if os.path.exists('random_points_irrigation_climate.csv'):\n",
        "  df_random_points = pd.read_csv('random_points_irrigation_climate.csv')\n",
        "else:\n",
        "  region = ee.FeatureCollection('projects/ee-scai62/assets/openET_states')\n",
        "\n",
        "  random_points = final_image.stratifiedSample(\n",
        "      numPoints=200,\n",
        "      classBand='constant',\n",
        "      region=region.geometry(),\n",
        "      scale=30,\n",
        "      geometries=True,\n",
        "      classValues=[101, 102, 201, 202],\n",
        "      classPoints=[50, 50, 50, 50],\n",
        "      seed=42\n",
        "  )\n",
        "\n",
        "  def add_coords(feature):\n",
        "      coords = feature.geometry().coordinates()\n",
        "      return feature.set({\n",
        "          'longitude': coords.get(0),\n",
        "          'latitude': coords.get(1)\n",
        "      })\n",
        "\n",
        "  random_points = random_points.map(add_coords)\n",
        "\n",
        "  fc_dict = random_points.getInfo()\n",
        "\n",
        "  df_random_points = pd.DataFrame([{\n",
        "      'latitude': f['properties']['latitude'],\n",
        "      'longitude': f['properties']['longitude'],\n",
        "      'cropland': f['properties']['cropland']\n",
        "  } for f in fc_dict['features']])\n",
        "\n",
        "  df_random_points['ID'] = df_random_points.index\n",
        "  df_random_points.to_csv('random_points_2016_2024.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5idb1G3WCMu"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_random_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHiJ8xPoWVJg"
      },
      "outputs": [],
      "source": [
        "# @title Define feature collections\n",
        "lons_sub = ee.List(df_random_points['longitude'].tolist())\n",
        "lats_sub = ee.List(df_random_points['latitude'].tolist())\n",
        "idnum_sub = ee.List(df_random_points['ID'].tolist())\n",
        "constant_sub = ee.List(df_random_points['constant'].tolist())\n",
        "\n",
        "# Combine all lists together\n",
        "combined = lons_sub.zip(lats_sub).zip(idnum_sub).zip(constant_sub)\n",
        "\n",
        "# Create FeatureCollection with geometry and properties\n",
        "points = combined.map(lambda x:\n",
        "    ee.Feature(\n",
        "        ee.Geometry.Point([\n",
        "            ee.List(ee.List(ee.List(x).get(0)).get(0)).get(0),  # lon\n",
        "            ee.List(ee.List(ee.List(x).get(0)).get(0)).get(1)   # lat\n",
        "        ]),\n",
        "        {\n",
        "            'id_num': ee.List(ee.List(x).get(0)).get(1),       # id_num\n",
        "            'constant': ee.List(x).get(1)                      # constant\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "feature_collection = ee.FeatureCollection(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziTvkDttJrpr"
      },
      "outputs": [],
      "source": [
        "# @title Visualize CDL image and random points\n",
        "Map = geemap.Map(center=[40, -100], zoom=4)\n",
        "\n",
        "# Add random_points layer\n",
        "Map.addLayer(feature_collection, {'color': 'red'}, 'Random Points')\n",
        "# Map.addLayer(region)\n",
        "\n",
        "Map.addLayerControl()\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHUbWlcdw0UD"
      },
      "source": [
        "## Read ee.images (constant images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADCf_9j1GbTn"
      },
      "outputs": [],
      "source": [
        "#@title read images to array\n",
        "\n",
        "# Landcover\n",
        "LC = ee.ImageCollection('USGS/NLCD_RELEASES/2019_REL/NLCD').filterDate('2016-01-01', '2018-01-01').select('landcover').first()\n",
        "lc_proj = LC.projection()\n",
        "\n",
        "#polaris\n",
        "bd_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_0_5').first().rename('bd_0_5')\n",
        "bd_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_5_15').first().rename('bd_5_15')\n",
        "bd_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_15_30').first().rename('bd_15_30')\n",
        "bd_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_30_60').first().rename('bd_30_60')\n",
        "bd_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/bd_mean').filterMetadata('system:index', 'equals', 'bd_60_100').first().rename('bd_60_100')\n",
        "bd_0_100 = bd_0_5.multiply(0.05).add(bd_5_15.multiply(0.1)).add(bd_15_30.multiply(0.15)).add(bd_30_60.multiply(0.3)).add(bd_60_100.multiply(0.4)).rename('bd_0_100')\n",
        "\n",
        "clay_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_0_5').first().rename('clay_0_5')\n",
        "clay_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_5_15').first().rename('clay_5_15')\n",
        "clay_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_15_30').first().rename('clay_15_30')\n",
        "clay_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_30_60').first().rename('clay_30_60')\n",
        "clay_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/clay_mean').filterMetadata('system:index', 'equals', 'clay_60_100').first().rename('clay_60_100')\n",
        "clay_0_100 = clay_0_5.multiply(0.05).add(clay_5_15.multiply(0.1)).add(clay_15_30.multiply(0.15)).add(clay_30_60.multiply(0.3)).add(clay_60_100.multiply(0.4)).rename('clay_0_100')\n",
        "\n",
        "ksat_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_0_5').first().rename('ksat_0_5')\n",
        "ksat_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_5_15').first().rename('ksat_5_15')\n",
        "ksat_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_15_30').first().rename('ksat_15_30')\n",
        "ksat_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_30_60').first().rename('ksat_30_60')\n",
        "ksat_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/ksat_mean').filterMetadata('system:index', 'equals', 'ksat_60_100').first().rename('ksat_60_100')\n",
        "\n",
        "\n",
        "sand_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_0_5').first().rename('sand_0_5')\n",
        "sand_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_5_15').first().rename('sand_5_15')\n",
        "sand_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_15_30').first().rename('sand_15_30')\n",
        "sand_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_30_60').first().rename('sand_30_60')\n",
        "sand_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/sand_mean').filterMetadata('system:index', 'equals', 'sand_60_100').first().rename('sand_60_100')\n",
        "sand_0_100 = sand_0_5.multiply(0.05).add(sand_5_15.multiply(0.1)).add(sand_15_30.multiply(0.15)).add(sand_30_60.multiply(0.3)).add(sand_60_100.multiply(0.4)).rename('sand_0_100')\n",
        "\n",
        "silt_0_5 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_0_5').first().rename('silt_0_5')\n",
        "silt_5_15 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_5_15').first().rename('silt_5_15')\n",
        "silt_15_30 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_15_30').first().rename('silt_15_30')\n",
        "silt_30_60 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_30_60').first().rename('silt_30_60')\n",
        "silt_60_100 = ee.ImageCollection('projects/sat-io/open-datasets/polaris/silt_mean').filterMetadata('system:index', 'equals', 'silt_60_100').first().rename('silt_60_100')\n",
        "silt_0_100 = silt_0_5.multiply(0.05).add(silt_5_15.multiply(0.1)).add(silt_15_30.multiply(0.15)).add(silt_30_60.multiply(0.3)).add(silt_60_100.multiply(0.4)).rename('silt_0_100')\n",
        "\n",
        "polaris = bd_0_5.addBands(bd_0_100)\\\n",
        "          .addBands(ksat_0_5).addBands(ksat_0_5)\\\n",
        "          .addBands(clay_0_5).addBands(clay_0_100)\\\n",
        "          .addBands(sand_0_5).addBands(sand_0_100)\\\n",
        "          .addBands(silt_0_5).addBands(silt_0_100)\n",
        "\n",
        "polaris_ksat = ksat_0_5.addBands(ksat_5_15).addBands(ksat_15_30).addBands(ksat_30_60).addBands(ksat_60_100)\n",
        "\n",
        "\n",
        "dem_image = ee.Image('projects/ee-scai62/assets/COUNS_dem100m')\n",
        "TWII_image = ee.Image('projects/ee-scai62/assets/CONUS_dem100m_TWII')\n",
        "aspect_image = ee.Image('projects/ee-scai62/assets/CONUS_dem100m_aspect')\n",
        "slopeDegree_image = ee.Image('projects/ee-scai62/assets/CONUS_dem100m_slopeDegree')\n",
        "\n",
        "dem_image = dem_image.reproject(lc_proj)\n",
        "TWII_image = TWII_image.reproject(lc_proj)\n",
        "aspect_image = aspect_image.reproject(lc_proj)\n",
        "slopeDegree_image = slopeDegree_image.reproject(lc_proj)\n",
        "\n",
        "dem_image = dem_image.reproject(crs='EPSG:4326', scale=100)\n",
        "TWII_image = TWII_image.reproject(crs='EPSG:4326', scale=100)\n",
        "aspect_image = aspect_image.reproject(crs='EPSG:4326', scale=100)\n",
        "slopeDegree_image = slopeDegree_image.reproject(crs='EPSG:4326', scale=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us0_zHMEHZGM"
      },
      "outputs": [],
      "source": [
        "# @title extract values from constant images\n",
        "\n",
        "dem_values = dem_image.sampleRegions(collection=feature_collection,scale=100)\n",
        "elevation_arr = np.array(dem_values.aggregate_array('b1').getInfo())[:]\n",
        "dem_idnum_arr = np.array(dem_values.aggregate_array('id_num').getInfo())[:]\n",
        "TWII_values = TWII_image.sampleRegions(collection=feature_collection,scale=100)\n",
        "TWII_arr = np.array(TWII_values.aggregate_array('b1').getInfo())[:]\n",
        "aspect_values = aspect_image.sampleRegions(collection=feature_collection,scale=100)\n",
        "aspect_arr = np.array(aspect_values.aggregate_array('b1').getInfo())[:]\n",
        "slopeDegree_values = slopeDegree_image.sampleRegions(collection=feature_collection,scale=100)\n",
        "slopeDegree_arr = np.array(slopeDegree_values.aggregate_array('b1').getInfo())[:]\n",
        "\n",
        "\n",
        "# extract pixel values of LC using feature collection\n",
        "LC_values = LC.sampleRegions(collection=feature_collection,scale=100)\n",
        "landcover_arr = np.array(LC_values.aggregate_array('landcover').getInfo())[:]\n",
        "LC_idnum_arr = np.array(LC_values.aggregate_array('id_num').getInfo())[:]\n",
        "\n",
        "# extract pixel values of Polaris soil using feature collection\n",
        "polaris_values = polaris.sampleRegions(collection=feature_collection,scale=100)\n",
        "ml_sand_0_5 = np.array(polaris_values.aggregate_array('sand_0_5').getInfo())[:]\n",
        "ml_sand_0_100 = np.array(polaris_values.aggregate_array('sand_0_100').getInfo())[:]\n",
        "ml_clay_0_5 = np.array(polaris_values.aggregate_array('clay_0_5').getInfo())[:]\n",
        "ml_clay_0_100 = np.array(polaris_values.aggregate_array('clay_0_100').getInfo())[:]\n",
        "ml_bd_0_5 = np.array(polaris_values.aggregate_array('bd_0_5').getInfo())[:]\n",
        "ml_bd_0_100 = np.array(polaris_values.aggregate_array('bd_0_100').getInfo())[:]\n",
        "ml_ksat_0_5 = np.array(polaris_values.aggregate_array('ksat_0_5').getInfo())[:]\n",
        "polaris_idnum_arr = np.array(polaris_values.aggregate_array('id_num').getInfo())[:]\n",
        "\n",
        "\n",
        "# extract pixel values of Polaris soil using feature collection\n",
        "polaris_ksat_values = polaris_ksat.sampleRegions(collection=feature_collection,scale=100)\n",
        "#ksat\n",
        "ksat_0_5_arr = np.array(polaris_ksat_values.aggregate_array('ksat_0_5').getInfo())[:]\n",
        "ksat_5_15_arr = np.array(polaris_ksat_values.aggregate_array('ksat_5_15').getInfo())[:]\n",
        "ksat_15_30_arr = np.array(polaris_ksat_values.aggregate_array('ksat_15_30').getInfo())[:]\n",
        "ksat_30_60_arr = np.array(polaris_ksat_values.aggregate_array('ksat_30_60').getInfo())[:]\n",
        "ksat_60_100_arr = np.array(polaris_ksat_values.aggregate_array('ksat_60_100').getInfo())[:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4MW_IleJYme"
      },
      "outputs": [],
      "source": [
        "#@title processing constant values\n",
        "elevation_arr[elevation_arr<-1000] = np.nan\n",
        "\n",
        "sinAsp_arr = np.sin(aspect_arr/180*np.pi)\n",
        "sinAsp_arr[aspect_arr==-1] = 0\n",
        "cosAsp_arr = np.cos(aspect_arr/180*np.pi)\n",
        "cosAsp_arr[aspect_arr == -1] = 0\n",
        "\n",
        "ml_ksat_0_5 = np.power(10, ml_ksat_0_5)  # convert to cm/hr\n",
        "ksat_0_5_arr = np.power(10, ksat_0_5_arr)\n",
        "ksat_5_15_arr = np.power(10, ksat_5_15_arr)\n",
        "ksat_15_30_arr = np.power(10, ksat_15_30_arr)\n",
        "ksat_30_60_arr = np.power(10, ksat_30_60_arr)\n",
        "ksat_60_100_arr = np.power(10, ksat_60_100_arr)\n",
        "\n",
        "d0_5 = 0.05\n",
        "d5_15 = 0.10\n",
        "d15_30 = 0.15\n",
        "d30_60 = 0.30\n",
        "d60_100 = 0.40\n",
        "d_total = d0_5 + d5_15 + d15_30 + d30_60 + d60_100\n",
        "ml_ksat_0_100 = d_total/(d0_5/ksat_0_5_arr +\n",
        "                                d5_15/ksat_5_15_arr +\n",
        "                                d15_30/ksat_15_30_arr +\n",
        "                                d30_60/ksat_30_60_arr +\n",
        "                                d60_100/ksat_60_100_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uju6qHVHLQvv"
      },
      "outputs": [],
      "source": [
        "print(\"max ksat at surface: \", ml_ksat_0_5.max())\n",
        "print(\"min ksat at surface: \", ml_ksat_0_5.min())\n",
        "print(\"max ksat at rootzone: \", ml_ksat_0_100.max())\n",
        "print(\"min ksat at rootzone: \", ml_ksat_0_100.min())\n",
        "print(\"max slope\", slopeDegree_arr.max())\n",
        "print(\"min slope\", slopeDegree_arr.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUivaCSVL16z"
      },
      "outputs": [],
      "source": [
        "#@title reorganize dataframe\n",
        "\n",
        "df_constant = pd.DataFrame({\n",
        "    'ID': dem_idnum_arr,\n",
        "    'elevation': elevation_arr,\n",
        "    'sinAspect': sinAsp_arr,\n",
        "    'cosAspect': cosAsp_arr,\n",
        "    'slope': slopeDegree_arr,\n",
        "    'landcover': landcover_arr,\n",
        "    'sand_0_5[0-100]': ml_sand_0_5,\n",
        "    'sand_0_100[0-100]': ml_sand_0_100,\n",
        "    'clay_0_5[0-100]': ml_clay_0_5,\n",
        "    'clay_0_100[0-100]': ml_clay_0_100,\n",
        "    'ksat_0_5[cm/hr]': ml_ksat_0_5,\n",
        "    'ksat_0_100[cm/hr]': ml_ksat_0_100,\n",
        "    'bd_0_5[g/cm3]': ml_bd_0_5,\n",
        "    'bd_0_100[g/cm3]': ml_bd_0_100})\n",
        "\n",
        "merged_df = df_constant.merge(df_random_points, on='ID')\n",
        "\n",
        "\n",
        "refer_id_arr = merged_df['ID'].to_numpy()\n",
        "refer_lon_arr = merged_df['longitude'].to_numpy()\n",
        "refer_lat_arr = merged_df['latitude'].to_numpy()\n",
        "\n",
        "refer_id_idx_dict = {}\n",
        "for i in range(len(refer_id_arr)):\n",
        "    refer_id_idx_dict[refer_id_arr[i]] = i\n",
        "\n",
        "\n",
        "len(refer_id_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgNztblENnPB"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irpDaBGt7m-W"
      },
      "source": [
        "## Define study area and period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRJHKFkR7mgg"
      },
      "outputs": [],
      "source": [
        "# prompt: get the boundary of feature_collection\n",
        "roi = feature_collection.geometry().bounds()\n",
        "\n",
        "start_date = '2016-01-01'\n",
        "end_date = '2025-01-01'\n",
        "\n",
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "\n",
        "num_hours = int((end_datetime - start_datetime).total_seconds()/3600)\n",
        "print(num_hours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QL9ACxF7h2t"
      },
      "source": [
        "## Get ee imagecollections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5HQkf4L7ijj"
      },
      "outputs": [],
      "source": [
        "# @title Era5-land\n",
        "start_date_era5 = (datetime.strptime(start_date, '%Y-%m-%d')- timedelta(hours=120+24)).strftime('%Y-%m-%d')\n",
        "\n",
        "era5hour_short = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\")\\\n",
        "                    .filterBounds(roi).filterDate(start_date_era5, end_date)\\\n",
        "                    .map(lambda img: img.addBands(\n",
        "                        img.select('u_component_of_wind_10m').hypot(img.select('v_component_of_wind_10m')).rename('wind_10m')\n",
        "                        )).select([\n",
        "                            'temperature_2m',    # 2m air temperature, K\n",
        "                            'dewpoint_temperature_2m',  # 2m dew point temperature, K\n",
        "                            'wind_10m',  # wind speed, m/s\n",
        "                            'surface_pressure',  # atmospheric surface pressure, Pa\n",
        "                            ])\n",
        "\n",
        "\n",
        "era5hour_mid = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").filterBounds(roi).filterDate(start_date_era5, end_date).select([\n",
        "    'surface_solar_radiation_downwards_hourly',  # solar radiation, J\n",
        "    'surface_thermal_radiation_downwards_hourly',  # thermal radiation, J\n",
        "    ])\n",
        "\n",
        "\n",
        "era5hour_long = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").filterBounds(roi).filterDate(start_date_era5, end_date).select([\n",
        "    'total_evaporation_hourly',  # total evaporation, m\n",
        "    'total_precipitation_hourly',   # precipitation, m\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59lw-CMw2E8f"
      },
      "outputs": [],
      "source": [
        "# @title HLSL30\n",
        "\n",
        "###note: no scale factor for HLSL30 product.\n",
        "\n",
        "def bitwiseExtract(img, fromBit, toBit):\n",
        "  maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
        "  mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
        "  return img.rightShift(fromBit).bitwiseAnd(mask)\n",
        "\n",
        "# remove low quality data\n",
        "def maskHLSL30(image):\n",
        "  qcDay = image.select('Fmask')\n",
        "  cloud = bitwiseExtract(qcDay, 1, 1).eq(0)\n",
        "  cloudshadow = bitwiseExtract(qcDay, 3, 3).eq(0)\n",
        "  snowice = bitwiseExtract(qcDay, 4, 4).eq(0)\n",
        "  water = bitwiseExtract(qcDay, 5, 5).eq(0)\n",
        "  aerosol = bitwiseExtract(qcDay, 6, 7).lte(2)\n",
        "  mask = cloud.And(cloudshadow).And(snowice).And(water).And(aerosol)\n",
        "\n",
        "  return    image.updateMask(mask).copyProperties(image, ['system:time_start'])\n",
        "\n",
        "\n",
        "HLSL30 = ee.ImageCollection(\"NASA/HLS/HLSL30/v002\").filterBounds(roi)\\\n",
        "    .filterDate(start_date, end_date).map(maskHLSL30) \\\n",
        "\t\t.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B9', 'B10', 'B11'])\n",
        "\n",
        "\n",
        "\n",
        "# HLSL30_timestamps = HLSL30.aggregate_array('system:time_start')\n",
        "# HLSL30_datetime_list = HLSL30_timestamps.map(\n",
        "#     lambda t: ee.Date(t).format('YYYY-MM-dd HH')\n",
        "# )\n",
        "# HLSL30_datetime_list_py = HLSL30_datetime_list.getInfo()\n",
        "# print(HLSL30_datetime_list_py[:10])  # preview first 10\n",
        "# print(\"Total unique timestamps:\", len(set(HLSL30_datetime_list_py)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN5LAdsW73HW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sentinel-2\n",
        "# Sentinel-2 images\n",
        "\n",
        "# old version\n",
        "# def maskSentinel2(img):\n",
        "#   cloudOpaqueBitMask = (1 << 10);\n",
        "#   cloudCirrusMask = (1 << 11);\n",
        "#   # Get the pixel QA band.\n",
        "#   qa = img.select('QA60')\n",
        "#   # Both flags should be set to zero, indicating clear conditions.\n",
        "#   mask = qa.bitwiseAnd(cloudOpaqueBitMask).eq(0) \\\n",
        "#                 .And(qa.bitwiseAnd(cloudCirrusMask).eq(0))\n",
        "#   return img.updateMask(mask).copyProperties(img, ['system:time_start'])                  #.multiply(0.0001).toFloat().copyProperties(img, ['mydate'])  # after applying updateMask(). all properties will be lost\n",
        "\n",
        "\n",
        "def maskSentinel2(img):\n",
        "  # Get the pixel QA band.\n",
        "  scl = img.select('SCL')\n",
        "  mask = scl.neq(8).And(scl.neq(9)).And(scl.neq(10)).And(scl.neq(11))\n",
        "  return img.updateMask(mask).copyProperties(img, ['system:time_start'])\n",
        "\n",
        "# def add_date(img):\n",
        "#   date_start = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd-HH')\n",
        "#   return img.set('mydate', date_start)\n",
        "\n",
        "\n",
        "Sentinel2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\\\n",
        "            .filterDate(start_date, end_date).filterBounds(roi)\\\n",
        "            .map(maskSentinel2)\\\n",
        "            .select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'])\n",
        "\n",
        "\n",
        "# Sentinel2 = Sentinel2.map(add_date)\n",
        "\n",
        "# Sentinel2_timestamps = Sentinel2.aggregate_array('system:time_start')\n",
        "# Sentinel2_datetime_list = Sentinel2_timestamps.map(\n",
        "#     lambda t: ee.Date(t).format('YYYY-MM-dd HH')\n",
        "# )\n",
        "# Sentinel2_datetime_list_py = Sentinel2_datetime_list.getInfo()\n",
        "# print(Sentinel2_datetime_list_py[:10])  # preview first 10\n",
        "# print(\"Total unique timestamps:\", len(set(Sentinel2_datetime_list_py)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L--AZmxi8Kx-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sentinel-1\n",
        "\n",
        "def preprocess_vv(image):\n",
        "    vv_masked = image.updateMask(image.gt(-20).And(image.lt(-5)))\n",
        "    vv_filtered = vv_masked.convolve(ee.Kernel.gaussian(3))\n",
        "    return vv_filtered #.rename('VV').copyProperties(image, ['system:time_start'])\n",
        "\n",
        "# Define preprocessing for VH\n",
        "def preprocess_vh(image):\n",
        "    vh_masked = image.updateMask(image.gt(-30).And(image.lt(-10)))\n",
        "    vh_filtered = vh_masked.convolve(ee.Kernel.gaussian(3))\n",
        "    return vh_filtered #.rename('VH').copyProperties(image, ['system:time_start'])\n",
        "\n",
        "\n",
        "def merge_bands(image):\n",
        "    vv = image.select('VV')\n",
        "    vh = image.select('VH')\n",
        "    angle = image.select('angle')\n",
        "\n",
        "    vv_prep = preprocess_vv(vv)      # Apply mask + smoothing to VV\n",
        "    vh_prep = preprocess_vh(vh)      # Apply mask + smoothing to VH\n",
        "\n",
        "    merged = vv_prep.addBands(vh_prep).addBands(angle.rename('angle'))\n",
        "\n",
        "    return merged.copyProperties(image, ['system:time_start'])\n",
        "\n",
        "def to_float(image):\n",
        "    all_bands = image.bandNames()\n",
        "    return image.select(all_bands).float().copyProperties(image, ['system:time_start'])\n",
        "\n",
        "Sentinel1 = (\n",
        "    ee.ImageCollection('COPERNICUS/S1_GRD')\n",
        "    .filterDate(start_date, end_date)\n",
        "    .filterBounds(roi)\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
        "    # .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
        "    .sort('SLC_Processing_start')\n",
        "    .map(merge_bands).map(to_float)\n",
        ")\n",
        "\n",
        "\n",
        "# Sentinel1_timestamps = Sentinel1.aggregate_array('system:time_start')\n",
        "# Sentinel1_datetime_list = Sentinel1_timestamps.map(\n",
        "#     lambda t: ee.Date(t).format('YYYY-MM-dd HH')\n",
        "# )\n",
        "# Sentinel1_datetime_list_py = Sentinel1_datetime_list.getInfo()\n",
        "# print(Sentinel1_datetime_list_py[:10])  # preview first 10\n",
        "# print(\"Total unique timestamps:\", len(set(Sentinel1_datetime_list_py)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Rki-WtaXVB"
      },
      "source": [
        "## Prepare Xy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwJUQCxJsU3"
      },
      "source": [
        "## Extract data from image collection\n",
        "\n",
        "The following code is to extract pixel values from imagecollections. Due to the request limiation of google earth engine (e.g., each time no more than 5000 observations), the feature collection is divdided into samll groups, and the study period is divided into small periods, and the parallel processing strategy is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-v4ongxacP5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define weights for ERA5 data\n",
        "def make_weights(n):\n",
        "    weights = 1.0 / np.arange(n, 0, -1)      # reversed sequence [n ... 1]\n",
        "    weights = weights / weights.sum()        # normalize\n",
        "    return weights, ee.List(weights.tolist())\n",
        "\n",
        "# Short-term (24h)\n",
        "short_term_weights, ee_short_term_weights = make_weights(24)\n",
        "# print(\"Short-term weights:\", short_term_weights)\n",
        "\n",
        "# Mid-term (72h)\n",
        "mid_term_weights, ee_mid_term_weights = make_weights(72)\n",
        "# print(\"Mid-term weights:\", mid_term_weights)\n",
        "\n",
        "# Long-term (120h)\n",
        "long_term_weights, ee_long_term_weights = make_weights(120)\n",
        "# print(\"Long-term weights:\", long_term_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA-_28K0agjC"
      },
      "outputs": [],
      "source": [
        "#@title Create year, doy, hour list: doy is an input for ML models, year, doy and hour provide time info for results\n",
        "start_datetime = datetime(2016, 1, 1, 0, 0, 0)\n",
        "datetime_list = [start_datetime + timedelta(hours=i) for i in range(num_hours)]\n",
        "doy_list = np.array([dt.timetuple().tm_yday for dt in datetime_list])\n",
        "year_list = np.array([dt.year for dt in datetime_list])\n",
        "hour_list = np.array([dt.hour for dt in datetime_list])\n",
        "print(len(doy_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjy78933OUPB"
      },
      "outputs": [],
      "source": [
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "\n",
        "num_hours = int((end_datetime - start_datetime).total_seconds()/3600)\n",
        "print(num_hours)\n",
        "\n",
        "era5hour_arr = np.zeros((8, num_hours, len(refer_id_arr)))\n",
        "sentinel1_arr = np.zeros((3, num_hours, len(refer_id_arr)))\n",
        "HLSL30_arr = np.zeros((9, num_hours, len(refer_id_arr)))\n",
        "Sentinel2_arr = np.zeros((10, num_hours, len(refer_id_arr)))\n",
        "\n",
        "era5hour_arr[:] = np.nan\n",
        "sentinel1_arr[:] = np.nan\n",
        "HLSL30_arr[:] = np.nan\n",
        "Sentinel2_arr[:] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIhwYG2pau-k"
      },
      "outputs": [],
      "source": [
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "current_datetime = start_datetime\n",
        "tfirsts = []\n",
        "tlasts = []\n",
        "deltaa = 15 # hours\n",
        "while current_datetime < end_datetime:\n",
        "  tfirsts.append(current_datetime)\n",
        "  tlasts.append( current_datetime + timedelta(hours=deltaa))\n",
        "  current_datetime += timedelta(hours=deltaa)\n",
        "\n",
        "print(len(tfirsts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOT5h7aPfW8I"
      },
      "outputs": [],
      "source": [
        "len(tfirsts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3lSw4b_mpeKT"
      },
      "outputs": [],
      "source": [
        "# @title parallel processing  for ERA5-Land\n",
        "import concurrent.futures\n",
        "import traceback\n",
        "\n",
        "def process_day(idx, feature_collection_crop):\n",
        "  print(f'is running the {idx}th group')\n",
        "  tfirst = tfirsts[idx]\n",
        "  tlast = tlasts[idx]\n",
        "\n",
        "  # ERA5 short\n",
        "  era5hour_short_sub = era5hour_short.filterDate(tfirst, tlast)\n",
        "\n",
        "  era5hour_sampled_points = era5hour_short_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  time.sleep(2) # Adjust the delay as needed\n",
        "  points_list = era5hour_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  # print(len(df))\n",
        "  df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  df['time_idx'] = [int((datetime.strptime(systm[0:8] + systm[9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "  grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "  indices = grouped_df.index.tolist()\n",
        "  for idx in indices:\n",
        "    era5hour_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'temperature_2m']   # 2m air temperature, K\n",
        "    era5hour_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'dewpoint_temperature_2m']   # 2m dew point temperature, K\n",
        "    era5hour_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'wind_10m']   # wind speed, m/s\n",
        "    era5hour_arr[3, idx[0], idx[1]] = grouped_df.loc[idx, 'surface_pressure']  # atmospheric surface pressure, Pa\n",
        "\n",
        "\n",
        "  # ERA5 middle\n",
        "  era5hour_mid_sub = era5hour_mid.filterDate(tfirst, tlast)\n",
        "  era5hour_sampled_points = era5hour_mid_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  time.sleep(2) # Adjust the delay as needed\n",
        "  points_list = era5hour_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  df['time_idx'] = [int((datetime.strptime(systm[0:8] + systm[9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "  grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "  indices = grouped_df.index.tolist()\n",
        "  for idx in indices:\n",
        "    era5hour_arr[4, idx[0], idx[1]] = grouped_df.loc[idx, 'surface_solar_radiation_downwards_hourly']   # solar radiation, J\n",
        "    era5hour_arr[5, idx[0], idx[1]] = grouped_df.loc[idx, 'surface_thermal_radiation_downwards_hourly']   # thermal radiation, J\n",
        "\n",
        "\n",
        "  # ERA5 long\n",
        "  era5hour_long_sub = era5hour_long.filterDate(tfirst, tlast)\n",
        "  era5hour_sampled_points = era5hour_long_sub.map(lambda image: image.sampleRegions(\n",
        "    collection=feature_collection_crop,\n",
        "    scale=100  # Adjust scale as needed\n",
        "  )).flatten()\n",
        "  time.sleep(2) # Adjust the delay as needed\n",
        "  points_list = era5hour_sampled_points.getInfo()['features']\n",
        "  system_times = [point['id'] for point in points_list]\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "  df['time_idx'] = [int((datetime.strptime(systm[0:8] + systm[9:11], '%Y%m%d%H') - start_datetime).total_seconds()/3600) for systm in system_times]\n",
        "  grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "  indices = grouped_df.index.tolist()\n",
        "  for idx in indices:\n",
        "    era5hour_arr[6, idx[0], idx[1]] = grouped_df.loc[idx, 'total_evaporation_hourly']   # total evaporation, m\n",
        "    era5hour_arr[7, idx[0], idx[1]] = grouped_df.loc[idx, 'total_precipitation_hourly']   # precipitation, m\n",
        "\n",
        "\n",
        "  return idx\n",
        "\n",
        "\n",
        "try:\n",
        "  if os.path.exists('era5hour_arr_irriClimate.npy'):\n",
        "    era5hour_arr = np.load('era5hour_arr_irriClimate.npy')\n",
        "  else:\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "          results = list(executor.map(lambda idx: process_day(idx, feature_collection), range(len(tfirsts)))) # len(tfirsts)\n",
        "    print('done')\n",
        "\n",
        "except Exception as main_error:\n",
        "  print(f\"ð¨ Unexpected main error: {main_error}\")\n",
        "  traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "  np.save('era5hour_arr_irriClimate.npy', era5hour_arr)\n",
        "\n",
        "  time.sleep(10)\n",
        "  from google.colab import files\n",
        "  files.download('era5hour_arr_irriClimate.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxxeoZCKBEeX"
      },
      "outputs": [],
      "source": [
        "axis1 = np.where(np.isnan(era5hour_arr))[0]\n",
        "axis2 = np.where(np.isnan(era5hour_arr))[1]\n",
        "axis3 = np.where(np.isnan(era5hour_arr))[2]\n",
        "\n",
        "axis1_unique = np.unique(axis1)\n",
        "axis2_unique = np.unique(axis2)\n",
        "axis3_unique = np.unique(axis3)\n",
        "\n",
        "axis3_unique\n",
        "# array([255, 353, 354, 355, 356, 374, 375, 384])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xweANpz6b3X5"
      },
      "outputs": [],
      "source": [
        "print('nan numbers: ',np.count_nonzero(np.isnan(era5hour_arr)))\n",
        "print('shape of ear5hour_arr: ',era5hour_arr.shape)\n",
        "nan_indices = np.argwhere(np.isnan(era5hour_arr))\n",
        "print('third dimension: ', set(nan_indices[:, 2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRSNiJndcWKC"
      },
      "outputs": [],
      "source": [
        "start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_datetime = datetime.strptime(end_date, '%Y-%m-%d')  # end_date\n",
        "current_datetime = start_datetime\n",
        "tfirsts = []\n",
        "tlasts = []\n",
        "deltaa = 15 # hours\n",
        "while current_datetime < end_datetime:\n",
        "  tfirsts.append(current_datetime)\n",
        "  tlasts.append( current_datetime + timedelta(hours=deltaa))\n",
        "  current_datetime += timedelta(hours=deltaa)\n",
        "\n",
        "print(len(tfirsts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao3-TZpWb9Pp"
      },
      "outputs": [],
      "source": [
        "# @title parallel processing  for HLSL30\n",
        "import concurrent.futures\n",
        "\n",
        "\n",
        "def process_day(idx, feature_collection_crop):\n",
        "  print(f'is running the {idx}th group')\n",
        "  tfirst = tfirsts[idx]\n",
        "  tlast = tlasts[idx]\n",
        "\n",
        "  HLSL30_sub = HLSL30.filterDate(tfirst, tlast)\n",
        "  Sentinel2_sub = Sentinel2.filterDate(tfirst, tlast)\n",
        "  sentinel1_sub = Sentinel1.filterDate(tfirst, tlast)\n",
        "\n",
        "  def sample_with_time(image):\n",
        "        samples = image.sampleRegions(\n",
        "            collection=feature_collection_crop,\n",
        "            scale=100\n",
        "        )\n",
        "        return samples.map(lambda f: f.set('system:time_start', image.get('system:time_start')))\n",
        "\n",
        "  # sentinel2\n",
        "  Sentinel2_sampled_points = Sentinel2_sub.map(sample_with_time).flatten()\n",
        "  points_list = Sentinel2_sampled_points.getInfo()['features']\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  # print('sentinel2: ',len(df))\n",
        "  if 'id_num' in df.columns:\n",
        "    df['datetime'] = pd.to_datetime(df['system:time_start'], unit='ms')\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = ((df['datetime'] - start_datetime).dt.total_seconds() / 3600).astype(int)\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      Sentinel2_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'B2']\n",
        "      Sentinel2_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'B3']\n",
        "      Sentinel2_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'B4']\n",
        "      Sentinel2_arr[3, idx[0], idx[1]] = grouped_df.loc[idx, 'B5']\n",
        "      Sentinel2_arr[4, idx[0], idx[1]] = grouped_df.loc[idx, 'B6']\n",
        "      Sentinel2_arr[5, idx[0], idx[1]] = grouped_df.loc[idx, 'B7']\n",
        "      Sentinel2_arr[6, idx[0], idx[1]] = grouped_df.loc[idx, 'B8']\n",
        "      Sentinel2_arr[7, idx[0], idx[1]] = grouped_df.loc[idx, 'B8A']\n",
        "      Sentinel2_arr[8, idx[0], idx[1]] = grouped_df.loc[idx, 'B11']\n",
        "      Sentinel2_arr[9, idx[0], idx[1]] = grouped_df.loc[idx, 'B12']\n",
        "\n",
        "  # hlsl30\n",
        "  HLSL30_sampled_points = HLSL30_sub.map(sample_with_time).flatten()\n",
        "  points_list = HLSL30_sampled_points.getInfo()['features']\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  # print('hlsl30: ',len(df))\n",
        "  if 'id_num' in df.columns:\n",
        "    df['datetime'] = pd.to_datetime(df['system:time_start'], unit='ms')\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = ((df['datetime'] - start_datetime).dt.total_seconds() / 3600).astype(int)\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      HLSL30_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'B2']\n",
        "      HLSL30_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'B3']\n",
        "      HLSL30_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'B4']\n",
        "      HLSL30_arr[3, idx[0], idx[1]] = grouped_df.loc[idx, 'B5']\n",
        "      HLSL30_arr[4, idx[0], idx[1]] = grouped_df.loc[idx, 'B6']\n",
        "      HLSL30_arr[5, idx[0], idx[1]] = grouped_df.loc[idx, 'B7']\n",
        "      HLSL30_arr[6, idx[0], idx[1]] = grouped_df.loc[idx, 'B9']\n",
        "      HLSL30_arr[7, idx[0], idx[1]] = grouped_df.loc[idx, 'B10']\n",
        "      HLSL30_arr[8, idx[0], idx[1]] = grouped_df.loc[idx, 'B11']\n",
        "\n",
        "  # sentinel1\n",
        "  sentinel1_sampled_points = sentinel1_sub.map(sample_with_time).flatten()\n",
        "  points_list = sentinel1_sampled_points.getInfo()['features']\n",
        "  data = [point['properties'] for point in points_list]\n",
        "  df = pd.DataFrame(data)\n",
        "  # print('sentinel1: ',len(df))\n",
        "  if 'id_num' in df.columns:\n",
        "    df['datetime'] = pd.to_datetime(df['system:time_start'], unit='ms')\n",
        "    df['id_idx'] = [refer_id_idx_dict[id] for id in df['id_num'].tolist()]\n",
        "    df['time_idx'] = ((df['datetime'] - start_datetime).dt.total_seconds() / 3600).astype(int)\n",
        "    grouped_df = df.groupby(['time_idx', 'id_idx']).mean()\n",
        "    indices = grouped_df.index.tolist()\n",
        "    for idx in indices:\n",
        "      sentinel1_arr[0, idx[0], idx[1]] = grouped_df.loc[idx, 'VV']\n",
        "      sentinel1_arr[1, idx[0], idx[1]] = grouped_df.loc[idx, 'VH']\n",
        "      sentinel1_arr[2, idx[0], idx[1]] = grouped_df.loc[idx, 'angle']\n",
        "\n",
        "\n",
        "  return idx\n",
        "\n",
        "\n",
        "try:\n",
        "  if os.path.exists('Sentinel2_arr_irriClimate.npy'):\n",
        "    Sentinel2_arr = np.load('Sentinel2_arr_irriClimate.npy')\n",
        "    HLSL30_arr = np.load('HLSL30_arr_irriClimate.npy')\n",
        "    sentinel1_arr = np.load('sentinel1_arr_irriClimate.npy')\n",
        "  else:\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "          results = list(executor.map(lambda idx: process_day(idx, feature_collection), range(5050, len(tfirsts)))) # len(tfirsts)\n",
        "    print('done')\n",
        "\n",
        "except Exception as main_error:\n",
        "  print(f\"ð¨ Unexpected main error: {main_error}\")\n",
        "  traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "  np.save('Sentinel2_arr_irriClimate.npy', Sentinel2_arr)\n",
        "  np.save('HLSL30_arr_irriClimate.npy', HLSL30_arr)\n",
        "  np.save('sentinel1_arr_irriClimate.npy', sentinel1_arr)\n",
        "\n",
        "  time.sleep(10)\n",
        "  from google.colab import files\n",
        "  files.download('Sentinel2_arr_irriClimate.npy')\n",
        "  files.download('HLSL30_arr_irriClimate.npy')\n",
        "  files.download('sentinel1_arr_irriClimate.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqntDKm3Y_IP"
      },
      "outputs": [],
      "source": [
        "print('shape of HLSL30_arr: ',HLSL30_arr.shape)\n",
        "print('shape of sentinel1_arr: ',sentinel1_arr.shape)\n",
        "print('shape of Sentinel2_arr: ',Sentinel2_arr.shape)\n",
        "print('NON-nan numbers: ',np.count_nonzero(~np.isnan(HLSL30_arr[:, :, :]))) #77000\n",
        "print('NON-nan numbers: ',np.count_nonzero(~np.isnan(sentinel1_arr[:, :, :])))\n",
        "print('NON-nan numbers: ',np.count_nonzero(~np.isnan(Sentinel2_arr[:, :, :])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IZ5oSDcn1Hi"
      },
      "source": [
        "## ML model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hIwqvVq6u5i"
      },
      "outputs": [],
      "source": [
        "era5hour_arr = np.load('era5hour_arr_irriClimate.npy')\n",
        "print('shape of era5hour_arr: ',era5hour_arr.shape)\n",
        "print('non-nan numbers: ', np.count_nonzero(~np.isnan(era5hour_arr)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOc0wZ7UaJ_3"
      },
      "outputs": [],
      "source": [
        "# @title ERA5 processing\n",
        "\n",
        "#========================short term====================\n",
        "\n",
        "ear5_d2m_arr_new = era5hour_arr[1, :, :].copy()\n",
        "print(ear5_d2m_arr_new.shape)\n",
        "ear5_t2m_arr_new = era5hour_arr[0, :, :].copy()\n",
        "ear5_wind_arr_new = era5hour_arr[2, :, :].copy()\n",
        "ear5_sp_arr_new = era5hour_arr[3, :, :].copy()\n",
        "for row_idx in range(24, ear5_d2m_arr_new.shape[0]):\n",
        "    ear5_d2m_arr_new[row_idx, :] = np.sum(era5hour_arr[1, row_idx - 23: row_idx + 1, :] * short_term_weights[:, np.newaxis], axis=0)\n",
        "    ear5_t2m_arr_new[row_idx, :] = np.sum(era5hour_arr[0, row_idx - 23: row_idx + 1, :] * short_term_weights[:, np.newaxis], axis=0)\n",
        "    ear5_wind_arr_new[row_idx, :] = np.sum(era5hour_arr[2, row_idx - 23: row_idx + 1, :] * short_term_weights[:, np.newaxis], axis=0)\n",
        "    ear5_sp_arr_new[row_idx, :] = np.sum(era5hour_arr[3, row_idx - 23: row_idx + 1, :] * short_term_weights[:, np.newaxis], axis=0)\n",
        "\n",
        "\n",
        "# ========================middle term====================\n",
        "\n",
        "ear5_ssr_arr_new = era5hour_arr[4, :, :].copy()\n",
        "ear5_str_arr_new = era5hour_arr[5, :, :].copy()\n",
        "for row_idx in range(72, ear5_str_arr_new.shape[0]):\n",
        "    ear5_ssr_arr_new[row_idx, :] = np.sum(era5hour_arr[4, row_idx - 71: row_idx + 1, :] * mid_term_weights[:, np.newaxis], axis=0)\n",
        "    ear5_str_arr_new[row_idx, :] = np.sum(era5hour_arr[5, row_idx - 71: row_idx + 1, :] * mid_term_weights[:, np.newaxis], axis=0)\n",
        "\n",
        "\n",
        "# ========================long term====================\n",
        "ear5_e_arr_new = era5hour_arr[6, :, :].copy()\n",
        "ear5_tp_arr_new = era5hour_arr[7, :, :].copy()\n",
        "for row_idx in range(120, ear5_e_arr_new.shape[0]):\n",
        "    ear5_e_arr_new[row_idx, :] = np.sum(era5hour_arr[6, row_idx - 119: row_idx + 1, :] * long_term_weights[:, np.newaxis] , axis=0)\n",
        "    ear5_tp_arr_new[row_idx, :] = np.sum(era5hour_arr[7, row_idx - 119: row_idx + 1, :] * long_term_weights[:, np.newaxis], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbNkrhDMaSyB"
      },
      "outputs": [],
      "source": [
        "print('shape of ear5_d2m_arr_new: ',ear5_d2m_arr_new.shape)\n",
        "print('shape of ear5_t2m_arr_new: ',ear5_t2m_arr_new.shape)\n",
        "print('shape of ear5_wind_arr_new: ',ear5_wind_arr_new.shape)\n",
        "print('shape of ear5_sp_arr_new: ',ear5_sp_arr_new.shape)\n",
        "print('shape of ear5_ssr_arr_new: ',ear5_ssr_arr_new.shape)\n",
        "print('shape of ear5_str_arr_new: ',ear5_str_arr_new.shape)\n",
        "print('shape of ear5_e_arr_new: ',ear5_e_arr_new.shape)\n",
        "print('shape of ear5_tp_arr_new: ',ear5_tp_arr_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGvIi_DAaWh4"
      },
      "outputs": [],
      "source": [
        "#@title Remove inf values of Sentinel 1 - there are some but I do not know why\n",
        "sentinel1_arr[sentinel1_arr>1000] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKWx92Kpb64r"
      },
      "outputs": [],
      "source": [
        "referIDArr = df_random_points['ID'].to_numpy()\n",
        "refer_lat_arr = df_random_points['latitude'].to_numpy()\n",
        "refer_lon_arr = df_random_points['longitude'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtNMzb5KaejA"
      },
      "outputs": [],
      "source": [
        "# @title prepare Xy for ML and store them in npy file\n",
        "\"\"\"-------------------------------------sentinel1-rootzone--------------------------------------\"\"\"\n",
        "# X - ID, year, doy, hour, latitude, longitude, dem, slope, sinAspect, cosAspect, TWI,\n",
        "# bd, sand, clay, ksat, SMAP, d2m, t2m, ssr,str, e, wind, sp, tp,\n",
        "# VV, VH, angle, CR,  DPSVIm, Pol  - 30\n",
        "num_ids = len(refer_id_arr)\n",
        "X = np.zeros((30, num_hours, num_ids))\n",
        "X[:] = np.nan\n",
        "X[0] = np.tile(referIDArr, (num_hours, 1))\n",
        "X[1] = np.repeat(year_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[2] = np.repeat(doy_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[3] = np.repeat(hour_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[4] = np.tile(refer_lat_arr, (num_hours, 1))\n",
        "X[5] = np.tile(refer_lon_arr, (num_hours, 1))\n",
        "X[6] = np.tile(elevation_arr, (num_hours, 1))\n",
        "X[7] = np.tile(slopeDegree_arr, (num_hours, 1))\n",
        "X[8] = np.tile(sinAsp_arr, (num_hours, 1))\n",
        "X[9] = np.tile(cosAsp_arr, (num_hours, 1))\n",
        "X[10] = np.tile(TWII_arr, (num_hours, 1))\n",
        "X[11] = np.tile(ml_bd_0_100, (num_hours, 1))\n",
        "X[12] = np.tile(ml_sand_0_100, (num_hours, 1))\n",
        "X[13] = np.tile(ml_clay_0_100, (num_hours, 1))\n",
        "X[14] = np.tile(ml_ksat_0_100, (num_hours, 1))\n",
        "X[15] = 0.3 # smap4_root_arr.T\n",
        "X[16] = ear5_d2m_arr_new\n",
        "X[17] = ear5_t2m_arr_new\n",
        "X[18] = ear5_ssr_arr_new\n",
        "X[19] = ear5_str_arr_new\n",
        "X[20] = ear5_e_arr_new\n",
        "X[21] = ear5_wind_arr_new\n",
        "X[22] = ear5_sp_arr_new\n",
        "X[23] = ear5_tp_arr_new\n",
        "X[24] = sentinel1_arr[0, :, :]\n",
        "X[25] = sentinel1_arr[1, :, :]\n",
        "X[26] = sentinel1_arr[2, :, :]\n",
        "X[27] = sentinel1_arr[0, :, :] / sentinel1_arr[1, :, :]\n",
        "X[28] = (sentinel1_arr[0, :, :]**2 + sentinel1_arr[1, :, :]**2)/np.sqrt(2)\n",
        "X[29] = (sentinel1_arr[0, :, :] + sentinel1_arr[1, :, :]) / (sentinel1_arr[0, :, :] - sentinel1_arr[1, :, :])\n",
        "\n",
        "X = X.reshape((30, num_hours*num_ids))\n",
        "\n",
        "X_mask = ~np.any(np.isnan(X), axis=0)\n",
        "\n",
        "X_all = X[:, X_mask].T\n",
        "\n",
        "print(X_all.shape)\n",
        "print(np.count_nonzero(np.isnan(X_all)))\n",
        "\n",
        "np.save('sentinel1_rootzone_X_all_CDL16_24.npy', X_all)\n",
        "\n",
        "\"\"\"-------------------------------------sentinel1-surface--------------------------------------\"\"\"\n",
        "# X - ID, year, doy, hour, latitude, longitude, dem, slope, sinAspect, cosAspect, TWI,\n",
        "# bd, sand, clay, ksat, SMAP, d2m, t2m, ssr,str, e, wind, sp, tp,\n",
        "# VV, VH, angle, CR,  DPSVIm, Pol  - 30\n",
        "X = np.zeros((30, num_hours, num_ids))\n",
        "X[:] = np.nan\n",
        "X[0] = np.tile(referIDArr, (num_hours, 1))\n",
        "X[1] = np.repeat(year_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[2] = np.repeat(doy_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[3] = np.repeat(hour_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[4] = np.tile(refer_lat_arr, (num_hours, 1))\n",
        "X[5] = np.tile(refer_lon_arr, (num_hours, 1))\n",
        "X[6] = np.tile(elevation_arr, (num_hours, 1))\n",
        "X[7] = np.tile(slopeDegree_arr, (num_hours, 1))\n",
        "X[8] = np.tile(sinAsp_arr, (num_hours, 1))\n",
        "X[9] = np.tile(cosAsp_arr, (num_hours, 1))\n",
        "X[10] = np.tile(TWII_arr, (num_hours, 1))\n",
        "X[11] = np.tile(ml_bd_0_100, (num_hours, 1))\n",
        "X[12] = np.tile(ml_sand_0_100, (num_hours, 1))\n",
        "X[13] = np.tile(ml_clay_0_100, (num_hours, 1))\n",
        "X[14] = np.tile(ml_ksat_0_100, (num_hours, 1))\n",
        "X[15] = 0.3 #smap4_surface_arr.T\n",
        "X[16] = ear5_d2m_arr_new\n",
        "X[17] = ear5_t2m_arr_new\n",
        "X[18] = ear5_ssr_arr_new\n",
        "X[19] = ear5_str_arr_new\n",
        "X[20] = ear5_e_arr_new\n",
        "X[21] = ear5_wind_arr_new\n",
        "X[22] = ear5_sp_arr_new\n",
        "X[23] = ear5_tp_arr_new\n",
        "X[24] = sentinel1_arr[0, :, :]\n",
        "X[25] = sentinel1_arr[1, :, :]\n",
        "X[26] = sentinel1_arr[2, :, :]\n",
        "X[27] = sentinel1_arr[0, :, :] / sentinel1_arr[1, :, :]\n",
        "X[28] = (sentinel1_arr[0, :, :] ** 2 + sentinel1_arr[1, :, :] ** 2) / np.sqrt(2)\n",
        "X[29] = (sentinel1_arr[0, :, :] + sentinel1_arr[1, :, :]) / (sentinel1_arr[0, :, :] - sentinel1_arr[1, :, :])\n",
        "\n",
        "X = X.reshape((30, num_hours * num_ids))\n",
        "\n",
        "X_mask = ~np.any(np.isnan(X), axis=0)\n",
        "\n",
        "X_all = X[:, X_mask].T\n",
        "\n",
        "print(X_all.shape)\n",
        "print(np.count_nonzero(np.isnan(X_all)))\n",
        "\n",
        "np.save('sentinel1_surface_X_all_CDL16_24.npy', X_all)\n",
        "\n",
        "\"\"\"-------------------------------------sentinel2-rootzone--------------------------------------\"\"\"\n",
        "# X - ID, year, doy, hour, latitude, longitude, dem, slope, sinAspect, cosAspect, TWI,\n",
        "# bd, sand, clay, ksat, SMAP, d2m, t2m, ssr,str, e, wind, sp, tp, - 24\n",
        "# B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12, NDVI, NDWI, greenness, brightness, wetness  - 39\n",
        "X = np.zeros((39, num_hours, num_ids))\n",
        "X[:] = np.nan\n",
        "X[0] = np.tile(referIDArr, (num_hours, 1))\n",
        "X[1] = np.repeat(year_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[2] = np.repeat(doy_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[3] = np.repeat(hour_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[4] = np.tile(refer_lat_arr, (num_hours, 1))\n",
        "X[5] = np.tile(refer_lon_arr, (num_hours, 1))\n",
        "X[6] = np.tile(elevation_arr, (num_hours, 1))\n",
        "X[7] = np.tile(slopeDegree_arr, (num_hours, 1))\n",
        "X[8] = np.tile(sinAsp_arr, (num_hours, 1))\n",
        "X[9] = np.tile(cosAsp_arr, (num_hours, 1))\n",
        "X[10] = np.tile(TWII_arr, (num_hours, 1))\n",
        "X[11] = np.tile(ml_bd_0_100, (num_hours, 1))\n",
        "X[12] = np.tile(ml_sand_0_100, (num_hours, 1))\n",
        "X[13] = np.tile(ml_clay_0_100, (num_hours, 1))\n",
        "X[14] = np.tile(ml_ksat_0_100, (num_hours, 1))\n",
        "X[15] = 0.3 #smap4_surface_arr.T\n",
        "X[16] = ear5_d2m_arr_new\n",
        "X[17] = ear5_t2m_arr_new\n",
        "X[18] = ear5_ssr_arr_new\n",
        "X[19] = ear5_str_arr_new\n",
        "X[20] = ear5_e_arr_new\n",
        "X[21] = ear5_wind_arr_new\n",
        "X[22] = ear5_sp_arr_new\n",
        "X[23] = ear5_tp_arr_new\n",
        "X[24] = Sentinel2_arr[0, :, :] * 0.0001 # B2\n",
        "X[25] = Sentinel2_arr[1, :, :] * 0.0001 # B3\n",
        "X[26] = Sentinel2_arr[2, :, :] * 0.0001 # B4\n",
        "X[27] = Sentinel2_arr[3, :, :] * 0.0001 # B5\n",
        "X[28] = Sentinel2_arr[4, :, :] * 0.0001 # B6\n",
        "X[29] = Sentinel2_arr[5, :, :] * 0.0001 # B7\n",
        "X[30] = Sentinel2_arr[6, :, :] * 0.0001 # B8\n",
        "X[31] = Sentinel2_arr[7, :, :] * 0.0001 # B8A\n",
        "X[32] = Sentinel2_arr[8, :, :] * 0.0001 # B11\n",
        "X[33] = Sentinel2_arr[9, :, :] * 0.0001 # B12\n",
        "X[34] = (X[30] - X[26])/(X[30] + X[26])  # NDVI\n",
        "X[35] = (X[25] - X[30])/(X[25] + X[30])  # NDWI\n",
        "X[36] = -0.3599 * X[24] - 0.3533 * X[25] - 0.4734 * X[26] + 0.6633 * X[30] - 0.0087 * X[32] - 0.2856 * X[33] # greenness\n",
        "X[37] = 0.3510 * X[24] + 0.3813 * X[25] + 0.3437 * X[26] + 0.7196 * X[30] + 0.2396 * X[32] + 0.1949 * X[33]  # brightness\n",
        "X[38] = 0.2578 * X[24] + 0.2305 * X[25] + 0.0883 * X[26] + 0.1071 * X[30] - 0.7611 * X[32] - 0.5308 * X[33]  # wetness\n",
        "\n",
        "print('sentinel2-rootzone')\n",
        "print('mean NDVI: ', np.nanmean(X[34]))\n",
        "print('mean NDWI: ', np.nanmean(X[35]))\n",
        "print('mean greenness: ', np.nanmean(X[36]))\n",
        "print('mean brightness: ', np.nanmean(X[37]))\n",
        "print('mean wetness: ', np.nanmean(X[38]))\n",
        "\n",
        "X = X.reshape((39, num_hours * num_ids))\n",
        "\n",
        "y = np.zeros((num_hours, num_ids))\n",
        "\n",
        "X_mask = ~np.any(np.isnan(X), axis=0)\n",
        "y_mask = ~np.isnan(y[0])\n",
        "\n",
        "X_all = X[:, X_mask].T\n",
        "\n",
        "print(X_all.shape)\n",
        "print(np.count_nonzero(np.isnan(X_all)))\n",
        "\n",
        "np.save('sentinel2_rootzone_X_all_CDL16_24.npy', X_all)\n",
        "\n",
        "\"\"\"-------------------------------------sentinel2-surface--------------------------------------\"\"\"\n",
        "# X - ID, year, doy, hour, latitude, longitude, dem, slope, sinAspect, cosAspect, TWI,\n",
        "# bd, sand, clay, ksat, SMAP, d2m, t2m, ssr,str, e, wind, sp, tp,- 24\n",
        "#     # B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12, NDVI, NDWI, greenness, brightness, wetness  - 39\n",
        "X = np.zeros((39, num_hours, num_ids))\n",
        "X[:] = np.nan\n",
        "X[0] = np.tile(referIDArr, (num_hours, 1))\n",
        "X[1] = np.repeat(year_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[2] = np.repeat(doy_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[3] = np.repeat(hour_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[4] = np.tile(refer_lat_arr, (num_hours, 1))\n",
        "X[5] = np.tile(refer_lon_arr, (num_hours, 1))\n",
        "X[6] = np.tile(elevation_arr, (num_hours, 1))\n",
        "X[7] = np.tile(slopeDegree_arr, (num_hours, 1))\n",
        "X[8] = np.tile(sinAsp_arr, (num_hours, 1))\n",
        "X[9] = np.tile(cosAsp_arr, (num_hours, 1))\n",
        "X[10] = np.tile(TWII_arr, (num_hours, 1))\n",
        "X[11] = np.tile(ml_bd_0_100, (num_hours, 1))\n",
        "X[12] = np.tile(ml_sand_0_100, (num_hours, 1))\n",
        "X[13] = np.tile(ml_clay_0_100, (num_hours, 1))\n",
        "X[14] = np.tile(ml_ksat_0_100, (num_hours, 1))\n",
        "X[15] = 0.3 #smap4_surface_arr.T\n",
        "X[16] = ear5_d2m_arr_new\n",
        "X[17] = ear5_t2m_arr_new\n",
        "X[18] = ear5_ssr_arr_new\n",
        "X[19] = ear5_str_arr_new\n",
        "X[20] = ear5_e_arr_new\n",
        "X[21] = ear5_wind_arr_new\n",
        "X[22] = ear5_sp_arr_new\n",
        "X[23] = ear5_tp_arr_new\n",
        "X[24] = Sentinel2_arr[0, :, :] * 0.0001 # B2\n",
        "X[25] = Sentinel2_arr[1, :, :] * 0.0001 # B3\n",
        "X[26] = Sentinel2_arr[2, :, :] * 0.0001 # B4\n",
        "X[27] = Sentinel2_arr[3, :, :] * 0.0001 # B5\n",
        "X[28] = Sentinel2_arr[4, :, :] * 0.0001 # B6\n",
        "X[29] = Sentinel2_arr[5, :, :] * 0.0001 # B7\n",
        "X[30] = Sentinel2_arr[6, :, :] * 0.0001 # B8\n",
        "X[31] = Sentinel2_arr[7, :, :] * 0.0001 # B8A\n",
        "X[32] = Sentinel2_arr[8, :, :] * 0.0001 # B11\n",
        "X[33] = Sentinel2_arr[9, :, :] * 0.0001 # B12\n",
        "X[34] = (X[30] - X[26])/(X[30] + X[26])  # NDVI\n",
        "X[35] = (X[25] - X[30])/(X[25] + X[30])  # NDWI\n",
        "X[36] = -0.3599 * X[24] - 0.3533 * X[25] - 0.4734 * X[26] + 0.6633 * X[30] - 0.0087 * X[32] - 0.2856 * X[33] # greenness\n",
        "X[37] = 0.3510 * X[24] + 0.3813 * X[25] + 0.3437 * X[26] + 0.7196 * X[30] + 0.2396 * X[32] + 0.1949 * X[33]  # brightness\n",
        "X[38] = 0.2578 * X[24] + 0.2305 * X[25] + 0.0883 * X[26] + 0.1071 * X[30] - 0.7611 * X[32] - 0.5308 * X[33]  # wetness\n",
        "\n",
        "print('sentinel2-surface')\n",
        "print('mean NDVI: ', np.nanmean(X[34]))\n",
        "print('mean NDWI: ', np.nanmean(X[35]))\n",
        "print('mean greenness: ', np.nanmean(X[36]))\n",
        "print('mean brightness: ', np.nanmean(X[37]))\n",
        "print('mean wetness: ', np.nanmean(X[38]))\n",
        "\n",
        "X = X.reshape((39, num_hours * num_ids))\n",
        "\n",
        "X_mask = ~np.any(np.isnan(X), axis=0)\n",
        "\n",
        "X_all = X[:, X_mask].T\n",
        "\n",
        "print(X_all.shape)\n",
        "print(np.count_nonzero(np.isnan(X_all)))\n",
        "\n",
        "np.save('sentinel2_surface_X_all_CDL16_24.npy', X_all)\n",
        "\n",
        "\"\"\"-------------------------------------HLSL30-rootzone--------------------------------------\"\"\"\n",
        "# X - ID, year, doy, hour, latitude, longitude, dem, slope, sinAspect, cosAspect, TWI,\n",
        "# bd, sand, clay, ksat, SMAP, d2m, t2m, ssr,str, e, wind, sp, tp, - 24\n",
        "# B2, B3, B4, B5, B6, B7, B10, B11, SZA, SAA, VZA, VAA, NDVI, NDWI, greenness, brightness, wetness  - 41\n",
        "X = np.zeros((41, num_hours, num_ids))\n",
        "X[:] = np.nan\n",
        "X[0] = np.tile(referIDArr, (num_hours, 1))\n",
        "X[1] = np.repeat(year_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[2] = np.repeat(doy_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[3] = np.repeat(hour_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[4] = np.tile(refer_lat_arr, (num_hours, 1))\n",
        "X[5] = np.tile(refer_lon_arr, (num_hours, 1))\n",
        "X[6] = np.tile(elevation_arr, (num_hours, 1))\n",
        "X[7] = np.tile(slopeDegree_arr, (num_hours, 1))\n",
        "X[8] = np.tile(sinAsp_arr, (num_hours, 1))\n",
        "X[9] = np.tile(cosAsp_arr, (num_hours, 1))\n",
        "X[10] = np.tile(TWII_arr, (num_hours, 1))\n",
        "X[11] = np.tile(ml_bd_0_100, (num_hours, 1))\n",
        "X[12] = np.tile(ml_sand_0_100, (num_hours, 1))\n",
        "X[13] = np.tile(ml_clay_0_100, (num_hours, 1))\n",
        "X[14] = np.tile(ml_ksat_0_100, (num_hours, 1))\n",
        "X[15] = 0.3 #smap4_surface_arr.T\n",
        "X[16] = ear5_d2m_arr_new\n",
        "X[17] = ear5_t2m_arr_new\n",
        "X[18] = ear5_ssr_arr_new\n",
        "X[19] = ear5_str_arr_new\n",
        "X[20] = ear5_e_arr_new\n",
        "X[21] = ear5_wind_arr_new\n",
        "X[22] = ear5_sp_arr_new\n",
        "X[23] = ear5_tp_arr_new\n",
        "X[24] = HLSL30_arr[0, :, :]  # B2\n",
        "X[25] = HLSL30_arr[1, :, :]   # B3\n",
        "X[26] = HLSL30_arr[2, :, :]   # B4\n",
        "X[27] = HLSL30_arr[3, :, :]   # B5\n",
        "X[28] = HLSL30_arr[4, :, :]  # B6\n",
        "X[29] = HLSL30_arr[5, :, :]   # B7\n",
        "X[30] = HLSL30_arr[7, :, :]   # B10\n",
        "X[31] = HLSL30_arr[8, :, :]   # B11\n",
        "X[32] = 90 #HLSL30_arr[9, :, :]   # SZA\n",
        "X[33] = 90 #HLSL30_arr[10, :, :]  # SAA\n",
        "X[34] = 90 #HLSL30_arr[11, :, :]  # VZA\n",
        "X[35] = 90 #HLSL30_arr[12, :, :]  # VAA\n",
        "X[36] = (X[27] - X[26]) / (X[27] + X[26])  # NDVI\n",
        "X[37] = (X[25] - X[27]) / (X[25] + X[27])  # NDWI\n",
        "X[38] = -0.2941 * X[24] - 0.243 * X[25] - 0.5424 * X[26] +0.7276 * X[27] +0.0713 * X[28] - 0.1608 * X[29]  # greenness\n",
        "X[39] = 0.3510 * X[24] + 0.3813 * X[25] + 0.3437 * X[26] + 0.7196 * X[27] + 0.2396 * X[28] + 0.1949 * X[29]  # brightness\n",
        "X[40] = 0.2578 * X[24] + 0.2305 * X[25] + 0.0883 * X[26] + 0.1071 * X[27] - 0.7611 * X[28] - 0.5308 * X[29]  # wetness\n",
        "\n",
        "print('HLSL30-rootzone')\n",
        "print('mean NDVI: ', np.nanmean(X[36]))\n",
        "print('mean NDWI: ', np.nanmean(X[37]))\n",
        "print('mean greenness: ', np.nanmean(X[38]))\n",
        "print('mean brightness: ', np.nanmean(X[39]))\n",
        "print('mean wetness: ', np.nanmean(X[40]))\n",
        "\n",
        "X = X.reshape((41, num_hours * num_ids))\n",
        "\n",
        "X_mask = ~np.any(np.isnan(X), axis=0)\n",
        "\n",
        "X_all = X[:, X_mask].T\n",
        "\n",
        "print(X_all.shape)\n",
        "print(np.count_nonzero(np.isnan(X_all)))\n",
        "\n",
        "np.save('HLSL30_rootzone_X_all_CDL16_24.npy', X_all)\n",
        "\n",
        "\"\"\"-------------------------------------HLSL30-surface--------------------------------------\"\"\"\n",
        "# X - ID, year, doy, hour, latitude, longitude, dem, slope, sinAspect, cosAspect, TWI,\n",
        "# bd, sand, clay, ksat, SMAP, d2m, t2m, ssr,str, e, wind, sp, tp,- 24\n",
        "#    B2, B3, B4, B5, B6, B7, B10, B11, SZA, SAA, VZA, VAA, NDVI, NDWI, greenness, brightness, wetness  - 41\n",
        "X = np.zeros((41, num_hours, num_ids))\n",
        "X[:] = np.nan\n",
        "X[0] = np.tile(referIDArr, (num_hours, 1))\n",
        "X[1] = np.repeat(year_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[2] = np.repeat(doy_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[3] = np.repeat(hour_list[:, np.newaxis], num_ids, axis=1)\n",
        "X[4] = np.tile(refer_lat_arr, (num_hours, 1))\n",
        "X[5] = np.tile(refer_lon_arr, (num_hours, 1))\n",
        "X[6] = np.tile(elevation_arr, (num_hours, 1))\n",
        "X[7] = np.tile(slopeDegree_arr, (num_hours, 1))\n",
        "X[8] = np.tile(sinAsp_arr, (num_hours, 1))\n",
        "X[9] = np.tile(cosAsp_arr, (num_hours, 1))\n",
        "X[10] = np.tile(TWII_arr, (num_hours, 1))\n",
        "X[11] = np.tile(ml_bd_0_100, (num_hours, 1))\n",
        "X[12] = np.tile(ml_sand_0_100, (num_hours, 1))\n",
        "X[13] = np.tile(ml_clay_0_100, (num_hours, 1))\n",
        "X[14] = np.tile(ml_ksat_0_100, (num_hours, 1))\n",
        "X[15] = 0.3 #smap4_surface_arr.T\n",
        "X[16] = ear5_d2m_arr_new\n",
        "X[17] = ear5_t2m_arr_new\n",
        "X[18] = ear5_ssr_arr_new\n",
        "X[19] = ear5_str_arr_new\n",
        "X[20] = ear5_e_arr_new\n",
        "X[21] = ear5_wind_arr_new\n",
        "X[22] = ear5_sp_arr_new\n",
        "X[23] = ear5_tp_arr_new\n",
        "X[24] = HLSL30_arr[0, :, :]  # B2\n",
        "X[25] = HLSL30_arr[1, :, :]  # B3\n",
        "X[26] = HLSL30_arr[2, :, :]  # B4\n",
        "X[27] = HLSL30_arr[3, :, :]  # B5\n",
        "X[28] = HLSL30_arr[4, :, :]  # B6\n",
        "X[29] = HLSL30_arr[5, :, :]  # B7\n",
        "X[30] = HLSL30_arr[7, :, :] # B10\n",
        "X[31] = HLSL30_arr[8, :, :]  # B11\n",
        "X[32] = 90 #HLSL30_arr[9, :, :]  # SZA\n",
        "X[33] = 90 #HLSL30_arr[10, :, :]  # SAA\n",
        "X[34] = 90 #HLSL30_arr[11, :, :]  # VZA\n",
        "X[35] = 90 #HLSL30_arr[12, :, :]  # VAA\n",
        "X[36] = (X[27] - X[26]) / (X[27] + X[26])  # NDVI\n",
        "X[37] = (X[25] - X[27]) / (X[25] + X[27])  # NDWI\n",
        "X[38] = -0.2941 * X[24] - 0.243 * X[25] - 0.5424 * X[26] +0.7276 * X[27] +0.0713 * X[28] - 0.1608 * X[29]  # greenness\n",
        "X[39] = 0.3510 * X[24] + 0.3813 * X[25] + 0.3437 * X[26] + 0.7196 * X[27] + 0.2396 * X[28] + 0.1949 * X[29]  # brightness\n",
        "X[40] = 0.2578 * X[24] + 0.2305 * X[25] + 0.0883 * X[26] + 0.1071 * X[27] - 0.7611 * X[28] - 0.5308 * X[29]  # wetness\n",
        "\n",
        "print('HLSL30-surface')\n",
        "print('mean NDVI: ', np.nanmean(X[36]))\n",
        "print('mean NDWI: ', np.nanmean(X[37]))\n",
        "print('mean greenness: ', np.nanmean(X[38]))\n",
        "print('mean brightness: ', np.nanmean(X[39]))\n",
        "print('mean wetness: ', np.nanmean(X[40]))\n",
        "\n",
        "X = X.reshape((41, num_hours * num_ids))\n",
        "\n",
        "X_mask = ~np.any(np.isnan(X), axis=0)\n",
        "\n",
        "X_all = X[:, X_mask].T\n",
        "\n",
        "print(X_all.shape)\n",
        "print(np.count_nonzero(np.isnan(X_all)))\n",
        "\n",
        "np.save('HLSL30_surface_X_all_CDL16_24.npy', X_all)\n",
        "\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yszFrLrydMqU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# List all files in the current directory (root folder)\n",
        "files_to_download = [f for f in os.listdir('.') if os.path.isfile(f) and f.endswith(('y_CDL16_24.npy', '_X_valid.npy', '_y_valid.npy'))]\n",
        "\n",
        "for file_name in files_to_download:\n",
        "  try:\n",
        "    files.download(file_name)\n",
        "  except Exception as e:\n",
        "    print(f\"Error downloading {file_name}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVDlysSodN9e"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxg8Yn1BdJvl"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(suppress=True, precision=6, floatmode='fixed')\n",
        "\n",
        "HLSL30_surface_X_all = np.load('HLSL30_surface_X_all_CDL16_24.npy')\n",
        "print(np.nanmean(HLSL30_surface_X_all, axis=0))\n",
        "HLSL30_rootzone_X_all = np.load('HLSL30_rootzone_X_all_CDL16_24.npy')\n",
        "print(np.nanmean(HLSL30_rootzone_X_all, axis=0))\n",
        "sentinel2_rootzone_X_all = np.load('sentinel2_rootzone_X_all_CDL16_24.npy')\n",
        "print(np.nanmean(sentinel2_rootzone_X_all, axis=0))\n",
        "sentinel2_surface_X_all = np.load('sentinel2_surface_X_all_CDL16_24.npy')\n",
        "print(np.nanmean(sentinel2_surface_X_all, axis=0))\n",
        "sentinel1_rootzone_X_all = np.load('sentinel1_rootzone_X_all_CDL16_24.npy')\n",
        "print(np.nanmean(sentinel1_rootzone_X_all, axis=0))\n",
        "sentinel1_surface_X_all = np.load('sentinel1_surface_X_all_CDL16_24.npy')\n",
        "print(np.nanmean(sentinel1_surface_X_all, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJChUb-0d-WK"
      },
      "source": [
        "## run ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qYIMSs6eAjK"
      },
      "outputs": [],
      "source": [
        "#@title Load ML models - this is the ML models we trained in HRSM_MAPPING_Sec1_ML_ParameterTune_Validation_Train.ipynb\n",
        "ML1_sly_scaler = joblib.load('./StartFolder/TrainedML/ML1_sly_scaler.joblib')\n",
        "ML1_rly_scaler = joblib.load('./StartFolder/TrainedML/ML1_rly_scaler.joblib')\n",
        "ML2_sly_scaler = joblib.load('./StartFolder/TrainedML/ML2_sly_scaler.joblib')\n",
        "ML2_rly_scaler = joblib.load('./StartFolder/TrainedML/ML2_rly_scaler.joblib')\n",
        "ML3_sly_scaler = joblib.load('./StartFolder/TrainedML/ML3_sly_scaler.joblib')\n",
        "ML3_rly_scaler = joblib.load('./StartFolder/TrainedML/ML3_rly_scaler.joblib')\n",
        "# ML1-sly\n",
        "ML1_sly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML1_sly_lgb_mean.joblib')\n",
        "ML1_sly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML1_sly_mapie_cqr.joblib')\n",
        "# ML1 - rly\n",
        "ML1_rly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML1_rly_lgb_mean.joblib')\n",
        "ML1_rly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML1_rly_mapie_cqr.joblib')\n",
        "# ML2-sly\n",
        "ML2_sly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML2_sly_lgb_mean.joblib')\n",
        "ML2_sly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML2_sly_mapie_cqr.joblib')\n",
        "# ML2-rly\n",
        "ML2_rly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML2_rly_lgb_mean.joblib')\n",
        "ML2_rly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML2_rly_mapie_cqr.joblib')\n",
        "# ML3-sly\n",
        "ML3_sly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML3_sly_lgb_mean.joblib')\n",
        "ML3_sly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML3_sly_mapie_cqr.joblib')\n",
        "# ML3-rly\n",
        "ML3_rly_lgb_mean = joblib.load('./StartFolder/TrainedML/ML3_rly_lgb_mean.joblib')\n",
        "ML3_rly_mapie_cqr = joblib.load('./StartFolder/TrainedML/ML3_rly_mapie_cqr.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdK-JoiJeOD4"
      },
      "outputs": [],
      "source": [
        "removed_indices_ML1 = [0, 1, 3, 15]\n",
        "removed_indices_ML2 = [0, 1, 3, 15, 24, 25, 26]  # remove B2, B3, B4\n",
        "removed_indices_ML3 = [0, 1, 3, 15, 24, 25, 26, 32, 33, 34, 35]  # remove B2, B3, B4, SZA, SAA, VZA, VAA, 32, 33, 34, 35\n",
        "\n",
        "\n",
        "sentinel1_surface_X_all = np.delete(sentinel1_surface_X_all, removed_indices_ML1, axis=1)  #, 15\n",
        "sentinel2_surface_X_all = np.delete(sentinel2_surface_X_all, removed_indices_ML2, axis=1)\n",
        "HLSL30_surface_X_all = np.delete(HLSL30_surface_X_all, removed_indices_ML3, axis=1)\n",
        "sentinel1_rootzone_X_all = np.delete(sentinel1_rootzone_X_all, removed_indices_ML1, axis=1)\n",
        "sentinel2_rootzone_X_all = np.delete(sentinel2_rootzone_X_all, removed_indices_ML2, axis=1)\n",
        "HLSL30_rootzone_X_all = np.delete(HLSL30_rootzone_X_all, removed_indices_ML3, axis=1)\n",
        "\n",
        "print(sentinel1_surface_X_all.shape)\n",
        "print(sentinel2_surface_X_all.shape)\n",
        "print(HLSL30_surface_X_all.shape)\n",
        "print(sentinel1_rootzone_X_all.shape)\n",
        "print(sentinel2_rootzone_X_all.shape)\n",
        "print(HLSL30_rootzone_X_all.shape)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFqb2Seue6ao"
      },
      "outputs": [],
      "source": [
        "sentinel1_surface_X_all = ML1_sly_scaler.transform(sentinel1_surface_X_all)\n",
        "sentinel1_rootzone_X_all = ML1_rly_scaler.transform(sentinel1_rootzone_X_all)\n",
        "sentinel2_surface_X_all = ML2_sly_scaler.transform(sentinel2_surface_X_all)\n",
        "sentinel2_rootzone_X_all = ML2_rly_scaler.transform(sentinel2_rootzone_X_all)\n",
        "HLSL30_surface_X_all = ML3_sly_scaler.transform(HLSL30_surface_X_all)\n",
        "HLSL30_rootzone_X_all = ML3_rly_scaler.transform(HLSL30_rootzone_X_all)\n",
        "\n",
        "sentinel1_surface_y = ML1_sly_lgb_mean.predict(sentinel1_surface_X_all)\n",
        "sentinel1_rootzone_y = ML1_rly_lgb_mean.predict(sentinel1_rootzone_X_all)\n",
        "sentinel2_surface_y = ML2_sly_lgb_mean.predict(sentinel2_surface_X_all)\n",
        "sentinel2_rootzone_y = ML2_rly_lgb_mean.predict(sentinel2_rootzone_X_all)\n",
        "HLSL30_surface_y = ML3_sly_lgb_mean.predict(HLSL30_surface_X_all)\n",
        "HLSL30_rootzone_y = ML3_rly_lgb_mean.predict(HLSL30_rootzone_X_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5XvlD6FfmdK"
      },
      "outputs": [],
      "source": [
        "print(sentinel1_surface_y.shape)\n",
        "print(sentinel1_rootzone_y.shape)\n",
        "print(sentinel2_surface_y.shape)\n",
        "print(sentinel2_rootzone_y.shape)\n",
        "print(HLSL30_surface_y.shape)\n",
        "print(HLSL30_rootzone_y.shape)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbCKs3g6fwUA"
      },
      "outputs": [],
      "source": [
        "np.save('sentinel1_surface_y_CDL16_24.npy', sentinel1_surface_y)\n",
        "np.save('sentinel1_rootzone_y_CDL16_24.npy', sentinel1_rootzone_y)\n",
        "np.save('sentinel2_surface_y_CDL16_24.npy', sentinel2_surface_y)\n",
        "np.save('sentinel2_rootzone_y_CDL16_24.npy', sentinel2_rootzone_y)\n",
        "np.save('HLSL30_surface_y_CDL16_24.npy', HLSL30_surface_y)\n",
        "np.save('HLSL30_rootzone_y_CDL16_24.npy', HLSL30_rootzone_y)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrE-Y0MO26Qx"
      },
      "source": [
        "## post-process predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElnjsAZE2-rg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the saved arrays\n",
        "sentinel1_surface_y_CDL16_24 = np.load('sentinel1_surface_y_CDL16_24.npy')\n",
        "sentinel1_rootzone_y_CDL16_24 = np.load('sentinel1_rootzone_y_CDL16_24.npy')\n",
        "sentinel2_surface_y_CDL16_24 = np.load('sentinel2_surface_y_CDL16_24.npy')\n",
        "sentinel2_rootzone_y_CDL16_24 = np.load('sentinel2_rootzone_y_CDL16_24.npy')\n",
        "HLSL30_surface_y_CDL16_24 = np.load('HLSL30_surface_y_CDL16_24.npy')\n",
        "HLSL30_rootzone_y_CDL16_24 = np.load('HLSL30_rootzone_y_CDL16_24.npy')\n",
        "#\n",
        "sentinel1_surface_X_all_CDL16_24 = np.load('sentinel1_surface_X_all_CDL16_24.npy')\n",
        "sentinel1_rootzone_X_all_CDL16_24 = np.load('sentinel1_rootzone_X_all_CDL16_24.npy')\n",
        "sentinel2_surface_X_all_CDL16_24 = np.load('sentinel2_surface_X_all_CDL16_24.npy')\n",
        "sentinel2_rootzone_X_all_CDL16_24 = np.load('sentinel2_rootzone_X_all_CDL16_24.npy')\n",
        "HLSL30_surface_X_all_CDL16_24 = np.load('HLSL30_surface_X_all_CDL16_24.npy')\n",
        "HLSL30_rootzone_X_all_CDL16_24 = np.load('HLSL30_rootzone_X_all_CDL16_24.npy')\n",
        "#\n",
        "print('sentinel1_surface_y_CDL16_24.shape: ', sentinel1_surface_y_CDL16_24.shape)\n",
        "print('sentinel1_rootzone_y_CDL16_24.shape: ', sentinel1_rootzone_y_CDL16_24.shape)\n",
        "print('sentinel2_surface_y_CDL16_24.shape: ', sentinel2_surface_y_CDL16_24.shape)\n",
        "print('sentinel2_rootzone_y_CDL16_24.shape: ', sentinel2_rootzone_y_CDL16_24.shape)\n",
        "print('HLSL30_surface_y_CDL16_24.shape: ', HLSL30_surface_y_CDL16_24.shape)\n",
        "print('HLSL30_rootzone_y_CDL16_24.shape: ', HLSL30_rootzone_y_CDL16_24.shape)\n",
        "#\n",
        "print('sentinel1_surface_X_all_CDL16_24.shape: ', sentinel1_surface_X_all_CDL16_24.shape)\n",
        "print('sentinel1_rootzone_X_all_CDL16_24.shape: ', sentinel1_rootzone_X_all_CDL16_24.shape)\n",
        "print('sentinel2_surface_X_all_CDL16_24.shape: ', sentinel2_surface_X_all_CDL16_24.shape)\n",
        "print('sentinel2_rootzone_X_all_CDL16_24.shape: ', sentinel2_rootzone_X_all_CDL16_24.shape)\n",
        "print('HLSL30_surface_X_all_CDL16_24.shape: ', HLSL30_surface_X_all_CDL16_24.shape)\n",
        "print('HLSL30_rootzone_X_all_CDL16_24.shape: ', HLSL30_rootzone_X_all_CDL16_24.shape)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEfyhHNu-wyq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: if your array is already defined\n",
        "# sentinel1_surface_y_CDL16_24 = np.load('your_array.npy')\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(HLSL30_surface_y_CDL16_24, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Value', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Histogram of Sentinel-1 Surface Values (CDL16_24)', fontsize=14)\n",
        "plt.grid(alpha=0.4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0UyE6kS8eg3"
      },
      "outputs": [],
      "source": [
        "#@title Re-organize to dataframe\n",
        "\n",
        "# reorganize to 2-d array:\n",
        "def get_time_index(initial_date, target_date):\n",
        "    return int((target_date - initial_date).total_seconds() / 3600)\n",
        "\n",
        "def DoY2Date(year, doy, hh=10):\n",
        "    base_date = datetime(year, 1, 1, hour=hh)\n",
        "    target_date = base_date + timedelta(days=(doy - 1))\n",
        "    return target_date\n",
        "\n",
        "\n",
        "\n",
        "def cdf_matching(source, target):\n",
        "    sorted_target = np.sort(target)  # Target distribution sorted\n",
        "    sorted_target_ranks = rankdata(sorted_target, method='average')\n",
        "    sorted_target_quantiles = sorted_target_ranks / len(sorted_target)\n",
        "    ranks = rankdata(source, method='average')  # Source data ranks\n",
        "    percentiles = ranks / len(source)  # Percentile positions of source data\n",
        "    matched_values = np.interp(percentiles, sorted_target_quantiles, sorted_target)  # Interpolation\n",
        "    return matched_values\n",
        "\n",
        "\n",
        "\n",
        "def CDF_maptching_and_reorganize_arr(ML1_sly_y_pred_all_arr, ML2_sly_y_pred_all_arr, ML3_sly_y_pred_all_arr, ML1_sly_info_arr, ML2_sly_info_arr, ML3_sly_info_arr, refer_id_arr):\n",
        "\n",
        "\n",
        "    refer_id_idx_dict = {}\n",
        "    for i in range(len(refer_id_arr)):\n",
        "      refer_id_idx_dict[refer_id_arr[i]] = i\n",
        "\n",
        "\n",
        "    ids = refer_id_arr\n",
        "    len_id = len(ids)\n",
        "\n",
        "    num_hours = 78192\n",
        "\n",
        "    ML_sly_y_pred_arr = np.zeros((4, num_hours, len_id))\n",
        "    ML_sly_y_pred_arr[:] = np.nan\n",
        "\n",
        "    start_time = datetime(2016, 1, 1, 0)\n",
        "\n",
        "    for i in range(len(ML1_sly_y_pred_all_arr)):\n",
        "      t_idx = get_time_index(start_time, DoY2Date(int(ML1_sly_info_arr[i,  1]), int(ML1_sly_info_arr[i, 2]), int(ML1_sly_info_arr[i, 3])))\n",
        "      id_idx = refer_id_idx_dict[int(ML1_sly_info_arr[i, 0])]\n",
        "      ML_sly_y_pred_arr[0, t_idx, id_idx] = ML1_sly_y_pred_all_arr[i]\n",
        "\n",
        "\n",
        "    for i in range(len(ML2_sly_y_pred_all_arr)):\n",
        "      t_idx = get_time_index(start_time, DoY2Date(int(ML2_sly_info_arr[i,  1]), int(ML2_sly_info_arr[i, 2]), int(ML2_sly_info_arr[i, 3])))\n",
        "      id_idx = refer_id_idx_dict[int(ML2_sly_info_arr[i, 0])]\n",
        "      ML_sly_y_pred_arr[1, t_idx, id_idx] = ML2_sly_y_pred_all_arr[i]\n",
        "\n",
        "\n",
        "    for i in range(len(ML3_sly_y_pred_all_arr)):\n",
        "      t_idx = get_time_index(start_time, DoY2Date(int(ML3_sly_info_arr[i,  1]), int(ML3_sly_info_arr[i, 2]), int(ML3_sly_info_arr[i, 3])))\n",
        "      id_idx = refer_id_idx_dict[int(ML3_sly_info_arr[i, 0])]\n",
        "      ML_sly_y_pred_arr[2, t_idx, id_idx] = ML3_sly_y_pred_all_arr[i]\n",
        "\n",
        "\n",
        "    for j in range(len_id):\n",
        "      sentinel1_arr = ML_sly_y_pred_arr[0, :, j]\n",
        "      sentinel2_arr = ML_sly_y_pred_arr[1, :, j]\n",
        "      HLSL30_arr = ML_sly_y_pred_arr[2, :, j]\n",
        "\n",
        "      sentinel1_nan_mask = np.isnan(sentinel1_arr)\n",
        "      sentinel2_nan_mask = np.isnan(sentinel2_arr)\n",
        "      HLSL30_nan_mask = np.isnan(HLSL30_arr)\n",
        "\n",
        "      ML_sly_y_pred_arr[3, ~sentinel1_nan_mask, j] = cdf_matching(sentinel1_arr[~sentinel1_nan_mask], HLSL30_arr[~HLSL30_nan_mask])\n",
        "      ML_sly_y_pred_arr[3, ~sentinel2_nan_mask, j] = cdf_matching(sentinel2_arr[~sentinel2_nan_mask], HLSL30_arr[~HLSL30_nan_mask])\n",
        "      ML_sly_y_pred_arr[3, ~HLSL30_nan_mask, j] = HLSL30_arr[~HLSL30_nan_mask]\n",
        "\n",
        "\n",
        "    return ML_sly_y_pred_arr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ML_sly_y_pred_arr = CDF_maptching_and_reorganize_arr(sentinel1_surface_y_CDL16_24, sentinel2_surface_y_CDL16_24, HLSL30_surface_y_CDL16_24,\n",
        "                                                     sentinel1_surface_X_all_CDL16_24[:, :4], sentinel2_surface_X_all_CDL16_24[:, :4], HLSL30_surface_X_all_CDL16_24[:, :4],\n",
        "                                                     referIDArr)\n",
        "np.save('ML_sly_y_pred_arr_CDL16_24.npy', ML_sly_y_pred_arr)\n",
        "\n",
        "ML_rly_y_pred_arr = CDF_maptching_and_reorganize_arr(sentinel1_rootzone_y_CDL16_24, sentinel2_rootzone_y_CDL16_24, HLSL30_rootzone_y_CDL16_24,\n",
        "                                                     sentinel1_rootzone_X_all_CDL16_24[:, :4], sentinel2_rootzone_X_all_CDL16_24[:, :4], HLSL30_rootzone_X_all_CDL16_24[:, :4],\n",
        "                                                     referIDArr)\n",
        "np.save('ML_rly_y_pred_arr_CDL16_24.npy', ML_rly_y_pred_arr)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2RUHFfXfLo7e",
        "1JXbDpQOLE7O"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMlgItJBQ+sKUntA99SykxH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}